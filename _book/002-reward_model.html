<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Human Decision Making and Choice Models – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./003-measure.html" rel="next">
<link href="./001-introduction.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./002-reward_model.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002-reward_model.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./003-measure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./004-optim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005-align.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>© 2024. This work is openly licensed via <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY NC 4.0</a>.</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">2.1</span> Introduction</a></li>
  <li><a href="#sec:foundations" id="toc-sec:foundations" class="nav-link" data-scroll-target="#sec\:foundations"><span class="header-section-number">2.2</span> Foundations of Preference Models</a>
  <ul class="collapse">
  <li><a href="#axiom-1-preference-models-model-choice" id="toc-axiom-1-preference-models-model-choice" class="nav-link" data-scroll-target="#axiom-1-preference-models-model-choice">Axiom 1: Preference models model choice</a></li>
  <li><a href="#axiom-2-preference-captures-decision-making" id="toc-axiom-2-preference-captures-decision-making" class="nav-link" data-scroll-target="#axiom-2-preference-captures-decision-making">Axiom 2: Preference captures decision-making</a></li>
  <li><a href="#axiom-3-preference-centers-around-utility" id="toc-axiom-3-preference-centers-around-utility" class="nav-link" data-scroll-target="#axiom-3-preference-centers-around-utility">Axiom 3: Preference centers around utility</a></li>
  </ul></li>
  <li><a href="#sec:models" id="toc-sec:models" class="nav-link" data-scroll-target="#sec\:models"><span class="header-section-number">2.3</span> Models of Individual Choices</a>
  <ul class="collapse">
  <li><a href="#data-collection" id="toc-data-collection" class="nav-link" data-scroll-target="#data-collection"><span class="header-section-number">2.3.1</span> Data Collection</a></li>
  <li><a href="#data-interpretation" id="toc-data-interpretation" class="nav-link" data-scroll-target="#data-interpretation"><span class="header-section-number">2.3.2</span> Data Interpretation</a></li>
  </ul></li>
  <li><a href="#sec:learning" id="toc-sec:learning" class="nav-link" data-scroll-target="#sec\:learning"><span class="header-section-number">2.4</span> Parameter Learning</a>
  <ul class="collapse">
  <li><a href="#reward-learning-with-large-language-models" id="toc-reward-learning-with-large-language-models" class="nav-link" data-scroll-target="#reward-learning-with-large-language-models"><span class="header-section-number">2.4.1</span> Reward Learning with Large Language Models</a></li>
  <li><a href="#reward-learning-in-robotics" id="toc-reward-learning-in-robotics" class="nav-link" data-scroll-target="#reward-learning-in-robotics"><span class="header-section-number">2.4.2</span> Reward Learning in Robotics</a></li>
  <li><a href="#reward-learning-with-meta-learning" id="toc-reward-learning-with-meta-learning" class="nav-link" data-scroll-target="#reward-learning-with-meta-learning"><span class="header-section-number">2.4.3</span> Reward Learning with Meta Learning</a></li>
  <li><a href="#direct-preference-optimization" id="toc-direct-preference-optimization" class="nav-link" data-scroll-target="#direct-preference-optimization"><span class="header-section-number">2.4.4</span> Direct Preference Optimization</a></li>
  <li><a href="#model-design-consideration" id="toc-model-design-consideration" class="nav-link" data-scroll-target="#model-design-consideration"><span class="header-section-number">2.4.5</span> Model Design Consideration</a></li>
  </ul></li>
  <li><a href="#multimodal-preferences" id="toc-multimodal-preferences" class="nav-link" data-scroll-target="#multimodal-preferences"><span class="header-section-number">2.5</span> Multimodal Preferences</a></li>
  <li><a href="#social-choices" id="toc-social-choices" class="nav-link" data-scroll-target="#social-choices"><span class="header-section-number">2.6</span> Social Choices</a></li>
  <li><a href="#excercises" id="toc-excercises" class="nav-link" data-scroll-target="#excercises"><span class="header-section-number">2.7</span> Excercises</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/002-reward_model.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="2model" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>Human preference modeling aims to capture humans’ decision making processes in a probabilistic framework. Many problems would benefit from a quantitative perspective, enabling an understanding of how humans engage with the world. While human decision-making is only somewhat understood, we can use real-world data representing the outcomes of decisions to align human-facing systems with user preferences. Through our exploration of human preference models, we will ground ourselves in building a health coaching system that can provide meal recommendations aligned with a user’s dietary needs and preferences. Examples of scenarios which can benefit from a model of how humans make choices include:</p>
<ol type="1">
<li><p><strong>Health coaching:</strong> Humans express their preferences every time they pick lunch for consumption. Humans may have several goals related to nutrition, such as weight loss and improving concentration. We can learn how a given individual or set of individuals prefer to eat to provide personalized recommendations to help them attain their goals. This chapter will use this use case to ground human preference modeling in a real-life application.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/health_coaching.png" class="img-fluid figure-img"></p>
<figcaption>Health coaching is a running example we use to instantiate modeling concepts discussed in this chapter. In this example, several users with various health profiles have to choose from various food options, each with varying carbohydrate, protein, and lipid contents. We employ a mini version of this problem throughout the text.</figcaption>
</figure>
</div></li>
<li><p><strong>Social media:</strong> Platforms have a far greater amount of content than one can consume in a lifetime, yet such products must aim to maximize user engagement. To accomplish this, we can learn what specific things people like to see in their feeds to optimize the value they gain out of their time on social media. For example, the video feed social media platform <a href="https://www.tiktok.com/">TikTok</a> has had viral adoption due to its notorious ability to personalize a feed for its users based on their preferences.</p></li>
<li><p><strong>Shopping:</strong> Retail corporations largely aim to maximize revenue by making it easy for people to make purchases. Recommendation systems on online shopping platforms provide a mechanism for curating specific items based on an individual’s previous purchases (or even browsing history) to make shoppers aware of items they may like and, therefore, purchase.</p></li>
</ol>
<div id="tab:philosophy">
<table class="caption-top table">
<caption>Examples of machine learning tasks and their interpretation as modeling human preferences.</caption>
<colgroup>
<col style="width: 48%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Application</th>
<th style="text-align: left;">Human Preference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Computer vision: train a neural network to predict bounding boxes delineating all instances of dogs in an image</td>
<td style="text-align: left;">This is how humans process images by identifying the position and geometry of the things we see in them</td>
</tr>
<tr class="even">
<td style="text-align: left;">Natural language processing: train a model to generate coherent text</td>
<td style="text-align: left;">Coherent text is itself a human-created and defined concept, and we prefer that any synthetically generated text matches that of humans</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Computer vision: train a diffusion model to generate realistic images of nature</td>
<td style="text-align: left;">Humans prefer that images accurately capture the world as observed by humans, and this generative model should reflect the details that comprise that preference</td>
</tr>
</tbody>
</table>
</div>
<p>In this chapter, we will explore how one can model human preferences, including different formulations of such models, how one can optimize these models given data, and considerations one must understand to create such systems. We note that the exact assumptions we make about human preferences in this chapter differentiate the <em>specific</em> human preference learning problem we are considering from the discriminative and generative tasks we describe in table <a href="#tab:philosophy" data-reference-type="ref" data-reference="tab:philosophy">1.1</a>. We describe these assumptions in section <a href="#sec:foundations" data-reference-type="ref" data-reference="sec:foundations">1.2</a>.</p>
</section>
<section id="sec:foundations" class="level2 page-columns page-full" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec:foundations"><span class="header-section-number">2.2</span> Foundations of Preference Models</h2>
<p>We introduce a framework for discussing human preferences. The different methods to model these preferences (<a href="#sec:models" data-reference-type="ref" data-reference="sec:models">1.3</a>) all build upon this framework.</p>
<section id="axiom-1-preference-models-model-choice" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="axiom-1-preference-models-model-choice">Axiom 1: Preference models model choice</h3>
<p>Human preference models model the preferred choice or choices amongst a set of options. In our health coaching example, this could be modeling which meal from a set of options a person will most likely choose. An alternative framework we will explore is ranking, in which we can model an ordering of given choices from most to least desirable. It is certainly possible that there is an infinite set of options (such as in a continuous action space); in this case, our model will have to reason about a discretized set of options and may fail to capture the full space of possibilities a human would choose from in the real world.</p>
<p>Choices are <em>collectively exhaustive</em>, <em>mutually exclusive</em>, and <em>finite</em>. Human preference models must enumerate an <em>action space</em>, or the set of all possible choices included in a human decision. As such, we must ensure that the choices we enumerate capture the entire domain (collectively exhaustive) but are indeed distinct (mutually exclusive) choices. In our health coaching example, a person either chooses to eat chicken or fish. Choosing one does not affect the other.</p>
<p>A discrete set of choices is a constraint we canonically impose to ensure we can tractably model preferences and aptly estimate the parameters of preference models. This is usually sufficiently expressive to create a powerful human preference model (for example, recent generative language models have vocabulary sizes of 40,000+ and can model nearly arbitrary language sequences <span class="citation" data-cites="Radford2018GPT">(<a href="#ref-Radford2018GPT" role="doc-biblioref">Radford et al. 2018</a>)</span>). While in theory, one can imagine a continuous domain for choices, a discrete set fits nicely with most decision-making processes humans face. While human thought is extremely nuanced, most thoughts are expressed as discrete words or discrete decisions in every step humans take in the world.</p>
</section>
<section id="axiom-2-preference-captures-decision-making" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="axiom-2-preference-captures-decision-making">Axiom 2: Preference captures decision-making</h3>
<p>There are certainly cases in which human preferences don’t reflect the human decision-making process, for example if there are external factors (social, political, economic) which govern a human’s choices, or if one is explicitly choosing to go against their preferences in the context of exploration. However, human preference models will always do their best to model the ultimate decision, and we assume that they are in some way accounting for these other factors (and any lack of such accounting will result in a biased model). Human preferences are generally classified into two categories:</p>
<ol type="1">
<li><p>Revealed preferences are those one can observe retroactively from existing data. The implicit decision-making knowledge can be captured via learnable parameters and their usage in models which represent relationships between input decision attributes that may have little human interpretability, but enable powerful models of human preference. For health coaching, we may have information about which foods an individual has chosen previously in different contexts, allowing us to build a model from their decisions. Such data may be easier to acquire and can reflect real-world outcomes (since they are, at least theoretically, inherently based on human preferences). However, if we fail to capture sufficient context in such data, human preference models may not sufficiently capture human preferences.</p></li>
<li><p>Stated preferences are those individuals explicitly indicate in potentially experimental conditions. The explicit knowledge may be leveraged by including inductive biases during modeling (for example, the context used in a model) which are reasonable assumptions for how a human would consider a set of options.This may include controlled experiments or studies. This may be harder to obtain and somewhat biased, as they can be hypothetical or only accurately reflect a piece of the overall context of a decision. However, they enable greater control of the decision-making process.</p></li>
</ol>
<p>[Talk about social choice models - the above paragraph applies but there is extra nuance e.g.&nbsp;preferences compounding or confounding/cancelling out]</p>
<p>[Talk about mechanism design to incentivise decision making matching human preferences]</p>
<section id="human-rationality" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="human-rationality">Human Rationality</h4>
<p>Modeling decision-making must also take into account the rational and irrational behaviour of humans. Therefore we consider <em>rationality assumptions</em> as a fundamental aspect of understanding how individuals make decisions. These assumptions provide a framework for predicting and modeling human behavior by outlining the principles that guide decision-making processes <span class="citation" data-cites="keisler2003common">(<a href="#ref-keisler2003common" role="doc-biblioref">Keisler and Lee 2003</a>)</span>.</p>
<p>Perfect rationality posits that individuals always make decisions that maximize their utility. It assumes that individuals have complete information and the cognitive ability to process this information to make optimal choices <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>. This assumption is often used in economic models to predict how rational agents would behave under ideal conditions. However, numerous studies have shown that this assumption frequently fails to describe actual human behavior, as individuals do not always act in ways that maximize their utility due to various constraints and biases <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>. Bounded rationality, on the other hand, acknowledges that individuals operate within the limits of their information and cognitive capabilities. Decisions are made using heuristics or rules of thumb rather than through exhaustive analysis, reflecting the practical constraints of real-world decision-making <span class="citation" data-cites="simon1972theories">(<a href="#ref-simon1972theories" role="doc-biblioref">Simon 1972</a>)</span>. This concept, introduced by Herbert Simon, recognizes the limitations of human cognitive processing and the impact of these limitations on decision-making. Simon’s theory suggests that instead of optimizing, individuals satisfy, seeking solutions or decisions that are “good enough” under the circumstances <span class="citation" data-cites="simon1972theories">(<a href="#ref-simon1972theories" role="doc-biblioref">Simon 1972</a>)</span>. Noisy rationality assumes that decisions are influenced by random noise, resulting in probabilistic choice behavior. This means that while individuals aim to maximize their utility, random factors can lead to deviations from perfectly rational choices. This approach is useful for modeling behavior in situations where decisions are not entirely deterministic and are subject to variability <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>. This probabilistic approach aligns with findings from behavioral economics and psychology, which indicate that human decision-making is often inconsistent and influenced by various random factors <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>.</p>
<p>Understanding rationality assumptions is crucial for modeling and predicting human behavior in various decision-making scenarios. These assumptions provide the foundation for developing models that can simulate and analyze how individuals interact with one another and their environment. By incorporating different types of rationality, researchers can create more accurate and realistic models that reflect the complexities of human decision-making. This comprehensive approach enhances the predictive power of models and improves the understanding of human behavior in economic and social contexts <span class="citation" data-cites="miljkovic2005rational simon1972theories">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>; <a href="#ref-simon1972theories" role="doc-biblioref">Simon 1972</a>)</span>.</p>
<p>Luce’s axiom of choice <span class="citation" data-cites="Luce1977">(<a href="#ref-Luce1977" role="doc-biblioref">Luce 1977</a>)</span> and Boltzmann’s Rationality provide a probabilistic framework for modeling noisily-rational human behavior. Luce’s axiom of choice addresses the likelihood of a human selecting an option <span class="math inline">\(o\)</span> from a set <span class="math inline">\(O\)</span>. Desirability is represented by a value function <span class="math inline">\(v : O \rightarrow \Rbb^+\)</span>, with the selection probability calculated as <span class="math inline">\(P(o) = \frac{v(o)}{\sum_{o' \in O} v(o')}\)</span>. Assuming there is an underlying reward for each option <span class="math inline">\(R(o) \in \Rbb\)</span> such that <span class="math inline">\(v(o) = e^{R(o)}\)</span>, we get <span class="math inline">\(P(o) = \frac{e^{R(o)}}{\sum_{\bar{o} \in \mathcal{O}} e^{R(\bar{o})}}\)</span>. Essentially, “A human will act out a trajectory with a probability proportional to the exponentiated return they receive for the trajectory.” This probabilistic approach challenges the traditional assumption of perfect economic rationality, where individuals always make decisions that maximize their utility. When choices involve trajectories <span class="math inline">\(\xi \in \Xi\)</span> (sequences of actions), the Boltzmann model <span class="citation" data-cites="VonNeumannMorgenstern1945">(<a href="#ref-VonNeumannMorgenstern1945" role="doc-biblioref">Neumann and Morgenstern 1945</a>)</span> is used. Here, the reward <span class="math inline">\(R\)</span> is typically a function of a feature vector <span class="math inline">\(\phi : \Xi \rightarrow \Rbb^k\)</span>, and the probability density is given by <span class="math inline">\(p(\xi) = \frac{e^{R(\phi(\xi))}}{\int_{\Xi} e^{R(\phi(\bar{\xi}))} d\bar{\xi}}\)</span>. Boltzmann Rationality serves a critical role in human preferences and decision-making. It captures the probabilistic nature of human choices, recognizing that decisions are often noisy and influenced by various factors. This model is instrumental in preference modeling, accommodating human preferences’ inherent variability and uncertainty.</p>
<p>However, the Luce choice axiom and Boltzmann Rationality encounter a known issue called the “duplicates problem,” where there is no concept of similar actions (e.g., choosing between using a car or a train for transportation, with no particular preference). The probability of making the decision is 50% for either option. However, if we now have 100 cars, under Luce/Boltzmann, we would have a 99% probability of choosing a car, which is unrealistic.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/car.png" class="img-fluid figure-img"></p>
<figcaption>Illustration of the duplicates problem</figcaption>
</figure>
</div>
<p>To address this issue, various extensions have been proposed. One such extension is the attribute rule, which interprets options as bundles of attributes. In this rule, attributes <span class="math inline">\(X\)</span> are associated with options, and they have desirability values <span class="math inline">\(w(x)\)</span>. An attribute intensity function <span class="math inline">\(s(x, o)\)</span> indicates the degree to which an attribute is expressed in an option. The probability of choosing option <span class="math inline">\(o\)</span> is calculated as:</p>
<p><span class="math display">\[P(o) = \sum_{x \in \mathcal{X}_o} \frac{w(x)}{\sum_{\bar{x} \in \mathcal{X}_o} w(\bar{x})} \cdot \frac{s(x, o)}{\sum_{\tilde{o} \in \mathcal{O}} s(x, \bar{o})}\]</span></p>
<p>This equation describes a two-step process where an attribute <span class="math inline">\(x \in X_O\)</span> is first chosen according to a Luce-like rule and then an option <span class="math inline">\(o \in O\)</span> with that attribute is selected using another Luce-like rule. This approach handles duplicates gracefully by effectively creating a two-layer hierarchy in choosing an option.</p>
<p>Boltzmann Rationality finds practical applications in various fields, particularly in reinforcement learning, where it models decision-making in uncertain environments. It also applies to trajectory selection, where the probability of a sequence of actions (trajectory) is proportional to the exponential return. These applications enhance the accuracy of models that interact with or predict human behavior, making Boltzmann Rationality a vital component of the models of interaction.</p>
<p>We next explore a case study to deepen our understanding of rationality: Limiting Errors due to Similar Selection (LESS) <span class="citation" data-cites="2001.04465">(<a href="#ref-2001.04465" role="doc-biblioref">Bobu et al. 2020</a>)</span>. LESS takes inspiration from the attribute rule and extends it to continuous trajectories <span class="citation" data-cites="2001.04465">(<a href="#ref-2001.04465" role="doc-biblioref">Bobu et al. 2020</a>)</span>. The key insight is that instead of creating “attributes”, which group together similar discrete options, it introduces a similarity metric on the space of continuous actions, thereby creating similar groupings on trajectories.</p>
<p>First, discussing the distinction between trajectory and feature space is important. The LESS similarity metric could be defined in trajectory space, where the trajectory is some theoretical notion of all states and actions one passes through over time. However, it is instead defined on the measured feature vector <span class="math inline">\(\phi(\xi)\)</span> associated with the agent’s trajectory <span class="math inline">\(\xi\)</span>. Why? In practice, one can never measure the exact trajectory with perfect fidelity. The feature vector will almost necessarily map in a one-to-many fashion with trajectories. Formally, let <span class="math inline">\(\phi \in \Phi\)</span> be the set of all possible feature vectors <span class="math inline">\(\xi \in \Xi\)</span> the set of all trajectories. The set of feature vectors belonging to a set of trajectories <span class="math inline">\(\Xi' \subseteq \Xi\)</span> is <span class="math inline">\(\Phi_{\Xi'}\)</span>. We begin with equation (4) and substitute our similarity metric on feature vectors of trajectories.</p>
<p><span class="math display">\[\begin{aligned}
    P(\xi) = \frac{e^{R(\phi(\xi))}}{\sum_{\bar{\phi} \in \Phi_{\Xi}} e^{R(\hat{\phi})}} \cdot \frac{s(\phi(\xi), \bar{\xi})}{\sum_{\hat{\xi} \in \Xi} s(\phi(\xi), \bar{\xi})}
\end{aligned}\]</span></p>
<p>In this formulation, the first half of the product is simply Boltzmann equation. The probability of choosing trajectory <span class="math inline">\(\xi\)</span> is proportional to the exponentiated reward for the agent’s measured trajectory <span class="math inline">\(\phi(\xi)\)</span>, normalized by the sum of all rewards over all possible measured trajectories. The second half of the product is a normalization factor based on how similar the current trajectory is to other trajectories in feature space. We can define the similarity function as an indicator function, where <span class="math inline">\(s(x, \xi) = 1\)</span> only if <span class="math inline">\(x = \phi(\xi)\)</span>. That means that multiple trajectories with the same feature vector will effectively be considered a single option. Thus, we achieve the “bundling” of trajectories, in the same way that the attribute rule bundled options under different attributes.</p>
<p>However, setting the similarity metric as an indicator function isn’t sufficiently flexible. We want a proper metric that acts more as a continuous distance over the feature space. We instead define <span class="math inline">\(s\)</span> to be a <em>soft similarity metric</em> <span class="math inline">\(s : \Phi \times \Xi \rightarrow \Rbb^+\)</span>. It has the following properties:</p>
<ol type="1">
<li><p><span class="math inline">\(s(\phi(\xi), \xi) = \max_{x \in \phi, \bar{\xi} \in \Xi} s(x, \hat{\xi})) \forall (\xi \in \Xi)\)</span></p></li>
<li><p>Symmetric: <span class="math inline">\(s(\phi(\xi), \bar{\xi}) = s(\phi(\bar{\xi}), \xi)\)</span></p></li>
<li><p>Positive Semidefinite: <span class="math inline">\(s(x, \xi) \geq 0\)</span></p></li>
</ol>
<p>Using this redefined similarity metric <span class="math inline">\(s\)</span>, we extend (5) to be a probability density on the continuous trajectory space <span class="math inline">\(\mathcal{E}\)</span>, as in (3).</p>
<p><span class="math display">\[p(\hat{\xi}) = \frac{\frac{e^{R(\phi(\xi))}}{\int_{\Xi} s(\phi(\xi), \bar{\xi}) d\bar{\xi}}}{\int_{\Xi}\frac{e^{R(\phi(\hat{\xi}))}}{\int_{\Xi} s(\phi(\hat{\xi}), \bar{\xi}) d\bar{\xi}}d\hat{\xi}} \propto \frac{e^{R(\phi(\hat{\xi}))}}{\int_{\Xi} s(\phi(\xi), \bar{\xi}) d\bar{\xi}}\]</span></p>
<p>Under this formulation, the likelihood of selecting a trajectory is inversely proportional to its feature-space similarity with other trajectories. This de-weights similar trajectories, which is the desired effect for our LESS model of human decision-making. This means, though, that the “trajectory bundle” of similar trajectories still has a reasonable probability of being chosen.</p>
</section>
</section>
<section id="axiom-3-preference-centers-around-utility" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="axiom-3-preference-centers-around-utility">Axiom 3: Preference centers around utility</h3>
<p>Human preference models are centered around the notion of utility, which can mean a reward one attains after expressing one’s preference over options.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> In health coaching, the utility, as a function of the health choices users make, may be satiety, latent promotion of overall health, or even a quantitative extension of life. Of course, humans don’t necessarily use an explicit measure of utility — frequently humans use qualitative factors such as emotion or external influence to make decision. However, we assume that the underlying utility mechanism of a human preference model still captures the final decision output from a human.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Note that an assumption we make in the notion of utility is that utility is a function of the frequency of choosing certain options over alternatives.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Note that utility is often not an observable quantity, but something our models can infer given a set of observed preferences or decisions over choices. In our running example of health coaching, the utility might be ‘satiety’, for instance, which we do not measure directly, but which we infer through the choices humans make over concrete meal options.</p></div></div><p>Utility can be interpreted as a scalar quantity representing the benefit or value an individual attains from selecting a given choice. Each choice has an associated utility. Human preference models capture both the utility of a choice (e.g.&nbsp;we model the utility value as a function of attributes of a given choice) and how the utilities interact to make a decision.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> We use the notation <span class="math inline">\(U_i\)</span> as the utility corresponding to choice <span class="math inline">\(i\)</span>.</p>
<ol type="1">
<li><p><strong>The utility of a choice is a stochastic function of the choice’s attributes.</strong> We will henceforth define utility as follows <span class="math inline">\(U_i = H_i(z_i)\)</span> where <span class="math inline">\(z_i\)</span> is a variable describing the attributes of choice <span class="math inline">\(i\)</span> and <span class="math inline">\(H_i\)</span> is the stochastic function defining this choice’s utility. As a simple example, we can use a 1-D linear stochastic function to define <span class="math inline">\(H_i\)</span>: <span class="math inline">\(U_i = H_i(z_i) = \beta z_i + \epsilon_i\)</span>, where <span class="math inline">\(\beta\)</span> is a parameter of the model and <span class="math inline">\(\epsilon_i\)</span> is an unobserved factor for choice <span class="math inline">\(i\)</span>. Generally, we assume that the <span class="math inline">\(\epsilon_i\)</span> factor is a random variable following a specified distribution, such as a standard normal distribution. The attributes we use to represent a choice (a single scalar value <span class="math inline">\(z_i\)</span> in this example) is a critical design decision in defining the human preference model. These attributes define the context our model has in representing the human behavior we wish to capture, when choice <span class="math inline">\(i\)</span> is made. In our health coaching example, we may hope to provide the best possible diet recommendations for an individual. However, if our vector representation <span class="math inline">\(z_i\)</span> of their choice <span class="math inline">\(i\)</span> does not include vital information, such as allergy risks associated to the choice or ingredients which make up the choice, our model may not have enough information to properly capture the human preference.</p></li>
<li><p><strong>The preferred choice is that whose corresponding utility is the largest.</strong> Given that we model utility as the underlying benefit or value a human derives from choosing a given option, intuitively, we expect a human to choose the option with the largest utility. In our example of health coaching, if we model utility as the expected increase in lifespan, we will surely opt for the choice that maximizes this notion of utility. In our example, since <span class="math inline">\(U_1 &gt; U_2\)</span>, our model indicates that a user would opt for the burrito.</p></li>
<li><p><strong>Relativity of Utility.</strong> Given the two previously defined characteristics of utility, we observe that only the relative difference in utility matters. Even if <span class="math inline">\(U_1 = 0.001\)</span> and <span class="math inline">\(U_2 = 0.0005\)</span>, the model indicates the same outcome: a user prefers option <span class="math inline">\(1\)</span>. As such, even the scale of the utilities is irrelevant within a given set of human preference data for a given individual. In our example, we can scale the value of <span class="math inline">\(\beta\)</span> without changing the overall outcome so long as we do not change the sign. The scale of utilities <em>is</em> important when comparing human preferences across datasets, or comparing the same model across different humans; since utility may be defined differently in various datasets, perhaps their exact values are not aligned in a manner which allows one to robustly compare preferences between them. A common practice to address this consideration is to standardize the utilities in each dataset based on its variance in the observed data. Furthermore, a human preference model may generate different scales of utilities across different humans (based on the inputs and representation of the human). In this case, one can standardize the utilities for each individual based on the observed variance for that human. As we can see, the relativity of utility can be both powerful (enabling us to create flexible models and efficiently optimize them) and limiting (requiring us to perform mitigations when translating models across datasets or individuals. Still, we find the notion of utility necessary to model human preferences as it provides a quantitative value we can use to model human decisions.</p></li>
</ol>
<p>As a concrete model of meal recommendation in health coaching, let us suppose that we have three choices:</p>
<ol type="1">
<li><p>A burrito with rice, beans, and cheese.</p></li>
<li><p>French fries covered in mayonnaise.</p></li>
<li><p>A rice bowl with beans and chicken.</p></li>
</ol>
<p>If we design <span class="math inline">\(z_i\)</span> to be 1D, for example:</p>
<ol type="1">
<li><p><span class="math inline">\(z_1 = 1\)</span> for the burrito since this is a somewhat balanced meal that may help prolong the lifespan, which a user prefers.</p></li>
<li><p><span class="math inline">\(z_2 = -1\)</span> since this is unhealthy due to being deep fried, including saturated fats, and potentially reducing lifespan.</p></li>
<li><p><span class="math inline">\(z_3 = 1\)</span> since a rice bowl is another healthy meal.</p></li>
</ol>
<p>After observing the choices of a user who likes to eat healthily, we might learn that <span class="math inline">\(\beta = 1\)</span> is the best parameter for this model, and maybe we assume that <span class="math inline">\(\epsilon \sim \Nc(0, 1)\)</span>. Then, this model implies that <span class="math inline">\(U_1 = 1 \cdot 1 + 0.03 = 1.03\)</span>, <span class="math inline">\(U_2 = 1 \cdot -1 + (-0.07) = -1.07\)</span>, <span class="math inline">\(U_3 = 1 \cdot 1 + (0.02) = 1.02\)</span>, which means that the user, for whom <span class="math inline">\(\beta = 1\)</span> is the learned parameter, they would prefer the first meal, with the third meal as a close second option.</p>
<p>If we design <span class="math inline">\(z_i\)</span> to be 3D, to indicate the carbohydrate, protein, and fat content of each meal, then for example:</p>
<ol type="1">
<li><p><span class="math inline">\(z_1 = (1, 1, 0.1)\)</span> for the burrito</p></li>
<li><p><span class="math inline">\(z_2 = (1, 0, 1)\)</span> for the fries</p></li>
<li><p><span class="math inline">\(z_3 = (1, 1, 0.2)\)</span> for the rice bowl.</p></li>
</ol>
<p>After observing the choices of a user who likes to eat healthy, we might learn that <span class="math inline">\(\beta = (1, 1, -1)\)</span> is the best parameter for this model, and maybe we assume that <span class="math inline">\(\epsilon \sim \Nc(0, 1)\)</span>. Then, this model implies that <span class="math inline">\(U_1 = (1, 1, 0.1) \cdot (1, 1, -1) + 0.01 = 1.91\)</span>, <span class="math inline">\(U_2 = (1, 0, 1) \cdot (1, 1, -1) + 0.03 = 0.03\)</span>, <span class="math inline">\(U_3 = (1, 1, 0.2) \cdot (1, 1, -1) - 0.07 = 1.73\)</span>, which means that the user prefers meals 1 and 3, which again have the best utility, but in this multi-dimensional representation of <span class="math inline">\(z_i\)</span>, we start understanding how the two preferred meals are related (low fat and high protein).</p>
</section>
</section>
<section id="sec:models" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec:models"><span class="header-section-number">2.3</span> Models of Individual Choices</h2>
<p>After exploring motivations for preference learning and the framework we use to characterize human preferences to enable modeling, we now expand on the common probabilistic methods used to model human preference tasks. We will instantiate these models for our real-world health coaching application throughout as a pedagogical example. Specifically, we can define the following domain for meal choices: <span class="math inline">\(z_i, \beta \in \Zbb^3\)</span>, where <span class="math inline">\(z_i\)</span> defines the representation of a meal option with the three dimensions representing the carbohydrate, protein, and lipid macronutrient content of the meal, respectively, all measured in grams. <span class="math inline">\(\beta\)</span> is a parameter of the model. This simple representation will allow us to consider how different probabilistic frameworks for human preferences can model a user’s meal preferences. The information representation we instantiate here can accommodate scalar and high-dimensional vectors. While we use a mixture of integer and real-valued vectors in this simple example, we refer the reader to code in the practicum section for an example where vectors are all real-valued. If we let <span class="math inline">\(z = [20, 15, 3]\)</span> and <span class="math inline">\(\beta = [0.2, 1, -3]\)</span>. This corresponds to a meal with 20g carbohydrates, 15g protein, and 3g lipids. In the following sections, we discover how to learn the parameter <span class="math inline">\(\beta\)</span> and how to predict <span class="math inline">\(y\)</span> for this meal, which indicates whether the user chooses it or refuses it.</p>
<section id="data-collection" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="data-collection"><span class="header-section-number">2.3.1</span> Data Collection</h3>
<section id="pairwise-sampling" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="pairwise-sampling">Pairwise Sampling</h5>
<p>In pairwise sampling, participants compare two options simultaneously to determine which is preferred. The goal is to understand relative preferences between pairs of items. This method is frequently used in preference and choice studies to gather detailed preference data. Two key models used in pairwise sampling are the Thurstonian and Bradley-Terry models <span class="citation" data-cites="cattelan2012">(<a href="#ref-cattelan2012" role="doc-biblioref">Cattelan 2012</a>)</span>. The Thurstonian model assumes each item <span class="math inline">\(i\)</span> has a true score <span class="math inline">\(u_i\)</span> following a normal distribution. The difference <span class="math inline">\(d_{ij} = u_i - u_j\)</span> is also normally distributed. The probability that item <span class="math inline">\(i\)</span> is preferred over item <span class="math inline">\(j\)</span> is given by <span class="math inline">\(P(i \succ j) = \Phi \left( \frac{u_i - u_j}{\sqrt{2\sigma^2}} \right)\)</span>, where <span class="math inline">\(\Phi\)</span> is the cumulative normal distribution function. The denominator <span class="math inline">\(\sqrt{2\sigma^2}\)</span> is the standard deviation of the difference <span class="math inline">\(d_{ij} = u_i - u_j\)</span> when <span class="math inline">\(u_i\)</span> and <span class="math inline">\(u_j\)</span> are normally distributed with variance <span class="math inline">\(\sigma^2\)</span><span class="citation" data-cites="cattelan2012">(<a href="#ref-cattelan2012" role="doc-biblioref">Cattelan 2012</a>)</span>. The Bradley-Terry model defines the probability of preference based on latent scores <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\beta_j\)</span>. The probability that item <span class="math inline">\(i\)</span> is preferred over item <span class="math inline">\(j\)</span> is <span class="math inline">\(P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}\)</span>. This model is used to estimate relative strengths or preferences based on latent scores. <span class="citation" data-cites="cattelan2012">(<a href="#ref-cattelan2012" role="doc-biblioref">Cattelan 2012</a>)</span>.</p>
</section>
<section id="rank-order-sampling" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="rank-order-sampling">Rank-Order Sampling</h5>
<p>Rank-order sampling methods enable analysis of human preferences by asking participants to rank a set of items from most to least preferred. This approach is widely used in voting systems, market research, and psychological studies to understand the overall preference ordering among a set of items. Rank-order sampling offers comprehensive preference data, capturing detailed information about the relative ranking of multiple items. This richness makes them suitable for various applications, including market research, voting systems, sports competitions, and recommender systems. However, these models can be more complex and time-consuming for participants compared to pairwise comparisons, and they impose a higher cognitive load, especially with large sets of items. Additionally, participants may show inconsistencies when ranking many items <span class="citation" data-cites="ragain2019">(<a href="#ref-ragain2019" role="doc-biblioref">Ragain and Ugander 2019</a>)</span>.</p>
</section>
<section id="rating-scale-sampling" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="rating-scale-sampling">Rating-Scale Sampling</h5>
<p>Rating-scale sampling is a method in which participants rate items on a numerical scale to measure the intensity of preference or attitude towards items. These models are commonly used in surveys, product reviews, and psychological assessments to gather detailed information on how participants feel about various subjects. The Likert scale is a widely used rating-scale model. In this approach, participants rate items on a fixed-point scale, typically ranging from 1 to 5 or 1 to 7, to measure levels of agreement or satisfaction. For instance, a Likert scale might ask participants to rate their agreement with statements such as “Strongly Disagree” to “Strongly Agree” <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>. This method is prevalent in survey research, customer satisfaction studies, and attitude measurement. Another key model is the continuous rating scale, where participants mark a point on a continuous line to indicate their preference or attitude. This provides a more nuanced measure compared to discrete scales. For example, participants might indicate their satisfaction on a line ranging from “Very Unsatisfied” to “Very Satisfied” <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>. This model is used in detailed feedback mechanisms, user experience studies, and fine-grained preference measurements.</p>
<p>Rating-scale sampling offers several advantages. They are simple for participants to understand and use, provide rich data on the intensity of preferences, and are flexible enough for various types of measurements (e.g., agreement, satisfaction). Moreover, the data collected can be easily analyzed using standard statistical methods <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>.</p>
<p>Applications include data collection on opinions, attitudes, and behaviors; in product reviews to measure customer satisfaction and product quality; in psychological assessments to evaluate mental states, personality traits, and attitudes; and in user experience studies to understand user satisfaction and usability of products <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>. However, rating-scale sampling methods also have limitations. Ratings can be influenced by personal biases and interpretations of scales, leading to subjectivity. There is a central tendency bias, where participants may avoid extreme ratings, resulting in a clustering of responses around the middle. Different participants might interpret scale points differently, and fixed-point scales may not capture the full nuance of participants’ preferences or attitudes <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>.</p>
</section>
<section id="best-worst-scaling" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="best-worst-scaling">Best-Worst Scaling</h5>
<p>Best-Worst Scaling (BWS) is a powerful method for understanding preferences and the relative importance of different items. In BWS, participants are presented with a set of items and are asked to identify the most and least preferred options. This method helps to gather detailed preference data, providing more nuanced insights than traditional ranking or rating systems. The primary objective of BWS is to discern the relative importance or preference of items within a set, making it widely applicable in various fields such as market research, health economics, and social sciences <span class="citation" data-cites="campbell2015">(<a href="#ref-campbell2015" role="doc-biblioref">Campbell and Erdem 2015</a>)</span>.</p>
<p>A key method within BWS is MaxDiff Analysis, which involves presenting participants with sets of items and asking them to select the best and worst options. This approach yields richer data by identifying extremes in preferences, offering a clearer picture of the relative importance of each item. For instance, in a product development context, MaxDiff Analysis can help identify the most and least important features according to consumer preferences <span class="citation" data-cites="campbell2015">(<a href="#ref-campbell2015" role="doc-biblioref">Campbell and Erdem 2015</a>)</span>.</p>
<p>The advantages of Best-Worst Scaling are significant. It provides rich data on the relative importance of items, helps clarify preferences, reduces biases found in traditional rating scales, and results in utility scores that are easy to interpret. BWS is particularly useful in market research for understanding consumer preferences, in health economics for evaluating patient treatment preferences, in social sciences for studying the importance of social issues, and in product development for identifying key features driving consumer choices <span class="citation" data-cites="campbell2015">(<a href="#ref-campbell2015" role="doc-biblioref">Campbell and Erdem 2015</a>)</span>.</p>
<p>However, BWS also has limitations, including increased complexity and cognitive load for participants compared to simpler rating scales, potential scale interpretation differences among participants, and design challenges to avoid biases. Additionally, differences in how participants interpret the scale can introduce variability, and the design of BWS studies requires careful consideration to avoid biases, such as the order effect or the context in which items are presented.</p>
</section>
<section id="multiple-choice-sampling" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="multiple-choice-sampling">Multiple-Choice Sampling</h5>
<p>Multiple-choice sampling models are widely used in various fields such as voting systems, surveys, and market research to understand the preferred choice among a set of alternatives. These models involve participants selecting one option from a set of alternatives, providing insights into the most favored options.</p>
<p>Multiple-choice sampling methods offer several advantages. They are simple for participants to understand and reflect on realistic decision-making scenarios where individuals choose one option from many. These models are versatile and can be applied in various applications, from voting to market research, providing clear preferences directly from the participants’ choices. It is particularly useful in complex choice scenarios such as mode of transportation, where choices are not independent <span class="citation" data-cites="bolt2009">(<a href="#ref-bolt2009" role="doc-biblioref">Bolt and Wollack 2009</a>)</span>.</p>
<p>However, multiple-choice sampling also has limitations. It often relies on simplistic assumptions such as the independence of irrelevant alternatives (IIA), which may not always hold true. Additionally, these models can place a cognitive load on participants, especially if the number of choices is large, leading to decision fatigue. This method may also fail to capture the variation in preferences among different individuals, as it typically records only the most preferred choice without accounting for the relative importance of other options.</p>
</section>
</section>
<section id="data-interpretation" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="data-interpretation"><span class="header-section-number">2.3.2</span> Data Interpretation</h3>
<section id="binary-choice-model" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="binary-choice-model">Binary Choice Model</h5>
<p>is centered around one specific user option. The model predicts, for that option, after observing user choices in the past, whether that option will be chosen or not. Specifically, if we are looking at a certain choice, we use binary variable <span class="math inline">\(y \in \{0, 1\}\)</span> to represent whether that choice will be picked or not by the user in the next phase of selection. Since <span class="math inline">\(\Pbb(y = 0) = 1 - \Pbb(y = 1)\)</span>, we only need to model <span class="math inline">\(\Pbb(y = 1)\)</span> which we will denote as <span class="math inline">\(P\)</span>.</p>
<p>We can use a linear model represented by the parameter <span class="math inline">\(\beta\)</span> we have already defined. Since utility is a stochastic function of the choice attributes, we will represent our utility as <span class="math inline">\(U = \beta^\top z + \epsilon\)</span>. We can formally model <span class="math inline">\(y\)</span> as a function of the utility of the positive choice: <span class="math inline">\(y = \mathds{1}[U&gt;0]\)</span>.</p>
<p>We explore two cases based on the choice of distribution for the unobserved random variable <span class="math inline">\(\epsilon\)</span>. If <span class="math inline">\(\epsilon \sim \text{Logistic}\)</span>, then <span class="math inline">\(\Pbb(\epsilon &lt; a) = \frac{1}{1 + \exp^{-a}}\)</span>. The probability <span class="math inline">\(P\)</span> can be modeled as: <span class="math display">\[\begin{aligned}
    P &amp; = \Pbb(U &gt; 0) = \Pbb(\beta^\top z + \epsilon &gt; 0) = \Pbb( \epsilon &gt; -\beta^\top z) = 1 - \Pbb( \epsilon &lt; -\beta^\top z) = 1 - \frac{1}{1 + \exp^{\beta^\top z}} \\
    &amp; = \frac{1 + \exp^{\beta^\top z}}{1 + \exp^{\beta^\top z}} - \frac{1}{1 + \exp^{\beta^\top z}} = \frac{\exp^{\beta^\top z}}{1 + \exp^{\beta^\top z}} = \frac{1}{1 + \exp^{-\beta^\top z}}
\end{aligned}\]</span></p>
<p>In the health coaching example, using this logistic model, we can compute the probability that an individual would choose this meal over no meal: <span class="math inline">\(P = \frac{1}{1 + \exp^{-(4 + 15 - 9)}} = 0.99995\)</span>. Therefore, the model predicts a high probability that the user would choose the meal over the no-meal option.</p>
<p>On the other hand, if <span class="math inline">\(\epsilon \sim \Nc(0, 1)\)</span>, then <span class="math inline">\(\Pbb(\epsilon &lt; a) = \Phi(a)\)</span>, where <span class="math inline">\(\Phi(a)\)</span> is the cumulative distribution function of the standard normal distribution. The probability <span class="math inline">\(P\)</span> is modeled as:</p>
<p><span class="math display">\[P = \Pbb(U &gt; 0) = \Pbb(\beta^\top z + \epsilon &gt; 0) = \Pbb( \epsilon &gt; -\beta^\top z) = \Pbb( \epsilon &lt; \beta^\top z) = \Phi(\beta^\top z)\]</span></p>
<p>In the same health coaching example, we can compute the probability that an individual would choose this meal over no meal: <span class="math inline">\(\Phi(4 + 15 - 9) = 1\)</span>. This model also predicts that the user will most likely take the meal!</p>
</section>
<section id="bradley-terry-model" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="bradley-terry-model">Bradley-Terry Model</h5>
<p>The Bradley-Terry (BT) model introduces a framework to model the utility of choice <em>over all others</em> (a multipronged prediction of overall choices, not just a binary prediction over one choice), given their attribute vectors <span class="citation" data-cites="bradley-terry-model">(<a href="#ref-bradley-terry-model" role="doc-biblioref">Bradley and Terry 1952b</a>)</span>. Given information about all available operations, this is a general yet powerful method for modeling human preferences. The core idea in this model is to compare utilities of all items at once to model the probability of a user’s actions and, therefore, their preferences. In the BT model, we have a discrete set of <span class="math inline">\(J\)</span> choices <span class="math inline">\(i \in \{1, 2, \dots, J\}\)</span>, each with an attribute representation <span class="math inline">\(z_i \in \Zbb^n\)</span> (where <span class="math inline">\(n\)</span> is the dimensionality of the representation). Each choice can also have its unique random noise variable representing the unobserved factor, although we can also choose to have all choices’ unobserved factors follow the same distribution (e.g.&nbsp;independent and identically distributed, or iid).</p>
<p>We keep the assumption from previous sections that the utility <span class="math inline">\(U_i\)</span> of choice <span class="math inline">\(i\)</span> is also a linear stochastic function where the noise is sampled from the specified distribution: <span class="math inline">\(U_i = \beta^\top z_i + \epsilon_i\)</span>. The noise is represented as an extreme value distribution, although we can choose alternatives such as a multivariate Gaussian distribution: <span class="math inline">\(\epsilon \sim \Nc(0, \Sigma)\)</span>. If <span class="math inline">\(\Sigma\)</span> is not a diagonal matrix, we effectively model correlations in the noise across choices, enabling us to avoid the iid assumption if necessary. In the case of the extreme value distribution, we model the probability of a user preferring choice <span class="math inline">\(i\)</span>, which we denote as <span class="math inline">\(P_i\)</span> as <span class="math inline">\(P_i = \exp(\beta^\top z_i)/Z\)</span> where <span class="math inline">\(Z = \sum_{j = 1}^{J} \exp(\beta^\top z_j)\)</span>.</p>
<p>We revisit the health coaching example. Denote two choices, where <span class="math inline">\(z_1 = [20, 15, 3]\)</span> is the choice from the previous example. Still, we now have a second choice <span class="math inline">\(z_2 = [60, 20, 7]\)</span> (which seems to be a very carbohydrate-heavy meal and potentially a larger meal overall). We will also assume we choose an extreme value distribution to model the unobserved factors, which are sampled i.i.d. Then, we have <span class="math inline">\(\beta^\top z_1 = 10\)</span> and <span class="math inline">\(\beta^\top z_2 = 11 \Rightarrow P_1 = \frac{1}{1 + \exp(1)} = 0.2689\)</span>. Since there are only two choices, the probabilities <span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_2\)</span> must sum to <span class="math inline">\(1\)</span>. Therefore, we can calculate <span class="math inline">\(P_2\)</span> as <span class="math inline">\(P_2 = 1 - P_1 = 1 - 0.2689 \approx 0.7311\)</span>. Our model predicts that choice 2 is more favorable between these two options.</p>
</section>
<section id="ordered-preferences-model" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="ordered-preferences-model">Ordered Preferences Model</h5>
<p>In all previous examples, we have assumed that we have no information on any explicit ordering of the available options a human can choose from: all choices were treated as independent by the model. The model aims to capture how an individual chooses between them. However, in many cases, we may introduce an inductive bias based on information about the options. For example, in a study for stated preferences, a user may be able to choose from intricately dependent options such as very poor, poor, fair, good, and great. In this case, it can be useful to include this bias in our model to represent a human’s decision-making process better. For such cases, instead of comparing choices against alternatives, we can focus on a single example and use additional parameters to define classification criteria based on the utility determined by the model. Formally, let us suppose we have a single example with attributes <span class="math inline">\(z_i\)</span>, and wish to know which of <span class="math inline">\(J\)</span> predefined options an individual will choose from. We can define <span class="math inline">\(J - 1\)</span> parameters, which act as thresholds on the utility computed by <span class="math inline">\(U_i = H(z_i)\)</span> to classify the predicted choice between these options. For example, if there are 3 predefined options, we can define parameters <span class="math inline">\(a, b \in \Rbb\)</span> such that <span class="math display">\[y_i = \begin{cases}
      1 &amp; U &lt; a \\
      2 &amp; a \le U &lt; b \\
      3 &amp; \text{else}
   \end{cases}\]</span></p>
<p><strong>1. Logistic Distribution</strong></p>
<p>From a probabilistic perspective, we can use our cumulative distributions as before to model the probability that a person will choose a given option. Continuing with our linear utility function <span class="math inline">\(U_i = \beta^\top z_i + \epsilon_i\)</span>, we can start with the setting that we assume unobserved factors follow a logistic distribution and focus on the first case: <span class="math display">\[\Pbb(y_i = 1) = \Pbb(U &lt; a) = \Pbb(\beta^\top z + \epsilon &lt; a )  = \Pbb( \epsilon &lt; a - \beta^\top z)  = \frac{1}{1 + \exp(\beta^\top z - a)}\]</span></p>
<p>Extending this method to the second case, where we estimate the probability of the utility falling within a specific interval: <span class="math display">\[\begin{aligned}
    \Pbb(y_i = 2) &amp; = \Pbb(a \le U &lt; b) = \Pbb(a - \beta^\top z \le \epsilon &lt; b - \beta^\top z) = \frac{1}{1 + \exp(\beta^\top z - b)}  - (1 - \Pbb( \epsilon &lt; a - \beta^\top z) ) \\
    &amp; = \frac{1}{1 + \exp(\beta^\top z - b)}  - (1 - \frac{1}{1 + \exp(\beta^\top z - a)}  ) = \frac{1}{1 + \exp(\beta^\top z - b)}  - \frac{1}{1 + \exp(a - \beta^\top z)}  ) \\
\end{aligned}\]</span></p>
<p>The final case follows the form of the inverse of the first case:</p>
<p><span class="math display">\[\Pbb(y_i = 3) = \Pbb(U &gt; b) = \Pbb(\beta^\top z + \epsilon &gt; b ) = \Pbb( \epsilon &gt; b - \beta^\top z) = 1 - \Pbb( \epsilon &lt; b - \beta^\top z) = \frac{1}{1 + \exp(\beta^\top z - b)}\]</span></p>
<p><strong>2. Normal Distribution</strong></p>
<p>In the case of modeling unobserved factors with a standard normal distribution, we have: <span class="math display">\[\begin{split}
    \Pbb(y_i = 1) &amp; = \Pbb(U &lt; a) = \Pbb(\beta^\top z + \epsilon &lt; a ) = \Pbb( \epsilon &lt; a - \beta^\top z) = \Phi(a - \beta^\top z) \\
    \Pbb(y_i = 2) &amp; = \Pbb(a \le U &lt; b)
    = \Pbb(a - \beta^\top z \le \epsilon &lt; b - \beta^\top z) = \Phi(b - \beta^\top z) - \Phi(a - \beta^\top z) \\
    \Pbb(y_i = 3) &amp; = \Pbb(U &gt; b)
    = 1 - \Phi(b - \beta^\top z)
\end{split}\]</span></p>
<p>In our health coaching example, the derivation above yields three exact expressions for computing the probability of choosing each of our meals. Each computation involves the normal cumulative distribution function as seen for the binary choice model with standard normal for <span class="math inline">\(\epsilon\)</span> after parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are learned (section <a href="#sec:learning" data-reference-type="ref" data-reference="sec:learning">1.4</a>).</p>
</section>
<section id="plackett-luce-model" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="plackett-luce-model">Plackett-Luce Model</h5>
<p>In other cases, we may need an even more general framework combining elements of the BT model and ordered preferences. Specifically, we can model an open-ended ranking of the available options in a similar probabilistic framework. To do so, we can leverage the Plackett-Luce (PL) Model, in which we jointly model the full sequence of choice ordering. <span class="citation" data-cites="plackett_luce">(<a href="#ref-plackett_luce" role="doc-biblioref">Plackett 1975</a>)</span></p>
<p>The general form models the joint distribution as the product of conditional probabilities, where each is conditioned on the preceding ranking terms. Given an ordering of <span class="math inline">\(J\)</span> choices <span class="math inline">\(\{Y_1, Y_2, \dots, Y_J\}\)</span> where <span class="math inline">\(Y_1\)</span> is the first selection, <span class="math inline">\(Y_2\)</span> is the second, and so on, we decompose the joint probability into its respective conditionals. To compute the conditional probabilities, we can use the same method as the BT model, using a softmax to produce valid conditional distributions for each element of the sequence: <span class="math display">\[\Pbb(Y_1, Y_2, \dots, Y_J) = \Pbb(Y_1) \cdot \Pbb(Y_2 | Y_1) \cdot \dots \cdot \Pbb(Y_J | Y_1, Y_2, \dots Y_{J - 1}) = \prod_{i = 1}^J \frac{\exp(\beta^\top z_i)}{\sum_{j \ge i} \exp(\beta^\top z_j)}\]</span></p>
<p>An interesting property of the PL Model is that in the naive case of only ordering a single choice, it is equivalent to the pairwise preference formulation of the BT model.</p>
<p><strong>Exercise (Health coaching example)</strong>: In our application, if we have <span class="math inline">\(3\)</span> choices (burrito (B), fries (F), rice bowl (R)), we can let <span class="math inline">\(Y_1, Y_2, Y_3\)</span> be variables to which we assign meals in a one-to-one manner to establish a ranking.</p>
<ol type="1">
<li><p>One of the possible ranking assignments is <span class="math inline">\(Y_1=B, Y_2=F, Y_3=R\)</span>. How many assignments are there in all, and what are they explicitly?</p></li>
<li><p>What would one expect the sign to be, out of <span class="math inline">\(\{\leq, \geq, =\}\)</span> in the following expression? (Hint: healthier meals should be placed earlier in the ranking.) <span class="math display">\[\Pbb(Y_1=F, Y_2=R, Y_3=B) \ \ \_\_\ \ \Pbb(Y_1=R, Y_2=B, Y_3=F)\]</span></p></li>
</ol>
</section>
<section id="ideal-point-model" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="ideal-point-model">Ideal Point Model</h5>
<p>An observation one can make is that we have strictly used linear functions to represent the utility. However, in the case of vector representations of choice attributes and the individual, one can exploit vector geometry to compute this utility value. The Ideal Point Model does this by using distance functions to compute utility for individual-choice pairs <span class="citation" data-cites="huber1976ideal">(<a href="#ref-huber1976ideal" role="doc-biblioref">Huber 1976</a>)</span>. Formally, with our vector representation <span class="math inline">\(z_i\)</span> of choice <span class="math inline">\(i\)</span> and a vector <span class="math inline">\(\textbf{v}_n\)</span> representing an individual <span class="math inline">\(n\)</span>, we can use a distance function to model a stochastic utility function, keeping the notion of unobserved factors following a specified distribution: <span class="math inline">\(U_{n, i} = \texttt{dist}(z_i, \textbf{v}_n) + \epsilon_{n, i}\)</span>. We continue with our framework of a human’s preference following the choice corresponding to the maximum utility: <span class="math inline">\(y_{n, i} = \mathds{1}[U_{n, i} &gt; U_{n, j}\ \forall i \ne j]\)</span>. The intuition supporting this type of model is that vectors exist in a shared <span class="math inline">\(n\)</span>-dimensional space, and as such we can use geometry to match choices whose representations are closest to that of a given individual.</p>
<p>An observation with this model type is that it can often result in faster learning compared to non-geometric approaches <span class="citation" data-cites="ideal_point tatli2022distancepreferences">(<a href="#ref-ideal_point" role="doc-biblioref">Jamieson and Nowak 2011</a>; <a href="#ref-tatli2022distancepreferences" role="doc-biblioref">Tatli, Nowak, and Vinayak 2022</a>)</span>. However, it carries the added burden of having to specify a distance metric. Certain distance metrics, such as Euclidian distance or inner product, can easily be biased by the scale of vectors. A distance measure such as cosine similarity, which compensates for scale by normalizing the inner product of two vectors by the product of their magnitudes, can mitigate this bias yet may discard valuable information encoded by the length of the vectors. Beyond the distance metric alone, this model places a strong inductive bias that the individual and choice representations all share a common embedding space. In some contexts, this can be a robust bias to add to the model <span class="citation" data-cites="idealpoints">(<a href="#ref-idealpoints" role="doc-biblioref">Greiner 2005</a>)</span>, but it is a key factor one must take into account before employing such a model, and is a key design choice for modeling.</p>
<p><strong>Health coaching example</strong>: vector representations may indeed be useful as an individual’s representation can capture the macronutrient proportions and volumes they wish to consume, enabling a distance metric such as inner product to be a powerful tool. This model also starts capturing user properties (e.g.&nbsp;a user may be more into working out, another into lowering anxiety and another into gaining weight) and implicitly the commonalities between user characteristics start being captured, akin to a recommendation system <span class="citation" data-cites="recommender_systems">(<a href="#ref-recommender_systems" role="doc-biblioref">Roy and Dutta 2022</a>)</span>. However, in other domains and formulations, where perhaps user profiles are not as explicit, this may certainly hinder performance and make learning human preferences difficult.</p>
</section>
</section>
</section>
<section id="sec:learning" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec:learning"><span class="header-section-number">2.4</span> Parameter Learning</h2>
<p>With an understanding of the various techniques we can use to model human preferences, we can now create robust models which utilize context attributes about the options an individual has in front of them and model their choices. However, these models on their own are powerless; their parameters are initialized randomly and we must fit the models to the actual human choice data!</p>
<p>Each of the models we have studied contain distinct parameters which aim to capture human preferences; for example <span class="math inline">\(\beta\)</span> is a parameter vector containing variables which represent a linear function to compute utility given a choice’s attributes. We can also choose to represent stochastic utility functions or embedding functions for Ideal Point Models as neural networks. But how can we compute the optimal values of these parameters?</p>
<p>In this section, we give the reader an overview of the different methods available to tune human preference model parameters using given data. We refer the reader to <span class="citation" data-cites="book_estimation_casella book_estimation_bock">(<a href="#ref-book_estimation_casella" role="doc-biblioref">Casella and Berger 1990</a>; <a href="#ref-book_estimation_bock" role="doc-biblioref">Bock et al. 2015</a>)</span> for first-principle derivations of these methods and a deeper dive into their theoretical properties (convergence, generalization, data-hungriness, etc.).</p>
<p>A common and powerful approach for computing the parameters of a model is maximum likelihood estimation <span class="citation" data-cites="book_estimation_casella book_estimation_bock">(<a href="#ref-book_estimation_casella" role="doc-biblioref">Casella and Berger 1990</a>; <a href="#ref-book_estimation_bock" role="doc-biblioref">Bock et al. 2015</a>)</span>. The likelihood of a model is the probability of the observed data given the model parameters; intuitively we wish to maximize this likelihood, as that would mean that our model associates observed human preferences in the data with high probability. We can formally define the likelihood for a model with parameters <span class="math inline">\(\beta\)</span> and a given data point <span class="math inline">\((z_i, y_i)\)</span> as: <span class="math display">\[\Lc(z_i, y_i; \beta) = \Pbb(y = y_i | z_i; \beta)\]</span></p>
<p>Assuming our data is independent and identically distributed (iid), the likelihood over the entire dataset is the joint probability of all observed data as defined by the model: <span class="math display">\[\Lc(z, Y; \beta) = \prod_{i = 1}^J \Pbb(y = y_i | z_i; \beta)\]</span></p>
<p>In our very first example of binary choice with logistic noise, this was simply the model’s probability of the observed preference value: <span class="math display">\[\Lc(z_i, y_i; \beta) = \frac{1}{1 + \exp^{-\beta^\top z}}\]</span></p>
<p>In the same case with noise following a standard normal distribution, this took the form: <span class="math display">\[\Lc(z_i, y_i; \beta) = \Phi(\beta^\top z)\]</span></p>
<p>Fortunately, in these cases, there are straightforward methods for parameter estimation: logistic regression and probit regression (binary or multinomial, depending on the model), respectively. We can use ordinal regression to estimate the model’s parameters for our ordered preference model.</p>
<p>Generally, the objective function commonly found in parameter learning can be optimized with stochastic gradient descent (SGD) <span class="citation" data-cites="gradient_descent">(<a href="#ref-gradient_descent" role="doc-biblioref">Ruder 2016</a>)</span>. We can define an objective function as the likelihood to maximize this objective. Since SGD minimizes a given objective, we must negate the likelihood, which ensures that a converged solution maximizes the likelihood. SGD operates by computing the gradient of the objective with respect to the parameters of the model, which provides a signal of the direction in which the parameters must move to <em>maximize</em> the objective. Then, SGD makes an update step by subtracting this gradient from the parameters (most often with a scale factor called a <em>learning rate</em>), to move the parameters in a direction which <em>minimizes</em> the objective. When the objective is the negative likelihood (or sometimes negative log-likelihood for convenience or tractability), the result is an increase in the overall likelihood.</p>
<p>In the case of logistic and Gaussian models, SGD may yield a challenging optimization problem as its stochasticity can lead to noisy updates, for example, if certain examples or batches of examples are biased. Mitigations include batched SGD, in which multiple samples are randomly sampled from the dataset at each iteration, learning rates, which reduce the impact of noisy gradient updates, and momentum and higher-order optimizers which reduce noise by using movering averages of gradients or provide better estimates of the best direction in which to update the gradients. Some models, such as those that use neural networks, may, in fact, be intractable to estimate without a method such as SGD (or its momentum-based derivatives). For example, neural networks with many layers, non-linearities, and parameters can only be efficiently computed with gradient-based methods.</p>
<section id="reward-learning-with-large-language-models" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="reward-learning-with-large-language-models"><span class="header-section-number">2.4.1</span> Reward Learning with Large Language Models</h3>
<p>Taking a step away from explicitly modeling human bias and preference, we consider applying a deep learning approach to state-of-the-art language models. We begin by introducing the concepts of <em>foundation models</em> and <em>alignment</em>. A foundation model <span class="citation" data-cites="Liang2021">(<a href="#ref-Liang2021" role="doc-biblioref">Bommasani et al. 2021</a>)</span> in machine learning typically refers to a large and pre-trained neural network model that serves as the basis for various downstream tasks. In natural language processing, models like GPT-3, Llama, and BERT are considered foundation models. They are pre-trained on a massive corpus of text data, learning to understand language and context, and are capable of various language-related tasks such as text classification, language generation, and question answering. Foundation models are important because they alleviate the need to train massive neural networks from scratch, a compute and data expensive endeavor. However, a raw foundation model, trained on a pretraining objective such as a language modeling objective, is not useful on its own. It must be aligned to respond correctly based on human preferences.</p>
<p>In short, alignment for foundation models is the process by which model behavior is aligned with human values, ethics, and societal norms. Large Language Models (LLMs) are a foundation model for natural language processing. They are trained using a next-word prediction objective, allowing them to generate coherent language. A simple way to align a Large Language Model is to train it to follow instructions in a supervised way, using instruction-response pairs curated by hand. However, this limits the upper limit of LLM performance to the performance of the annotators’ writing abilities. This type of annotation is also expensive.</p>
<p>An alternative, more promising approach is to train LLMs using reinforcement learning, potentially enabling them to surpass human-level performance. The main challenge with this method lies in defining an explicit reward function for generating free-form text. To address this, a reward model (RM) can be trained based on human preferences, providing a mechanism to score the quality of the generated text. This approach, known as Reinforcement Learning from Human Feedback (RLHF), leverages human feedback to guide model training, allowing LLMs to better align with human expectations while continuously improving performance.</p>
<p><img src="Figures/arch.png" class="img-fluid"></p>
<p>The Llama2 reward model <span class="citation" data-cites="2307.09288">(<a href="#ref-2307.09288" role="doc-biblioref">al. 2023</a>)</span> is initialized from the pretrained Llama2 LLM. In the LLM, the last layer is a mapping <span class="math inline">\(L: \Rbb^D \rightarrow \Rbb^V\)</span>, where <span class="math inline">\(D\)</span> is the embedding dimension from the transformer decoder stack and <span class="math inline">\(V\)</span> is the vocabulary size. To get the RM, we replace that last layer with a randomly initialized scalar head that maps <span class="math inline">\(L: \Rbb^D \rightarrow \Rbb^1\)</span>. It’s important to initialize the RM from the LLM it’s meant to evaluate. This is because:</p>
<ol type="1">
<li><p>The RM will have the same “knowledge” as the LLM. This is particularly useful if evaluating things like “does the LLM know when it doesn’t know?”. However, in cases where the RM is simply evaluating helpfulness or factuality, it may be useful to have the RM know more.</p></li>
<li><p>The RM is on distribution for the LLM - it is initialized in a way where it semantically understands the LLM’s outputs.</p></li>
</ol>
<p>An RM is trained with paired preferences, following the format: <span class="math display">\[\begin{aligned}
    \langle prompt\_history, response\_accepted, response\_rejected \rangle
\end{aligned}\]</span> Prompt_history is a multiturn history of user prompts and model generations, response_accepted is the preferred final model generation by an annotator, and response_rejected is the unpreferred response. The RM is trained with a binary ranking loss with an optional margin term m(r), shown in equation (7). There is also often a small regularization term added to center the score distribution on 0. <span class="math display">\[\Lc_{\text{ranking}} = -\log(\sigma(r_\theta(x,y_c) - r_\theta(x,y_r) - m(r)))\]</span> The margin term increases the distance in scores specifically for preference pairs annotators rate as easier to separate.</p>
<div id="tab:margin_nums">
<table class="caption-top table">
<caption>Two variants of preference rating based margin with different magnitude.</caption>
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;">Significantly</td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;">Slightly</td>
<td style="text-align: center;">Negligibly</td>
</tr>
<tr class="even">
<td></td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;">Better / Unsure</td>
</tr>
<tr class="odd">
<td>Margin Small</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2/3</td>
<td style="text-align: center;">1/3</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td>Margin Large</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
<div id="fig:margin-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/margin-2.png" class="img-fluid figure-img"></p>
<figcaption><strong>Reward model score distribution shift caused by incorporating preference rating based margin in ranking loss.</strong> With the margin term, we observe a binary split pattern in reward distribution, especially for a larger margin.</figcaption>
</figure>
</div>
<p>It may seem confusing how the margins were chosen. It’s primarily because the sigmoid function, which is used to normalize the raw reward model score, flattens out beyond the range of <span class="math inline">\([-4, 4]\)</span>. Thus, the maximum possible margin is eight.</p>
<p>When training or using a reward model, watching for the following is important:</p>
<ol type="1">
<li><p><strong>LLM Distribution Shift</strong>: With each finetune of the LLM, the RM should be updated through a collection of fresh human preferences using generations from the new LLM. This ensures that the RM stays aligned with the current distribution of the LLM and avoids drifting off-distribution.</p></li>
<li><p><strong>RM and LLM are coupled</strong>: An RM is generally optimized to distinguish human preferences more efficiently within the specific distribution of the LLM to be optimized. However, this specialization poses a challenge: such an RM will underperform when dealing with generations not aligned with this specific LLM distribution, such as generations from a completely different LLM.</p></li>
<li><p><strong>Training Sensitivities of RMs</strong>: Training RMs can be unstable and prone to overfitting, especially with multiple training epochs. It’s generally advisable to limit the number of epochs during RM training to avoid this issue.</p></li>
</ol>
<p>The industry has centered around optimizing for two primary qualities in LLMs: helpfulness and harmlessness (safety). There are also other axes such as factuality, reasoning, tool use, code, multilingual, and more, but these are out of scope for us. In the Llama2 paper, preference data was collected from humans for each quality, with separate guidelines. This presents a challenge for co-optimizing the final LLM towards both goals.</p>
<p>Two main approaches can be taken for Reinforcement Learning from Human Feedback (RLHF) in this context:</p>
<ol type="1">
<li><p>Train a unified reward model that integrates both datasets.</p></li>
<li><p>Train two separate reward models, one for each quality, and optimize the LLM toward both.</p></li>
</ol>
<p>Option 1 is difficult because of the tension between helpfulness and harmlessness. They trade off against each other, confusing an RM trained on both. The chosen solution was option 2, where two RMs are used to train the LLM in a piecewise fashion. The helpfulness RM is used as the primary optimization term, while the harmlessness RM acts as a penalty term, driving the behavior of the LLM away from unsafe territory only when the LLM veers beyond a certain threshold. This is formalized as follows, where <span class="math inline">\(R_s\)</span>, <span class="math inline">\(R_h\)</span>, and <span class="math inline">\(R_c\)</span> are the safety, helpfulness, and combined reward, respectively. <span class="math inline">\(g\)</span> and <span class="math inline">\(p\)</span> are the model generation and the user prompt: <span class="math display">\[\begin{aligned}
    R_c(g \mid p) =
    \begin{cases}
        R_s(g \mid p) &amp; \text{if } \text{is\_safety}(p) \text{ or } R_s(g \mid p) &lt; 0.15 \\
        R_h(g \mid p) &amp; \text{otherwise}
    \end{cases}
\end{aligned}\]</span></p>
<p>There are several open issues with reward models alluded to in the paper. For example, how best to collect human feedback? Training annotators and making sure they do the correct thing is hard. What should the guidelines be? Another question is whether RMs can be made robust to adversarial prompts. Last but not least, do RMs have well-calibrated scores? This matters for RLHF - pure preference accuracy isn’t enough.</p>
</section>
<section id="reward-learning-in-robotics" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="reward-learning-in-robotics"><span class="header-section-number">2.4.2</span> Reward Learning in Robotics</h3>
<p>To help set up our basic reward learning problem, consider a user and a robot. The user’s preferences or goals can be represented by an internal reward function, R(<span class="math inline">\(\xi\)</span>), which the robot needs to learn. Since the reward function isn’t explicit, there are a variety of ways that the robot can learn this reward function, which we will discuss in the next section. An example method of learning a reward function from human data is using pairwise comparison. Consider the robot example from section one, but now, the robot shows the human two possible trajectories <span class="math inline">\(\xi_A\)</span> and <span class="math inline">\(\xi_B\)</span> as depicted in the diagram below.</p>
<div id="sec:reward-robot-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/robots.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>A figure showing 2 different trajectories taken by a robot to prompt user ranking.</figcaption>
</figure>
</div>
<p>The user is show both the trajectories above and asked to rank which one is better. Based on iterations of multiple trajectories and ranking, the robot is able to learn the user’s internal reward function. There quite a lot of ways that models can learn a reward function from human data. Here’s a list <span class="citation" data-cites="erdembconf2021">(<a href="#ref-erdembconf2021" role="doc-biblioref">Myers et al. 2021</a>)</span> of some of them:</p>
<ol type="1">
<li><p>Pairwise comparison: This is the method that we saw illustrated in the previous example. The robot is able to learn based on a comparison ranking provided by the user.</p></li>
<li><p>Expert demonstrations: Experts perform the task and the robot learns the optimal reward function from these demonstrations.</p></li>
<li><p>Sub-optimal demonstrations: The robot is provided with demonstrations that are not quite as good as the expert demonstrations but it is still able to learn a noisy reward function from the demonstrations.</p></li>
<li><p>Physical Corrections: While the robot is performing the task, at each point in its trajectory (or at an arbitrary point in its trajectory) its arm is corrected to a more suitable position. Based on these corrections, the robot is able to learn the reward function.</p></li>
<li><p>Ranking: This method is similar to pairwise comparison but involves more trajectories than 2. All the trajectories may have subtle differences from each other, but these differences help provide insight to the model.</p></li>
<li><p>Trajectory Assessment: Given a single trajectory, the user rates how close it is to optimal, typically using a ranking scale.</p>
<p>Each of these methods allows the robot to refine its understanding of the user’s reward function, but their effectiveness can vary depending on the application. For instance, expert demonstrations tend to produce more reliable results but may not always be feasible in everyday tasks. Pairwise comparison and ranking methods offer more flexibility but might require a higher number of iterations.</p></li>
</ol>
</section>
<section id="reward-learning-with-meta-learning" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="reward-learning-with-meta-learning"><span class="header-section-number">2.4.3</span> Reward Learning with Meta Learning</h3>
<p>Learning a reward function from human preferences is an intricate and complicated task. At its core, this task is about designing algorithms that can capture what humans value based on their elicited preferences. However, due to the nuanced and multifaceted nature of human desires, learning reward functions from human can be a difficult task. Therefore, meta-learning rewards may be considered to facilitate the reward learning processes. Meta-learning, often referred to as “learning to learn,” aims to design models that can adapt to new tasks with minimal additional efforts. We discuss paper <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> in Section <a href="#sec:few-shot" data-reference-type="ref" data-reference="sec:few-shot">[sec:few-shot]</a> showing how meta-learning can be leveraged for few-shot preference learning, where a system can quickly adapt to a new task after only a few queries to pairwise preferences from human.</p>
<p>Moving beyond the concept of learning from pairwise preferences, in Section <a href="#sec:watch" data-reference-type="ref" data-reference="sec:watch">[sec:watch]</a> we discuss a different approach where meta-learning intersects with both demonstrations and rewards <span class="citation" data-cites="zhou2019watch">(<a href="#ref-zhou2019watch" role="doc-biblioref">Zhou et al. 2019</a>)</span>. This paper considers the use of both demonstrations and rewards elicited from human that guide the learning process.</p>
<p>In the regular learning setting, a model is fitted to a dataset with certain learning algorithm. The learning algorithm, for example, can be the minimization of a loss function. To formulate the “regular” learning procedure, let’s denote the training dataset as <span class="math inline">\(D\)</span>, and the test dataset as <span class="math inline">\(S\)</span>. Given a model parameterized by <span class="math inline">\(\theta\)</span>; training loss function <span class="math inline">\(L(\theta, D)\)</span>; and test loss function <span class="math inline">\(L(\theta, S)\)</span>, we can formulate a process of “regular” machine learning process as <span class="math display">\[\begin{aligned}
    \theta^\star = \arg\min_\theta\quad L(\theta, D).
\end{aligned}\]</span> Note that the minimization of the training loss function is essentially <em>one</em> possible learning algorithm. For example, instead of minimizing the loss function, one may do gradient descent with model regularization on the loss function, where the final solution may not be the one that actually minimizes the loss function. As a result, we may want to be more general and more abstract for the moment, and denote the learning algorithm as <span class="math inline">\(\mathcal{A}\)</span>. Thus, we can write <span class="math display">\[\begin{aligned}
    \theta^\star = \mathcal{A}(D),
\end{aligned}\]</span> i.e., the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> takes in a training dataset and outputs a model parameter <span class="math inline">\(\theta^\star\)</span>. Then, the performance of the model is evaluated by the test loss <span class="math inline">\(L(\mathcal{A}(D), S)\)</span>. As we can see, in the regime of “regular” learning, the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> is pre-defined and fixed.</p>
<p>Meta-learning, or learning-to-learn, essentially asks the question of whether one can <em>learn</em> the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> from prior tasks, such that the modal can adapt to a new task more quickly/proficiently. For example, different human languages share similar ideas, and therefore a human expert who has learned many languages should be able to learn a new language easier than an average person. In other words, the human expert should have learned how to learn new languages more quickly based on their past experiences on learning languages.</p>
<p>To mathematically formulate meta-learning, we consider a family of learning algorithms <span class="math inline">\(\mathcal{A}_\omega\)</span> parameterized by <span class="math inline">\(\omega\)</span>. The “prior” tasks are represented by a set of meta-training datasets <span class="math inline">\(\{(D_i, S_i)\}_{i=1}^N\)</span> consists of <span class="math inline">\(N\)</span> pairs of training dataset <span class="math inline">\(D_i\)</span> and test dataset <span class="math inline">\(S_i\)</span>. As we noted before, a learning algorithm <span class="math inline">\(\mathcal{A}_\omega\)</span> takes in a training dataset, and outputs a model, i.e., <span class="math display">\[\begin{aligned}
    \forall i: \quad \theta^\star_i=\mathcal{A}_\omega(D_i).
\end{aligned}\]</span></p>
<p>Therefore, the <strong>meta-learning objective</strong> is <span class="math display">\[\begin{aligned}
    \min_\omega \quad \sum_{i}\ L(\mathcal{A}_\omega(D_i), S_i).
\end{aligned}\]</span> The above optimization problem gives a solution <span class="math inline">\(\omega^\star\)</span> which we use as the meta-parameter. Then, when a new task comes with a new training dataset <span class="math inline">\(D_{new}\)</span>, we can simply apply <span class="math inline">\(\theta^\star_{new}=\mathcal{A}_{\omega^\star}(D_{new})\)</span> to obtain the adapted model <span class="math inline">\(\theta^\star_{new}\)</span>. Note that we usually assume the meta-training datasets <span class="math inline">\(D_i, S_i\)</span> and the new dataset <span class="math inline">\(D_{new}\)</span> share the same underlying structure, or they come from the same distribution of datasets.</p>
<p>One of the most popular meta-learning method is Model-Agnosic Meta-Learning (MAML) <span class="citation" data-cites="finn2017model">(<a href="#ref-finn2017model" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>. In MAML, the meta-parameter <span class="math inline">\(\omega\)</span> shares the same space as the model parameter <span class="math inline">\(\theta\)</span>. At its core, in MAML the learning algorithm is defined to be <span class="math display">\[\begin{aligned}
    \mathcal{A}_\omega(D_i)=\omega-\alpha \nabla_\omega L(\omega, D_i),
\end{aligned}\]</span> where <span class="math inline">\(\alpha\)</span> is the step size. As we can see, in fact <span class="math inline">\(\omega\)</span> is defined as the initialization of fine-tuning <span class="math inline">\(\theta\)</span>. With a good <span class="math inline">\(\omega\)</span> learned, the model can adapt to a new task very quickly. In general, meta-learning can be summarized as follows: Given data from prior tasks, learn to solve a new task more quickly/proficiently. Given the general nature of meta-learning, one may be curious about whether preference learning can be benefited from meta-learning, which we discuss in the following section.</p>
<section id="sec:few-shot" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sec:few-shot">Few-Shot Preference Learning for Reinforcement Learning</h4>
<p>Reinforcement learning (RL) in robotics often stumbles when it comes to devising reward functions aligning with human intentions. Preference-based RL algorithms aim to solve this by learning from human feedback, but this often demands a <em>highly impractical number of queries</em> or leads to oversimplified reward functions that don’t hold up in real-world tasks.</p>
<p>To address the impractical requirement of human queries, as we discussed in the previous section, one may apply meta-learning so that the RL agent can adapt to new tasks with fewer human queries. <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> proposes to pre-training models on previous tasks with the meta-learning method MAML <span class="citation" data-cites="finn2017model">(<a href="#ref-finn2017model" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>, and then the meta-trained model can adapt to new tasks with fewer queries.</p>
<p>We consider Reinforcement Learning (RL) settings where a state is denoted as <span class="math inline">\(s\in S\)</span>, and action is denoted as <span class="math inline">\(a\in A\)</span>, for state space <span class="math inline">\(S\)</span> and action space <span class="math inline">\(A\)</span>. The reward function <span class="math inline">\(r:S\times A \to \Rbb\)</span> is unknown and need to be learned from eliciting human preferences. There are multiple tasks, where each task has its own reward function and transition probabilities. The reward model is parameterized by <span class="math inline">\(\psi\)</span>. We denote <span class="math inline">\(\hat{r}_\psi(s,a)\)</span> to be a learned estimate of an unknown ground-truth reward function <span class="math inline">\(r(s,a)\)</span>, parameterized by <span class="math inline">\(\psi\)</span>. Accordingly, a reward model determines a RL policy <span class="math inline">\(\phi\)</span> by maximizing the accumulated rewards. The preferences is learned via pairwise comparison of trajectory segments <span class="math display">\[\begin{aligned}
    \sigma = (s_t, a_t, s_{t+1}, a_{t+1}, ..., s_{t+k-1}, s_{t+k-1})
\end{aligned}\]</span> of <span class="math inline">\(k\)</span> states and actions.</p>
<p>For each pre-training task, there is a dataset <span class="math inline">\(D\)</span> consists of labeled queries <span class="math inline">\((\sigma_1, \sigma_2, y)\)</span> where <span class="math inline">\(y\in \{0, 1\}\)</span> is the label representing which trajectory is preferred. Therefore, a loss function <span class="math inline">\(L(\psi, D)\)</span> captures how well the reward model characterizes the preferences in dataset <span class="math inline">\(D\)</span>. In <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> they the preference predictor over segments using the Bradley-Terry model of paired comparisons <span class="citation" data-cites="bradley1952rank">(<a href="#ref-bradley1952rank" role="doc-biblioref">Bradley and Terry 1952a</a>)</span>, i.e., <span class="math display">\[\begin{aligned}
    P[\sigma_1 \succ \sigma_2 ] = \frac{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1})}{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1}) + \exp \sum_t \hat{r}_\psi(s_t^{2}, a_t^{2})}.
\end{aligned}\]</span> Then, the loss function is essentially a binary cross-entropy which the reward model <span class="math inline">\(\psi\)</span> aims to minimize, i.e., <span class="math display">\[\begin{aligned}
    {L}(\psi,  {D}) = - \mathbb{E}_{(\sigma^1, \sigma^2, y) \sim {D}} \left[ y(1) \log (P[\sigma_1 \succ \sigma_2 ]) + y(2)\log(1 - P[\sigma_1 \succ \sigma_2 ]) \right].
\end{aligned}\]</span></p>
<section id="method-component-1-pre-training-with-meta-learning" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="method-component-1-pre-training-with-meta-learning">Method Component 1: Pre-Training with Meta Learning</h5>
<p>To efficiently approximate the reward function <span class="math inline">\(r_\text{new}\)</span> for a new task with minimal queries, as described in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>, we aim to utilize a pre-trained reward function <span class="math inline">\(\hat{r}_\psi\)</span> that can be quickly fine-tuned using just a few preference comparisons. By pre-training on data from prior tasks, we can leverage the common structure across tasks to speed up the adaptation process. Although any meta-learning method is compatible, <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> opt for Model Agnostic Meta-Learning (MAML) due to its simplicity. Therefore, the pre-training update for the reward model <span class="math inline">\(\psi\)</span> is <span class="math display">\[\begin{aligned}
    \psi \xleftarrow{} \psi - \beta \nabla_\psi \sum_{i = 1}^N {L} (\psi - \alpha \nabla_\psi {L}(\psi, {D}_i), {D}_i),
\end{aligned}\]</span> where <span class="math inline">\(\alpha, \beta\)</span> are the inner and outer learning rate, respectively. We note that data <span class="math inline">\(\{D_i\}_i\)</span> of labeled preferences queries for prior tasks can come from offline datasets, simulated policies, or actual humans.</p>
</section>
<section id="method-component-2-few-shot-adaptation" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="method-component-2-few-shot-adaptation">Method Component 2: Few-Shot Adaptation</h5>
<p>With the aforementioned pre-training with meta learning, the meta-learned reward model can then be used for few-shot preference based RL during an online adaptation phase. The core procedure of the few-shot adaption is descibed as below</p>
<ol type="1">
<li><p>Given a pre-trained reward model <span class="math inline">\(\psi\)</span></p></li>
<li><p>For time step <span class="math inline">\(t=1, 2, \dots\)</span></p>
<ol type="1">
<li><p>Find pairs of trajectories <span class="math inline">\((\sigma_1, \sigma_2)\)</span> with preference uncertainty based on <span class="math inline">\(\psi\)</span>.</p></li>
<li><p>Query human preference <span class="math inline">\(y\)</span> and forms a new dataset <span class="math inline">\(D_{new}\)</span></p></li>
<li><p>Update the reward model by <span class="math inline">\(\psi'\leftarrow \psi - \alpha \nabla_\psi L(\psi, D_{new})\)</span></p></li>
<li><p>Update the policy with the new reward model <span class="math inline">\(\psi'\)</span></p></li>
</ol></li>
</ol>
<p>As mentioned in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>, uncertain queries are selected using the disagreement of an ensemble of reward functions over the preference predictors. Specifically, comparisons that maximize <span class="math inline">\(\texttt{std}(P[\sigma_1 \succ \sigma_2])\)</span> are selected each time feedback is collected.</p>
<p>The whole pipeline of the method is outlined in Figure <a href="#fig:few-1" data-reference-type="ref" data-reference="fig:few-1">1.3</a></p>
<div id="fig:few-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/overview-few.png" class="img-fluid figure-img"></p>
<figcaption>An overview of the proposed method in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>. <strong>Pre-training (left):</strong> In the pre-training phase, trajectory segment comparisons are generated using data from previously learned tasks. Then, they are used to train a reward model. <strong>Online-Adaptation (Right)</strong>: After pre-training the reward model, it is adapted to new data from human feedback. The adapted reward model is then used to train a policy for a new task in a closed loop manner.</figcaption>
</figure>
</div>
<p>We present one set of experiment from the paper, as it illustrates the effectiveness of the proposed method in a straightforward way. The experiment test the propoesed method on the Meta-World benchmark <span class="citation" data-cites="yu2020meta">(<a href="#ref-yu2020meta" role="doc-biblioref">Yu et al. 2020</a>)</span>. Three baselines are compared with the proposed method:</p>
<ol type="1">
<li><p>SAC: The Soft-Actor Critic RL algorithm trained from ground truth rewards. This represents the standard best possible method given the ground-truth reward.</p></li>
<li><p>PEBBLE: The PEBBLE algorithm <span class="citation" data-cites="lee2021pebble">(<a href="#ref-lee2021pebble" role="doc-biblioref">Lee, Smith, and Abbeel 2021</a>)</span>. It does not use information from pripor tasks.</p></li>
<li><p>Init: This method initialize the reward model with the pretained weights from meta learning. However, instead of adapting the reward model to the new task, it performs standard updates as in PEBBLE.</p></li>
</ol>
<p>The results are shown in Figure <a href="#fig:few-exp" data-reference-type="ref" data-reference="fig:few-exp">1.4</a>, where we can see that the proposed methord outperforms all of the baselines.</p>
<div id="fig:few-exp" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/few-exp.png" class="img-fluid figure-img"></p>
<figcaption>Results on MetaWorld tasks. The title of each subplot indicates the task and number of artificial feedback queries used in training. Results for each method are shown across five seeds.</figcaption>
</figure>
</div>
<p>This paper <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> shows that meta reward learning indeed reduce the number of queries of human preferences. However, as mentioned in the paper, there are still some drawbacks, as shown in the following.</p>
<p>Many of the queries the model pick for human preference elicitation are actually almost identical to human. After all, the model would pick the most uncertain pair of trajectories for human preference queries, and similar trajectories are for sure having high uncertainty in their preference. This suggest the need of new ways for designing the query selection strategy.</p>
<p>Moreover, despite the improved query complexity, it still needs an impractical amount of queries. As shown in Figure <a href="#fig:few-exp" data-reference-type="ref" data-reference="fig:few-exp">1.4</a>, the “sweep into” task still needs 2500 human queries for it to work properly, which is still not ideal for what we want them to be.</p>
<p>In addition, it is mentioned in the paper that the proposed method may be even worse than training from scratch, if the new task is too out-of-distribution. Certainly, since meta-learning assumes in-distribution tasks, we cannot expect the proposed method to be good for out-of-distribution task. It is thus an interesting future direction to investigate whether one can design a method that automatically balance between using the prior information or training from scratch.</p>
</section>
</section>
<section id="sec:watch" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sec:watch">Watch Try Learn</h4>
<p>Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards <span class="citation" data-cites="zhou2019watch">(<a href="#ref-zhou2019watch" role="doc-biblioref">Zhou et al. 2019</a>)</span> asks the question “How can we efficiently learn both from expert demonstrations and from trials where we only get <strong>binary</strong> feedback from a human". Why do we care about this question? In the context of robotics, a very compelling answer is the <em>cost of data-collection</em>. In a hypothetical world in which we have a vast number of <strong>expert demonstrations</strong> of robots accomplishing a large number of diverse tasks, we don’t necessarily need to worry about learning from trials or from humans. We could simply learn a very capable imitation agent to perform any task. Natural Language Processing could be seen as living in this world, because internet-scale data is available. <strong>Robots, however, are expensive</strong>, so people generally don’t have access to them, and therefore cannot use them to produce information to imitate. Similarly, <strong>human time is expensive</strong>, so even for large organizations that do have access to a lot of robots, it’s still hard to collect a lot of expert demonstrations.</p>
<p>The largest available collection of robotics datasets today is Open X-Embodiment (<span class="citation" data-cites="padalkar2023open">(<a href="#ref-padalkar2023open" role="doc-biblioref">Padalkar et al. 2023</a>)</span>), which consists of around 1M episodes from more than 300 different scenes. Even such large datastes are not enough to learn generally-capable robotic policies from imitation learning alone.</p>
<div id="fig:open-x-embodiment" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/open_x_embodiment.png" class="img-fluid figure-img"></p>
<figcaption>Visualization of the Open X-Embodiment dataset collection. Even this large-scale dataset for robot learning is not yet enough to learn generally-capable robotic policies.</figcaption>
</figure>
</div>
<p><strong>Main insight:</strong> binary feedback is much cheaper to obtain than expert demonstrations! Instead of hiring people to act as robot operators to tell the robot exactly what to do, if there was a way of having many robots trying things in parallel, we can have humans watch videos of what the robots did and then give a success classification of whether the robot accomplished the goal. This is a much cheaper form of human supervision because the human labels don’t necessarily need to be given in real time, so one human labeler can label many trajectories in parallel, and the human doesn’t need to be a skilled robot operator.</p>
<p>Concretely, this paper seeks to learn new tasks with the following general problem setting:</p>
<ol type="1">
<li><p>We only get 1 expert demonstration of the target task</p></li>
<li><p>After seeing the expert demonstration, we have robots try to solve the task 1 or more times.</p></li>
<li><p>The user (or some pre-defined reward function) annotates each trial as success/failure.</p></li>
<li><p>The agent learns from both the demos and the annotated trials to perform well on the target task.</p></li>
</ol>
<p>Note that this work falls under the <strong>meta-learning</strong> umbrella, because we are learning an algorithm for quickly learning new tasks given new observations (demos, trials, and success labels.)</p>
<p>The <strong>main contribution</strong> of this paper is a meta-learning algorithm for incorporating demonstrations and binary feedback from trials to solve new tasks.</p>
<p>Meta-Learning deals with efficient learning of new tasks. In the context of robotics or reinforcement learning in general, <strong>how do we define tasks</strong>? We will use the Markov decision process (<strong>MDP</strong>) formalism. A task <span class="math inline">\(T_i\)</span> is described with the tuple <span class="math inline">\(\{S, A, r_i, P_i\}\)</span>.</p>
<ol type="1">
<li><p><span class="math inline">\(S\)</span> represents the <em>state-space</em> of the task, or all possible states the agent could find itself in. This work uses image-observations, so <span class="math inline">\(S\)</span> is the space of all possible RGB images.</p></li>
<li><p><span class="math inline">\(A\)</span> is the action space, meaning the set of all possible actions the agent could take. In robotics there are many ways of representing action spaces, and this work considers end-effector positions, rotations, and opening.</p></li>
<li><p><span class="math inline">\(r_i\)</span> is the reward function for the task, with function signature <span class="math inline">\(r_i : S \times A \to \Rbb\)</span>. This work assumes all reward functions are binary.</p></li>
<li><p><span class="math inline">\(P_i\)</span> is the transition dynamics function. It’s a function that maps state-action pairs to probability distributions over next states.</p></li>
</ol>
<p>Notice that <span class="math inline">\(S\)</span> and <span class="math inline">\(A\)</span> are shared across tasks. Transition dynamics functions are normally also shared between tasks because they represent the laws of physics. However, this work considers environments with different objects, so they don’t share the dynamics function. Given this definition for tasks, they assume that the tasks from the data that they get come from some unknown task-generating distribution <span class="math inline">\(p(T)\)</span>.</p>
<p>Let’s give a more precise definition of the problem statement considered by <strong>Watch, Try, Learn</strong>. As the paper name suggests, there are 3 phases for the problem statement.</p>
<p><strong>Watch:</strong> During the <em>watch</em> phase, we give the agent <span class="math inline">\(K\)</span> demonstrations of the target tasks. This paper considers the case where <span class="math inline">\(K\)</span> always equals 1, and all demonstrations are successful. That is, each demonstration consists of a trajectory <span class="math inline">\(\{(s_0, a_0), \ldots, (s_H, a_H)\}\)</span> where <span class="math inline">\(H\)</span> is the task horizon, and the final state is always successful, that is <span class="math inline">\(r_i(s_H, a_H) = 1, r_i(s_j, a_j) = 0\)</span> for every <span class="math inline">\(j \neq H\)</span>.</p>
<p>Importantly, these demonstrations alone might not be sufficient for <strong>full task specification</strong>. As an example, consider a demonstration in which an apple is moved to the right, next to a pan. Seeing this demonstration alone, the task could be always moving the apple to the right, or it could be always moving the apple next to the pan, irrespective of where the pan is. The expected output after the Watch phase is a policy capable of gathering information about a task, given demonstrations.</p>
<p><strong>Try:</strong> In the Try phase, we use the agent learned during the Watch phase to attempt the task for <span class="math inline">\(L\)</span> trials. As specified earlier, this paper considers the casae where <span class="math inline">\(L\)</span> always equals 1. After the agent completes the trials, humans (or pre-programmed reward functions) provide one binary reward for each trial, indicating whether the trial was successful. The expected output of this phase is <span class="math inline">\(L\)</span> trajectories and corresponding feedback that hopefully <em>disambiguate</em> the task.</p>
<p><strong>Learn:</strong> After completing the trials, the agent must learn from both the original expert demonstrations and the trials, and become capable of solving the target task.</p>
<p><strong>Given Data:</strong> To train agents that can Watch, Try, and Learn, we are given a dataset of expert demonstrations containing multiple demos for each task, and the dataset contains hundreds of tasks. Importantly, <strong>no online interaction</strong> is needed for training, and this method trains only with <strong>supervised learning</strong> and no reinforcement learning.</p>
<p>This section describes exactly how this paper trains an agent from the given expert demonstrations, and how to incorporate the trials and human feedback into the loop.</p>
<p><strong>Training to Watch:</strong> We now describe the algorithm to obtain an agent conditioned on the given expert demonstration. In particular, what we want to obtain out of the Watch phase is a policy conditioned on a set of expert demonstrations. Formally, we want to obtain <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})\)</span>.</p>
<p>The way we can obtain this policy is through <strong>meta-imitation learning</strong>. Given the demonstrations <span class="math inline">\(\{\textbf{d}_{i,k}\}\)</span> for task <span class="math inline">\(i\)</span>, we sample another <em>different</em> demonstration coming from the same task <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span>. The key insight here is that <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span> is an example of <strong>optimal behavior</strong> given the demonstrations. Therefore, to obtain <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})\)</span>, we simply regress the policy to imitate actions taken on <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span>. Concretely, we train policy parameters <span class="math inline">\(\theta\)</span> to minimize the following loss:</p>
<p><span class="math inline">\(\Lc^\text{watch}(\theta, \mathcal{D}_i^*) = \mathbb{E}_{\{d_{i,k}\} \sim \mathcal{D}_i^*} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \text{\textbackslash} \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[
- \log \pi_\theta^{\text{watch}} (a_t | s_t, \{d_{i,k}\}) \big]\)</span></p>
<p>This corresponds to doing imitation learning by minimizing the negative log-likelihood of the test trajectory actions, conditioning the policy on the entire demo set. However, how is the conditioning on the demo set achieved?</p>
<div id="fig:watch-try-learn-arch" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/watch-try-learn-architecture.png" class="img-fluid figure-img"></p>
<figcaption>Vision-based policy architecture that conditions on a set of demonstrations.</figcaption>
</figure>
</div>
<p>Figure <a href="#fig:watch-try-learn-arch" data-reference-type="ref" data-reference="fig:watch-try-learn-arch">1.6</a> visualizes how Watch Try Learn deals with conditioning on demonstrations. In addition to using features obtained from the images of the current state, the architecture uses features from frames sampled (in order) from the demonstration episodes, which are concatenated together.</p>
<p><strong>Trying:</strong> On the <strong>Try</strong> phase, when the agent is given a set of demonstrations <span class="math inline">\(\{\textbf{d}_{i,k}\}\)</span>, we deploy the policy <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\})\)</span> to collect <span class="math inline">\(L\)</span> trials. There is no training involved in the Try phase, we simply condition the policy on the given demonstrations</p>
<p><strong>Training to Learn:</strong> During the Watch phase the objective was to train a policy conditioned on demonstrations <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\})\)</span>. The authors of Watch, Try, Learn use a similar strategy as the Watch phase for the Learn phase. We now want to train a policy that is conditioned on the demonstrations, as well as the trials and binary feedback. That is, we want to learn <span class="math inline">\(\pi_\phi^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\}, \{\mathbf{\tau}_{i, l}\})\)</span>. To train the policy, we again use meta-imitation learning where we additionally sample yet another trajectory from the same task. Concretely, we train policy parameters <span class="math inline">\(\phi\)</span> to minimize the following loss:</p>
<p><span class="math inline">\(\Lc^{\text{learn}}(\phi, \mathcal{D}_i, \mathcal{D}_i^*) = \mathbb{E}_{(\{d_{i,k}\}, \{\mathbf{\tau}_{i,l}\}) \sim \mathcal{D}_i} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \text{\textbackslash} \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[
- \log \pi_\theta^{\text{learn}} (a_t | s_t, \{d_{i,k}\}, \{\tau_{i,l}\}) \big]\)</span></p>
<p>The conditioning on both the demo episodes and the trial episodes is achieved in the exact same way as in the Watch phase, and is visualized in Figure <a href="#fig:watch-try-learn-arch" data-reference-type="ref" data-reference="fig:watch-try-learn-arch">1.6</a>. The architecture is simply adjusted to be able to take in more images fro mthe trial episodes.</p>
<p>In this section, we describe the evaluation suite for the paper, including the simulation benchmark used, the baselines considered, and the results.</p>
<p><strong>Gripper environment setup:</strong></p>
<div id="fig:envs" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/watch-try-learn-envs.png" class="img-fluid figure-img"></p>
<figcaption>Visualization of different tasks from the simulated benchmark for Watch Try Learn.</figcaption>
</figure>
</div>
<p>Figure <a href="#fig:envs" data-reference-type="ref" data-reference="fig:envs">1.7</a> illustrates the different task families considered in the simulated Gripper environment. Button Pressing, Grasping, Pushing, and Pick and Place. For each task family, the environment supports hundreds of different tasks by changing the objects in the scene and the objectives (e.g.&nbsp;which object to pick and where to place). For each task in each task family, a handful of expert demonstrations are given in a demonstrations dataset. As mentioned previously, the environment gives the agent image observations, and take in actions as end-effector (gripper) positions, angles, and opening.</p>
<p><strong>Baselines:</strong> The following three baselines are considered:</p>
<ol type="1">
<li><p><strong>Behavior Cloning</strong>: simple imitation learning based on maximum log-likelihood training using data from all tasks.</p></li>
<li><p><strong>Meta-imitation learning</strong>: This baseline corresponds to simply running the policy from the Watch step, without using any trial data. That is, we only condition on the set of expert demonstrations, but no online trials.</p></li>
<li><p><strong>Behavior Cloning + SAC</strong>: Pre-train a policy with Behavior Cloning on all data, and follow that with Reinforcement Learning fine-tuning for the specific target task, using the maximum-entropy algorithm SAC (<span class="citation" data-cites="haarnoja2018soft">(<a href="#ref-haarnoja2018soft" role="doc-biblioref">Haarnoja et al. 2018</a>)</span>).</p></li>
</ol>
<div id="fig:watch-try-learn-results" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/watch-try-learn-results.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Results for Watch Try Learn on the gripper control environment, and comparisons with baselines.</figcaption>
</figure>
</div>
<div id="tab:watch-try-learn-table">
<table class="caption-top table">
<caption>Table with average success rates over all tasks.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>METHOD</strong></th>
<th style="text-align: center;"><strong>SUCCESS RATE</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">BC</td>
<td style="text-align: center;">.09 <span class="math inline">\(\pm\)</span> .01</td>
</tr>
<tr class="even">
<td style="text-align: left;">MIL</td>
<td style="text-align: center;">.30 <span class="math inline">\(\pm\)</span> .02</td>
</tr>
<tr class="odd">
<td style="text-align: left;">WTL, 1 TRIAL (OURS)</td>
<td style="text-align: center;">.42 <span class="math inline">\(\pm\)</span> .02</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>RL FINE-TUNING WITH SAC</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">BC + SAC, 1500 TRIALS</td>
<td style="text-align: center;">.11 <span class="math inline">\(\pm\)</span> .07</td>
</tr>
<tr class="even">
<td style="text-align: left;">BC + SAC, 2000 TRIALS</td>
<td style="text-align: center;">.29 <span class="math inline">\(\pm\)</span> .10</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BC + SAC, 2500 TRIALS</td>
<td style="text-align: center;">.39 <span class="math inline">\(\pm\)</span> .11</td>
</tr>
</tbody>
</table>
</div>
<p>Figure <a href="#fig:watch-try-learn-results" data-reference-type="ref" data-reference="fig:watch-try-learn-results">1.8</a> shows average success rates for Watch Try Learn compared to baselines. Watch Try Learn significantly outperforms baselines on every task family. In particular, it is far superior to Behavior Cloning, which is a very weak baseline, and it significantly surpasses Meta-Imitation Learning on 3 out of 4 task families. Table <a href="#tab:watch-try-learn-table" data-reference-type="ref" data-reference="tab:watch-try-learn-table">1.3</a> includes comparison with BC fine-tuned with Reinforcement Learning. Even after 2500 online trials, SAC is not able to obtain the success rate that Watch Try Learn achieves after only 1 trial. Overall, Watch Try Learn exhibits very significant performance gains over prior methods.</p>
</section>
</section>
<section id="direct-preference-optimization" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="direct-preference-optimization"><span class="header-section-number">2.4.4</span> Direct Preference Optimization</h3>
<p>A modern method for estimating the parameters of a human preference model is direct preference optimization <span class="citation" data-cites="rafailov2023direct">(<a href="#ref-rafailov2023direct" role="doc-biblioref">Rafailov et al. 2023</a>)</span>, which is used in the context of aligning language models to human preferences. A recent approach <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span> first trains a reward model that captures human preferences and then uses proximal policy optimization to train a language model-based policy to reflect those learned preferences. Direct Preference Optimization (DPO), on the other hand, removes the need for a reward model by directly using the model likelihood of two outcomes (a preferred or highly-ranked sequence and an unpreferred or low-ranked sequence) to capture the preference represented in the data. DPO provides a simpler framework than its reinforcement learning approach and results in comparable performance with improved stability. Furthermore, it obviates the need to train a reward model, instead using a language model policy and human preference dataset to align the policy directly to human preferences.</p>
</section>
<section id="model-design-consideration" class="level3" data-number="2.4.5">
<h3 data-number="2.4.5" class="anchored" data-anchor-id="model-design-consideration"><span class="header-section-number">2.4.5</span> Model Design Consideration</h3>
<p>When designing models and learning their parameters, one must account for important tradeoffs when designing and optimizing a model to learn human preferences.</p>
<p><strong>Bias vs.&nbsp;Variance Trade-off.</strong> In modeling human preferences, we aim to ensure that predicted utilities accurately reflect overall human preferences. One key challenge is managing the bias and variance trade-off.</p>
<p>Bias refers to assumptions made during model design and training that can skew predictions. For example, in Ideal Point Models, we make the assumption that the representations we use for individuals and choices are aligned in the embedding space, and that this representation is sufficient to capture human preferences using distance metrics. However, there are myriad cases in which this may break down, for example if the two sets of vectors follow different distributions each with their own unique biases. If the representations do not come from the same domain, one may have little visibility into how a distance metric computes the final utility value for a choice for a given individual. Some ways to mitigate bias in human preference models include increasing the number of parameters in a model (allowing for better learning of patterns in the data) or removing inductive biases based on our assumptions of the underlying data.</p>
<p>On the other hand, variance refers to the model’s sensitivity to small changes in the input, which leads to significant changes in the outp ut. This phenomenon is often termed ‘overfitting’ or ‘overparameterization.’ This behavior can occur in models that have many parameters, and learn correlations in the data that do not contribute to learning human preferences, but are artifacts of noise in the dataset that one should ultimately ignore. One can address variance in models by reducing the number of parameters or incorporating biases in the model based on factors we can assume about the data.</p>
<p><strong>Model Scope.</strong> One important consideration unique to human preference models is that we wish to model individual preferences, and we may choose to do so at arbitrary granularity. For example, we can fit models to a specific individual or even multiple models for an individual, each for different purposes or contexts. On the other end of the spectrum, we may create a model to capture human preferences across large populations or the world.</p>
<p>Individual models may certainly prove to be more powerful, as they do not need to generalize across multiple individuals and can dedicate all of their parameters to learning the preferences of a single user. In the context of human behavior, this can be a significant advantage as any two individuals can be arbitrarily different or even opposite in their preferences. On the other hand, models fit only one person can tremendously overfit to the training distribution and capture noise in the data, which is not truly representative of human preferences.</p>
<p>On the end of the spectrum, models fit to the entire world may be inadequate to model human preferences for arbitrary individuals, especially those whose data it has not been fit to. As such, models may underfit the given training distribution. These models aim to generalize to many people but may fail to capture the nuances of individual preferences, especially for those whose data is not represented in the training set. As a result, they may not perform well for arbitrary individuals within the target population</p>
<p>Choosing the appropriate scope for a model is crucial. ne must balance the trade-off between overfitting to noise in highly granular models and underfitting in broader models that may not capture individual nuances.</p>
</section>
</section>
<section id="multimodal-preferences" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="multimodal-preferences"><span class="header-section-number">2.5</span> Multimodal Preferences</h2>
<p>One of the core assumptions about learning a reward function is that it is unimodal, meaning that it consists of data from one person with a certain set of preferences or a group of people with similar preferences. However, the model of unimodality often oversimplifies human preferences and their often conflicting nature. To accurately capture all the nuances of human preference, we examine a multi-modal distribution with some baseline assumptions. Consider a scenario where we, as regular drivers, make a left-hand turn at an intersection <span class="citation" data-cites="erdembconf2021">(<a href="#ref-erdembconf2021" role="doc-biblioref">Myers et al. 2021</a>)</span>. What would we do if we saw a car speeding down the road approaching us? The figure below describes some options. Following a timid driving pattern, some vehicles would stop to let the other car go, preventing a collision. Other vehicles would be more aggressive and try to make the turn before colliding with the oncoming vehicle. Given the data of one of these driving patterns, our model (our autonomous vehicle) can make an appropriate decision. However, what if our model was given data from both aggressive and timid drivers, and we don’t know which data corresponds to which type of driver? If we applied standard learning based on comparison techniques, we see, as illustrated by the figure below, that the car would have an accident trying to find a policy close enough to both driving patterns.</p>
<div id="sec:driving-patt" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/driving-patt.png" class="img-fluid figure-img"></p>
<figcaption><span class="citation" data-cites="erdembconf2021">(<a href="#ref-erdembconf2021" role="doc-biblioref">Myers et al. 2021</a>)</span> shows the possibilities of 2 different driving patterns when a car is taking a left-hand turn at an intersection and sees another car approaching head-on.</figcaption>
</figure>
</div>
<div id="sec:driving-coll" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/driving-coll.png" class="img-fluid figure-img"></p>
<figcaption>The figure <span class="citation" data-cites="erdembconf2021">(<a href="#ref-erdembconf2021" role="doc-biblioref">Myers et al. 2021</a>)</span> depicts the resultant collision when we try to find a policy close enough to both the driving patterns.</figcaption>
</figure>
</div>
<p>As illustrated by the driving example, we see that multi-modality for our reward function is extremely important and, in some cases, if it is not considered, can lead to fatal decisions <span class="citation" data-cites="erdembconf2021">(<a href="#ref-erdembconf2021" role="doc-biblioref">Myers et al. 2021</a>)</span>. But why can’t we label the groups, which would be the timid and aggressive drivers in the driving case, and then learn separate reward functions for each driver? The first problem with this approach is that it is inefficient and time-consuming to separate the data into groups because we would have to cluster and label the data. Secondly, it would not be accurate just to split the data because a more timid driver can be aggressive when they are in a hurry.</p>
<p>To formulate this problem of learning reward functions and mixing coefficients from ranking queries in a fully observable deterministic dynamical system, we begin by describing the system as a trajectory <span class="math inline">\(\xi = (s_0, a_0, ..., s_T, a_T)\)</span>, where the sequence of states and actions represents the system’s evolution over time. Assume there are <span class="math inline">\(M\)</span> different reward functions, each representing an expert’s preferences. Using the linearity assumption in reward learning, we model each expert’s reward function as a linear combination of features in a known, fixed feature space <span class="math inline">\(\phi(\xi)\)</span>. The reward for the <span class="math inline">\(m\)</span>-th expert is given by: <span class="math display">\[R_m(\xi) = \omega^T_m \phi(\xi),\]</span> where <span class="math inline">\(\omega_m\)</span> is a vector of parameters corresponding to the <span class="math inline">\(m\)</span>-th expert’s preferences. There exists an unknown distribution over the reward parameters and we can represent this distribution with mixing coefficients <span class="math inline">\(\alpha_m\)</span> such that <span class="math inline">\(\sum_M^{m = 1} \alpha_m = 1\)</span>. Our goal is to learn reward functions and mixing coefficients using ranking queries.</p>
<p>To define our problem, let’s consider a robot who performs the following trajectories and asks a user to rank all the trajectories.</p>
<div id="sec:robot-traj" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/robot-traj.png" class="img-fluid figure-img"></p>
<figcaption>The figure <span class="citation" data-cites="myers2022learning">(<a href="#ref-myers2022learning" role="doc-biblioref">Myers et al. 2022</a>)</span> depicts a few different trajectories for an example multi-modal ranking scenario.</figcaption>
</figure>
</div>
<p>The robot will be given back a set of trajectory rankings, coming from M humans and the objective is to learn the underlying reward function. We can represent the response of the ranking query as <span class="math inline">\(x = (\xi_{a_1},\ ...\ ,\xi_{a_K})\)</span> where <span class="math inline">\(a_1\)</span> is the index of the expert’s top choice, <span class="math inline">\(a_2\)</span> is the index of the expert’s second choice, ... and so on. With the response <span class="math inline">\(x\)</span>, we generate a probability distribution with the softmax rule <span class="citation" data-cites="myers2022learning">(<a href="#ref-myers2022learning" role="doc-biblioref">Myers et al. 2022</a>)</span>: <span class="math inline">\(Pr(x_1 = \xi_{a_1} | R = R_m) = \frac{e^R_m(\xi_{a_1})}{\sum_{j=1}^Ke^R_m(\xi_{a_j})}\)</span>. where <span class="math inline">\(R_m(\xi_{a_i})\)</span> denotes the reward assigned by the <span class="math inline">\(m\)</span>-th expert to trajectory <span class="math inline">\(\xi_{a_i}\)</span>. Then, we randomly sample our probability distribution to pick our top choice. From the remaining trajectories, we noisily choose from our distribution to rank our second-best option. We repeat this process until we have ranked all our trajectories. This follows what is known as the Plackett-Luce Ranking Model.</p>
<p>Given knowledge of the true reward function weights <span class="math inline">\(\omega_m\)</span> and mixing coefficients <span class="math inline">\(\alpha_m\)</span>, we have the following joint mass over observations x from a query Q: <span class="math display">\[Pr(x\ |\ Q) = \sum_{m = 1}^M \alpha_m\prod_{i = 1}^K\frac{e^{\omega_m^T \Phi(\xi_{a_i})}}{\sum_{j = i}^K e^{\omega_m^T \Phi(\xi_{a_j})}} \ \ \ \ \ \ \ \ \ \cite{myers2022learning}\]</span>.</p>
<p>With the above formulation of the joint mass distribution over observation and queries, we can now formulate an objective. Specifically, it is to present users with the best set of queries that learn reward weights, <span class="math inline">\(\omega\)</span>, and mixing coefficient, <span class="math inline">\(\alpha\)</span>, based upon user rankings of preferred query responses. By learning these parameters, we can have an accurate estimation of the joint mass distribution of the observations.</p>
<p>To learn these parameters, we use a Bayesian learning framework. The goal will be to learn the reward weights, <span class="math inline">\(\omega_m\)</span>, and all mixing coefficients <span class="math inline">\(\alpha_m\)</span>. Thus, define the parameters to be <span class="math inline">\(\theta = \{\omega, \alpha\}\)</span>. We start by simplifying the posterior over the parameters.</p>
<p><span class="math display">\[\begin{aligned}
\Pr(\Theta | Q^{(1)}, x^{(1)}, Q^{(2)}, x^{(2)}, \ldots) &amp; \propto \Pr(\Theta) \Pr(Q^{(1)} | x^{(1)}, Q^{(2)}, x^{(2)}, \ldots | \Theta) \\
&amp; = \Pr(\Theta) \prod_t \Pr(x^{(t)} | Q^{(t)}, \Theta, Q^{(1)}, x^{(1)}, \ldots, Q^{(t-1)}, x^{(t-1)}) \\
&amp; \propto \Pr(\Theta) \prod_t \Pr(x^{(t)} | \Theta, Q^{(t)})
\end{aligned}\]</span></p>
<p>Note that the first proportionality term is directly from Bayes rule (removing normalization constant). The first equation comes directly from the assumption that the queries at timestamp <span class="math inline">\(t\)</span> are conditionally independent of the parameters given previous queries &amp; rankings. This assumption is reasonable because the previous queries &amp; rankings ideally give all the information to inform the choice of the next set of. The last proportionality term comes from the assumption that the ranked queries are conditionally independent given the parameters</p>
<p>The prior distribution is dependent on use case. For example, in the user studies conducted by the authors to verify this method, they use a standard Gaussian for the reward weights and the mixing coefficients to be uniform on a <span class="math inline">\(M - 1\)</span> simplex to ensure that they add up to 1. Then we can use maximum likelihood estimation to compute the parameters with the simplified posterior.</p>
</section>
<section id="social-choices" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="social-choices"><span class="header-section-number">2.6</span> Social Choices</h2>
<p>Game theory provides a mathematical framework for analyzing strategic interactions among rational agents. These models help in understanding and predicting human behavior by considering multiple criteria and the associated trade-offs. They enhance the understanding of preferences across multiple criteria and allow for richer and more accurate feedback through structured comparisons. Game-theory framings capture the complexity of preferences and interactions in decision-making processes <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The most popular form of preference elicitation involves pairwise comparisons. Users are asked to choose between two options, such as product A or product B. This method is used in various applications like search engines, recommender systems, and interactive robotics. Key concepts include the Von Neumann Winner and the Blackwell Winner. The Von Neumann Winner refers to a distribution over objects that beats or ties every other object in the collection under the expected utility assumption. The Blackwell Winner generalizes the Von Neumann Winner for multi-criteria problems using a target set for acceptable payoff vectors <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>Game-theory framings provide a framework for preference learning along multiple criteria. These models use tools from vector-valued payoffs in game theory, with Blackwell’s approach being a key concept. This approach allows for a more comprehensive understanding of preferences by considering multiple criteria simultaneously <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>In game-theory framings, pairwise preferences are modeled as random variables. Comparisons between objects along different criteria are captured in a preference tensor <span class="math inline">\(P\)</span>. This tensor models the probability that one object is preferred over another along a specific criterion, allowing for a detailed understanding of preferences across multiple dimensions <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The preference tensor <span class="math inline">\(P\)</span> captures object comparisons along different criteria. It is defined as: <span class="math display">\[P(i_1, i_2; j) = P(i_1 \succ i_2 \text{ along criterion } j)\]</span> where <span class="math inline">\(P(i_2, i_1; j) = 1 - P(i_1, i_2; j)\)</span>. These values are aggregated to form an overall preference matrix <span class="math inline">\(P_{ov}\)</span> <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The Blackwell Winner is defined using a target set <span class="math inline">\(S\)</span> of acceptable score vectors. The goal is to find a distribution <span class="math inline">\(\pi^*\)</span> such that <span class="math inline">\(P(\pi^*, \pi) \in S\)</span> for all <span class="math inline">\(\pi\)</span>. This method minimizes the maximum distance to the target set, providing a robust solution to multi-criteria preference problems <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The optimization problem for finding the Blackwell Winner is defined as: <span class="math display">\[\pi(P, S, \|\cdot\|) = \arg \min_{\pi \in \Delta_d} \left[ \max_{\pi' \in \Delta_d} \rho(P(\pi, \pi'), S) \right]\]</span> where <span class="math inline">\(\rho(u, v) = \|u - v\|\)</span>. This measures the distance to the target set, ensuring that the selected distribution is as close as possible to the ideal preference vector <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
</section>
<section id="excercises" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="excercises"><span class="header-section-number">2.7</span> Excercises</h2>
<section id="question-1-synthetic-experiment-modeling-reward-based-on-binary-choice" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="question-1-synthetic-experiment-modeling-reward-based-on-binary-choice">Question 1: Synthetic experiment: modeling reward based on binary choice</h5>
<p>In this exercise, we want to find model parameters <span class="math inline">\(w\)</span> of a reward model of polynomial degree 5. We assume, that we can’t directly access its true parameters <span class="math inline">\(w*\)</span>. We also can’t access the reward value <span class="math inline">\(f_{w*}(x)\)</span> for a given input <span class="math inline">\(x\)</span>. We can only access a comparison of two inputs <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, leading to a binary output: <span class="math inline">\(o(x_1,x_2)=\mathbb{I}(f(x_1)&gt;f(x_2))\)</span>, where <span class="math inline">\(\mathbb{I}\)</span> is the indicator function, returning 1 if <span class="math inline">\(f(x_1)\)</span> is greater than <span class="math inline">\(f(x_2)\)</span> and zero otherwise.</p>
<ol type="1">
<li><p>How can you achieve a polynomial reward function (degree = 5) using a single linear neural network layer with 6 nodes? Implement your reward function in the code.</p></li>
<li><p>Define the likelihood of <span class="math inline">\(x_1\)</span> being chosen over <span class="math inline">\(x_2\)</span> based on that reward function. Pay attention to the case, where <span class="math inline">\(x_1 = x_2\)</span> and adapt your approach accordingly.</p></li>
<li><p>Given <span class="math inline">\(N\)</span> binary choices over <span class="math inline">\(2N\)</span> inputs, how can we find the model parameters, that maximize the likelihood of this data?</p></li>
<li><p>How is this approach realted to standard logistic regression? Can we use the Logistic regression scikit-learn toolkit for that?</p></li>
<li><p>How would a maximum a posteriori approach look like and what distribution are likelihood and prior? Is there a closed form solution for the posterior distribution?</p></li>
</ol>
<p>answer:</p>
<div class="python">
<p>import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.metrics import accuracy_score, f1_score, roc_auc_score</p>
<p>np.random.seed(42) # keep same seed for reproducability DEGREE = 5 N_TRAIN_SAMPLES = 10000 N_TEST_SAMPLES = 1000</p>
<p># Create hidden polynomial x = np.linspace(0, 10, 30) y = np.sin(x) + 0.5 * np.sin(2 * x) + np.random.normal(0, 0.2, x.shape) coefficients = np.polyfit(x, y, DEGREE) # DEGREE: 5 polynomial = np.poly1d(coefficients)</p>
<p># Create training data x1_train = np.random.rand(N_TRAIN_SAMPLES)*10 x2_train = np.random.rand(N_TRAIN_SAMPLES)*10 y1_train = polynomial(x1_train) y2_train = polynomial(x2_train) y_train = (y1_train &gt; y2_train).astype(int)</p>
<p># fit a polynomial function using logistic regression reg, _ = log_reg(x1_train, x2_train, y_train, fit = True) fit_pol = np.poly1d(reg.coef_.squeeze()[::-1])</p>
<p># Create testing data x1_test = np.random.rand(N_TEST_SAMPLES)*10 x2_test = np.random.rand(N_TEST_SAMPLES)*10 y1_test = polynomial(x1_test) y2_test = polynomial(x2_test) y_test = (y1_test &gt; y2_test).astype(int)</p>
<p># predict labels _, y_test_pred = reg.predict(x1_test, x2_test, fit = True)</p>
<p># Evaluate model acc = accuracy_score(y_test, y_test_pred) f1 = f1_score(y_test, y_test_pred) auc = roc_auc_score(y_test, y_test_pred) print("Accuracy:", acc, "-Score:", f1, "-AUC:", auc)</p>
<p># Plot the original polynomial and the recovered function x_plot = np.random.rand(1000)*10 plt.scatter(x, y, color=‘black’, label=‘Data’) plt.scatter(x_plot, polynomial(x_plot), label=‘Original Polynomial’, color=‘blue’) plt.scatter(x_plot, fit_pol(x_plot), label=‘Recovered Function’, color=‘green’) plt.figtext(0.5, -0.1, f"Original poly-params: polynomial.coef.round(2)", wrap=True, horizontalalignment=‘center’) plt.figtext(0.5, -0.15, f"Poly-params green: fit_pol.coef.round(2)", wrap=True, horizontalalignment=‘center’) plt.xlabel(‘x’) plt.ylabel(‘y’) plt.title(‘Original Polynomial vs. Recovered Function’) plt.legend() plt.show()</p>
<p>def power_features(x,deg):</p>
<p>""" This function creates power-features of an input x. Input: x: a vector x containing scalar values of length N deg: the DEGREE of the polynomail features being returned Output: X_poly: A numpy-matrix of shape N x (deg+1) containing features of x, s.t. multiplyiing w x X yields the output of a polynomial function f_w(x) with parameters w. """ ########################################################################### # ToDo: Create power features X_poly ###########################################################################</p>
<p># SOLUTION: X_poly = np.ones(len(x)) for i in range(1,deg): X_poly = np.vstack((X_poly, np.power(x, i))).T</p>
<p>###########################################################################</p>
<p>return X_poly</p>
<p>def log_reg(x1, x2, y=None, fit=False): """ fits a linear model using scikit-learn LogisticRegression model on the data, s.t. the model parameters recover the original polynomial parameters. Input: x1, x2: Two different input vectors, shape: N y: a one-hot vector indicating when x1 yields higher reward, shape: N None, if log_reg used for prediction fit: Bool variable indicating whether, or not to fit the model to the data Output: reg: A fitted logistic regression object of Class linear_model pred: predictions of the input data """</p>
<p>reg = linear_model.LogisticRegression() ########################################################################### # ToDo: Fit Logistic Regression model on data using power feature function: ###########################################################################</p>
<p># SOLUTION: X_poly1 = power_features(x1,DEGREE) X_poly2 = power_features(x2,DEGREE) x_diff = (X_poly1 - X_poly2) if fit == True: assert y is not None reg.fit(x_diff, y) pred = reg.predict(x_diff)</p>
<p>########################################################################### return reg, pred</p>
</div>


</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-2307.09288" class="csl-entry" role="listitem">
al., Hugo Touvron et. 2023. <span>“Llama 2: Open Foundation and Fine-Tuned Chat Models.”</span> <a href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a>.
</div>
<div id="ref-bhatia2020preference" class="csl-entry" role="listitem">
Bhatia, Kush, Ashwin Pananjady, Peter L. Bartlett, Anca D. Dragan, and Martin J. Wainwright. 2020. <span>“Preference Learning Along Multiple Criteria: A Game-Theoretic Perspective.”</span> <em>Neural Information Processing Systems</em> 34 (1): 1–12.
</div>
<div id="ref-2001.04465" class="csl-entry" role="listitem">
Bobu, Andreea, Dexter R. R. Scobee, Jaime F. Fisac, S. Shankar Sastry, and Anca D. Dragan. 2020. <span>“LESS Is More: Rethinking Probabilistic Models of Human Behavior.”</span> <a href="https://doi.org/10.1145/3319502.3374811">https://doi.org/10.1145/3319502.3374811</a>.
</div>
<div id="ref-book_estimation_bock" class="csl-entry" role="listitem">
Bock, Hans Georg, Thomas Carraro, Willi Jäger, Stefan Körkel, Rolf Rannacher, and Johannes P. Schlöder. 2015. <em>Model Based Parameter Estimation: Theory and Applications</em>. Springer. <a href="https://api.semanticscholar.org/CorpusID:60333071">https://api.semanticscholar.org/CorpusID:60333071</a>.
</div>
<div id="ref-bolt2009" class="csl-entry" role="listitem">
Bolt, Daniel M., and James A. Wollack. 2009. <span>“Application of a Multidimensional Nested Logit Model to Multiple-Choice Test Items.”</span> <em>Journal of Educational Measurement</em> 46 (3): 181–98. <a href="https://doi.org/10.1111/j.1745-3984.2009.00081.x">https://doi.org/10.1111/j.1745-3984.2009.00081.x</a>.
</div>
<div id="ref-Liang2021" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, et al. 2021. <span>“On the Opportunities and Risks of Foundation Models.”</span>
</div>
<div id="ref-bradley1952rank" class="csl-entry" role="listitem">
Bradley, Ralph Allan, and Milton E Terry. 1952a. <span>“Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons.”</span> <em>Biometrika</em> 39 (3/4): 324–45.
</div>
<div id="ref-bradley-terry-model" class="csl-entry" role="listitem">
Bradley, Ralph Allan, and Milton E. Terry. 1952b. <span>“Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons.”</span> <em>Biometrika</em> 39 (3/4): 324–45. <a href="http://www.jstor.org/stable/2334029">http://www.jstor.org/stable/2334029</a>.
</div>
<div id="ref-campbell2015" class="csl-entry" role="listitem">
Campbell, Danny, and Seda Erdem. 2015. <span>“Position Bias in Best-Worst Scaling Surveys: A Case Study on Trust in Institutions.”</span> <em>American Journal of Agricultural Economics</em> 97 (2): 526–45. <a href="https://doi.org/10.1093/ajae/aau112">https://doi.org/10.1093/ajae/aau112</a>.
</div>
<div id="ref-book_estimation_casella" class="csl-entry" role="listitem">
Casella, George, and Roger L. Berger. 1990. <em>Statistical Inference</em>. Springer. <a href="https://api.semanticscholar.org/CorpusID:125727004">https://api.semanticscholar.org/CorpusID:125727004</a>.
</div>
<div id="ref-cattelan2012" class="csl-entry" role="listitem">
Cattelan, Manuela. 2012. <span>“Models for Paired Comparison Data: A Review with Emphasis on Dependent Data.”</span> <em>Statistical Science</em> 27 (3): 412–33. <a href="https://doi.org/10.1214/12-STS396">https://doi.org/10.1214/12-STS396</a>.
</div>
<div id="ref-christiano2023deep" class="csl-entry" role="listitem">
Christiano, Paul, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2023. <span>“Deep Reinforcement Learning from Human Preferences.”</span> <a href="https://arxiv.org/abs/1706.03741">https://arxiv.org/abs/1706.03741</a>.
</div>
<div id="ref-finn2017model" class="csl-entry" role="listitem">
Finn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. <span>“Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.”</span> In <em>International Conference on Machine Learning</em>, 1126–35. PMLR.
</div>
<div id="ref-idealpoints" class="csl-entry" role="listitem">
Greiner, James. 2005. <span>“Ideal Points.”</span> Harvard IQSS Blog. <a href="https://blogs.iq.harvard.edu/ideal_points_1">https://blogs.iq.harvard.edu/ideal_points_1</a>.
</div>
<div id="ref-haarnoja2018soft" class="csl-entry" role="listitem">
Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. <span>“Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.”</span> In <em>International Conference on Machine Learning</em>, 1861–70. PMLR.
</div>
<div id="ref-harpe2015" class="csl-entry" role="listitem">
Harpe, Spencer E. 2015. <span>“How to Analyze Likert and Other Rating Scale Data.”</span> <em>Currents in Pharmacy Teaching and Learning</em> 7 (5): 836–50. <a href="http://dx.doi.org/10.1016/j.cptl.2015.08.001">http://dx.doi.org/10.1016/j.cptl.2015.08.001</a>.
</div>
<div id="ref-hejna2023few" class="csl-entry" role="listitem">
Hejna III, Donald Joseph, and Dorsa Sadigh. 2023. <span>“Few-Shot Preference Learning for Human-in-the-Loop Rl.”</span> In <em>Conference on Robot Learning</em>, 2014–25. PMLR.
</div>
<div id="ref-huber1976ideal" class="csl-entry" role="listitem">
Huber, Joel. 1976. <span>“Ideal Point Models of Preference.”</span> In <em>Advances in Consumer Research</em>, 03:138–42. Association for Consumer Research.
</div>
<div id="ref-ideal_point" class="csl-entry" role="listitem">
Jamieson, Kevin G, and Robert Nowak. 2011. <span>“Active Ranking Using Pairwise Comparisons.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger. Vol. 24. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf</a>.
</div>
<div id="ref-keisler2003common" class="csl-entry" role="listitem">
Keisler, H. Jerome, and Byung Soo Lee. 2003. <span>“Common Assumption of Rationality.”</span> <em>Economic Theory Journal</em> 30 (2): 123–45.
</div>
<div id="ref-lee2021pebble" class="csl-entry" role="listitem">
Lee, Kimin, Laura Smith, and Pieter Abbeel. 2021. <span>“Pebble: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-Training.”</span> <em>arXiv Preprint arXiv:2106.05091</em>.
</div>
<div id="ref-Luce1977" class="csl-entry" role="listitem">
Luce, R.Duncan. 1977. <span>“The Choice Axiom After Twenty Years.”</span> <em>Journal of Mathematical Psychology</em> 15 (3): 215–33. <a href="https://doi.org/10.1016/0022-2496(77)90032-3">https://doi.org/10.1016/0022-2496(77)90032-3</a>.
</div>
<div id="ref-miljkovic2005rational" class="csl-entry" role="listitem">
Miljkovic, Dragan. 2005. <span>“Rational Choice and Irrational Individuals or Simply an Irrational Theory: A Critical Review of the Hypothesis of Perfect Rationality.”</span> <em>The Journal of Socio-Economics</em> 34 (5): 621–34. <a href="https://doi.org/10.1016/j.socec.2003.12.031">https://doi.org/10.1016/j.socec.2003.12.031</a>.
</div>
<div id="ref-erdembconf2021" class="csl-entry" role="listitem">
Myers, Vivek, Erdem Biyik, Nima Anari, and Dorsa Sadigh. 2021. <span>“<a href="www.youtube.com/watch?v=7_dddErEMzo&amp;ab_channel=ConferenceonRobotLearning">Learning Multimodal Rewards from Rankings Presentation</a> at Conference on Robot Learning.”</span>
</div>
<div id="ref-myers2022learning" class="csl-entry" role="listitem">
———. 2022. <span>“Learning Multimodal Rewards from Rankings.”</span> In <em>Conference on Robot Learning</em>, 342–52. PMLR.
</div>
<div id="ref-VonNeumannMorgenstern1945" class="csl-entry" role="listitem">
Neumann, John Von, and Oskar Morgenstern. 1945. <em>Theory of Games and Economic Behavior</em>. Princeton, NJ: Princeton University Press.
</div>
<div id="ref-padalkar2023open" class="csl-entry" role="listitem">
Padalkar, Abhishek, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, et al. 2023. <span>“Open x-Embodiment: Robotic Learning Datasets and RT-x Models.”</span> <em>arXiv Preprint arXiv:2310.08864</em>.
</div>
<div id="ref-plackett_luce" class="csl-entry" role="listitem">
Plackett, R. L. 1975. <span>“The Analysis of Permutations.”</span> <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 24 (2): 193–202. <a href="http://www.jstor.org/stable/2346567">http://www.jstor.org/stable/2346567</a>.
</div>
<div id="ref-Radford2018GPT" class="csl-entry" role="listitem">
Radford, Alec, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. <span>“Improving Language Understanding by Generative Pre-Training.”</span> San Francisco, CA, USA.
</div>
<div id="ref-rafailov2023direct" class="csl-entry" role="listitem">
Rafailov, Rafael, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. <span>“Direct Preference Optimization: Your Language Model Is Secretly a Reward Model.”</span> <a href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a>.
</div>
<div id="ref-ragain2019" class="csl-entry" role="listitem">
Ragain, Stephen, and Johan Ugander. 2019. <span>“Choosing to Rank.”</span> <em>arXiv Preprint arXiv:1809.05139</em>. <a href="https://arxiv.org/abs/1809.05139">https://arxiv.org/abs/1809.05139</a>.
</div>
<div id="ref-recommender_systems" class="csl-entry" role="listitem">
Roy, Deepjyoti, and Mala Dutta. 2022. <span>“A Systematic Review and Research Perspective on Recommender Systems.”</span> <em>Journal of Big Data</em> 9: 1–36. <a href="https://api.semanticscholar.org/CorpusID:248508374">https://api.semanticscholar.org/CorpusID:248508374</a>.
</div>
<div id="ref-gradient_descent" class="csl-entry" role="listitem">
Ruder, Sebastian. 2016. <span>“An Overview of Gradient Descent Optimization Algorithms.”</span> <em>ArXiv</em> abs/1609.04747. <a href="https://api.semanticscholar.org/CorpusID:17485266">https://api.semanticscholar.org/CorpusID:17485266</a>.
</div>
<div id="ref-simon1972theories" class="csl-entry" role="listitem">
Simon, Herbert A. 1972. <span>“Theories of Bounded Rationality.”</span> In <em>Decision and Organization</em>, edited by C. B. McGuire and Roy Radner, 161–76. North-Holland Publishing Company.
</div>
<div id="ref-tatli2022distancepreferences" class="csl-entry" role="listitem">
Tatli, Gokcan, Rob Nowak, and Ramya Korlakai Vinayak. 2022. <span>“Learning Preference Distributions from Distance Measurements.”</span> In <em>2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</em>, 1–8. <a href="https://doi.org/10.1109/Allerton49937.2022.9929404">https://doi.org/10.1109/Allerton49937.2022.9929404</a>.
</div>
<div id="ref-yu2020meta" class="csl-entry" role="listitem">
Yu, Tianhe, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. 2020. <span>“Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning.”</span> In <em>Conference on Robot Learning</em>, 1094–1100. PMLR.
</div>
<div id="ref-zhou2019watch" class="csl-entry" role="listitem">
Zhou, Allan, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, and Chelsea Finn. 2019. <span>“Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards.”</span> In <em>International Conference on Learning Representations</em>.
</div>
</div>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./001-introduction.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./003-measure.html" class="pagination-link" aria-label="Model-Based Preference Optimization">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/002-reward_model.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>