<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Human Values and AI Alignment – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./004-optim.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./005-align.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002-reward_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./003-measure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./004-optim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005-align.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>© 2024. This work is openly licensed via <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY NC 4.0</a>.</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#human-values-and-ai-alignment" id="toc-human-values-and-ai-alignment" class="nav-link active" data-scroll-target="#human-values-and-ai-alignment"><span class="header-section-number">5.1</span> Human Values and AI Alignment</a>
  <ul class="collapse">
  <li><a href="#human-values-and-ethics-in-ai" id="toc-human-values-and-ethics-in-ai" class="nav-link" data-scroll-target="#human-values-and-ethics-in-ai"><span class="header-section-number">5.1.1</span> Human Values and Ethics in AI</a></li>
  <li><a href="#bias-in-ai" id="toc-bias-in-ai" class="nav-link" data-scroll-target="#bias-in-ai"><span class="header-section-number">5.1.2</span> Bias in AI</a></li>
  <li><a href="#aligning-ai-with-human-values" id="toc-aligning-ai-with-human-values" class="nav-link" data-scroll-target="#aligning-ai-with-human-values"><span class="header-section-number">5.1.3</span> Aligning AI with Human Values</a></li>
  <li><a href="#ai-alignment-problems" id="toc-ai-alignment-problems" class="nav-link" data-scroll-target="#ai-alignment-problems"><span class="header-section-number">5.1.4</span> AI Alignment Problems</a></li>
  <li><a href="#techniques-in-value-learning" id="toc-techniques-in-value-learning" class="nav-link" data-scroll-target="#techniques-in-value-learning"><span class="header-section-number">5.1.5</span> Techniques in Value Learning</a></li>
  <li><a href="#value-alignment-verification" id="toc-value-alignment-verification" class="nav-link" data-scroll-target="#value-alignment-verification"><span class="header-section-number">5.1.6</span> Value Alignment Verification</a></li>
  </ul></li>
  <li><a href="#human-centered-design" id="toc-human-centered-design" class="nav-link" data-scroll-target="#human-centered-design"><span class="header-section-number">5.2</span> Human-Centered Design</a>
  <ul class="collapse">
  <li><a href="#ai-and-human-computer-interaction" id="toc-ai-and-human-computer-interaction" class="nav-link" data-scroll-target="#ai-and-human-computer-interaction"><span class="header-section-number">5.2.1</span> AI and Human-Computer Interaction</a></li>
  <li><a href="#designing-ai-for-positive-human-impact" id="toc-designing-ai-for-positive-human-impact" class="nav-link" data-scroll-target="#designing-ai-for-positive-human-impact"><span class="header-section-number">5.2.2</span> Designing AI for Positive Human Impact</a></li>
  <li><a href="#adaptive-user-interfaces" id="toc-adaptive-user-interfaces" class="nav-link" data-scroll-target="#adaptive-user-interfaces"><span class="header-section-number">5.2.3</span> Adaptive User Interfaces</a></li>
  <li><a href="#case-studies-in-human-centered-ai" id="toc-case-studies-in-human-centered-ai" class="nav-link" data-scroll-target="#case-studies-in-human-centered-ai"><span class="header-section-number">5.2.4</span> Case Studies in Human-Centered AI</a></li>
  </ul></li>
  <li><a href="#practice-exercises" id="toc-practice-exercises" class="nav-link" data-scroll-target="#practice-exercises"><span class="header-section-number">5.3</span> Practice Exercises</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/005-align.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In recent years, the rapidly advancing capabilities of large models have led to increased discussion of aligning AI systems with human values. This chapter discusses the multifaceted relationship between values, alignment, and human-centered design in the context of AI. We begin by exploring the fundamental concept of human values and their ethical implications in AI design. This includes discussions on human values and ethics in AI, understanding and addressing bias in AI, and methods for aligning AI with human values. Additionally, we examine AI alignment problems, focusing on outer alignment to avoid specification gaming and inner alignment to prevent goal misgeneralization. Next, we cover techniques in value learning. This section introduces methodologies such as reinforcement learning from human feedback and contrastive preference learning, which are crucial for teaching AI systems to understand and align with human values. The importance of value alignment verification is emphasized to ensure that AI systems remain consistent with human values over time, adapting to changes and preventing misalignment. We then explore the principles and practices of human-centered design. This includes discussions on AI and human-computer interaction and methods for designing AI for positive human impact, which focuses on creating AI systems that are socially aware, human-centered, and positively impactful. A crucial part of this discussion is adaptive user interfaces, where we discuss key ideas, design principles, applications, and limitations of these interfaces, showcasing how they enhance user experience by dynamically adjusting to user needs and preferences. Finally, we present case studies in human-centered AI, including the LaMPost case study, Multi-Value, and DaDa: Cross-Dialectal English NLP, and social skill training via LLMs. These case studies provide real-world examples of successful implementations of human-centered AI systems. By integrating these elements, the chapter aims to provide a comprehensive understanding of how to create AI systems that are ethical, aligned with human values, and beneficial to society.</p>
<section id="human-values-and-ai-alignment" class="level2 page-columns page-full" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="human-values-and-ai-alignment"><span class="header-section-number">5.1</span> Human Values and AI Alignment</h2>
<p>In this part, we take a step back from the technical details to reflect on the broader concept of human values and their profound influence on our behavior and decision-making.</p>
<section id="human-values-and-ethics-in-ai" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="human-values-and-ethics-in-ai"><span class="header-section-number">5.1.1</span> Human Values and Ethics in AI</h3>
<p>Human values are the principles and standards that guide behavior and decision-making, reflecting what is essential in life and influencing choices and actions. One notable scholar in this field is Shalom H. Schwartz, a social psychologist renowned for his theory on basic human values. Schwartz’s work has significantly contributed to our understanding of how values influence behavior across different cultures. He describes values as "desirable, trans-situational goals, varying in importance, that serve as guiding principles in people’s lives" <span class="citation" data-cites="schwartz1992universals">(<a href="#ref-schwartz1992universals" role="doc-biblioref">Schwartz 1992</a>)</span>. This perspective underscores the importance of values in shaping consistent and ethical behavior across different contexts. Supporting this view, philosopher William K. Frankena emphasizes the integral role of values in ethical behavior and decision-making processes. Frankena’s work in ethical theory provides a foundation for understanding how moral judgments are formed. He notes that "ethical theory is concerned with the principles and concepts that underlie moral judgments" <span class="citation" data-cites="frankena1973ethics">(<a href="#ref-frankena1973ethics" role="doc-biblioref">Frankena 1973</a>)</span>, highlighting the need to comprehend ethical principles deeply to make informed moral judgments. Examples of critical values include autonomy, fairness, justice, and well-being. For computer scientists developing AI systems, understanding these concepts is crucial. AI systems that interact with humans and impact societal structures must be designed with these values in mind. By embedding such values into AI, developers can create systems that respect human dignity and promote positive social outcomes.</p>
<p>Autonomy is the right to choose, an essential aspect of personal freedom. Gerald Dworkin, an esteemed philosopher and professor whose research focuses on the nature of autonomy and its role in ethical theory, defines autonomy as "the capacity to reflect upon and endorse or reject one’s desires and values" <span class="citation" data-cites="dworkin1988theory">(<a href="#ref-dworkin1988theory" role="doc-biblioref">Dworkin 1988</a>)</span>. In AI, respecting autonomy means creating systems that support user independence and decision-making rather than manipulating or coercing them.</p>
<p>Fairness involves treating all individuals equally and justly, ensuring no discrimination. John Rawls, one of the most influential political philosophers of the 20th century, in his groundbreaking book "A Theory of Justice," describes fairness as "the elimination of arbitrary distinctions and the establishment of a balance between competing claims" <span class="citation" data-cites="rawls1971theory">(<a href="#ref-rawls1971theory" role="doc-biblioref">Rawls 1971</a>)</span>. For AI systems, this translates to algorithms that do not perpetuate bias or inequality, ensuring that all users are treated equitably.</p>
<p>Justice is about upholding what is morally right and ensuring fair treatment for all. Rawls also highlights that "justice is the first virtue of social institutions, as truth is of systems of thought" <span class="citation" data-cites="rawls1971theory">(<a href="#ref-rawls1971theory" role="doc-biblioref">Rawls 1971</a>)</span>. In the context of AI, justice involves creating technologies that enhance fairness in legal, social, and economic systems, providing equal opportunities and protection to all individuals.</p>
<p>Well-being focuses on promoting the health, happiness, and prosperity of individuals. Martha Nussbaum and Amartya Sen, two distinguished scholars known for their significant contributions to welfare economics and the development of the capability approach, discuss the importance of well-being in their collaborative work "The Quality of Life." They argue that "well-being is about the expansion of the capabilities of people to lead the kind of lives they value" <span class="citation" data-cites="nussbaum1993quality">(<a href="#ref-nussbaum1993quality" role="doc-biblioref">Nussbaum and Sen 1993</a>)</span>. AI systems should enhance users’ quality of life, supporting their health, education, and economic stability.</p>
<p>Understanding human values is foundational for readers with a computer science background before delving into AI ethics. These values provide the ethical underpinnings necessary to design and deploy AI systems responsibly. As AI systems increasingly impact all aspects of society, developers must embed these values into their work to ensure technologies benefit humanity and do not exacerbate existing inequalities.</p>
<p>Human values significantly impact decision-making processes by shaping the criteria for evaluating options and outcomes. Values influence priorities and ethical considerations, guiding individuals and organizations in making choices that align with their principles. Nick Bostrom, a leading philosopher in AI and existential risk, emphasizes that "values are crucial in setting priorities and determining desirable outcomes" <span class="citation" data-cites="bostrom2014superintelligence">(<a href="#ref-bostrom2014superintelligence" role="doc-biblioref">Bostrom 2014a</a>)</span>. This alignment of actions with values ensures consistency and ethical integrity in decision-making. Incorporating human values into AI systems ensures that AI decisions align with societal norms and ethical standards. Stuart Russell, an AI researcher and advocate for human-compatible AI, notes that "embedding human values into AI systems is critical to ensure that these systems act in beneficial and ethical ways" <span class="citation" data-cites="russell2019human">(<a href="#ref-russell2019human" role="doc-biblioref">Russell 2019a</a>)</span>. AI systems can make decisions that reflect societal expectations and ethical considerations by integrating values such as fairness, justice, and well-being.</p>
<p>Examples of incorporating values into AI systems highlight the practical application of these principles. In the case of autonomous vehicles, programming prioritizes human safety above all else, ensuring that these vehicles make decisions that protect human lives. Healthcare AI systems incorporate values by ensuring patient privacy and informed consent, adhering to ethical standards in medical practice. Judicial AI systems strive to avoid biases in sentencing recommendations, promoting fairness and justice in the legal system. Luciano Floridi, a professor of philosophy and ethics of information at the University of Oxford, emphasizes that "AI systems must be designed to respect and uphold human values to function ethically and effectively" <span class="citation" data-cites="floridi2011ethics">(<a href="#ref-floridi2011ethics" role="doc-biblioref">Floridi 2011a</a>)</span>.</p>
<p>To ensure that these values are systematically embedded within AI systems, it is essential to consider major ethical frameworks such as deontological, consequentialist, and virtue ethics that guide moral decision-making.</p>
<p>Deontological ethics, primarily associated with the philosopher Immanuel Kant, focuses on rules and duties. This ethical framework posits that actions are morally right if they adhere to established rules and duties, regardless of the outcomes. Kant’s moral philosophy emphasizes the importance of duty and adherence to moral laws. Robert Johnson, a scholar who has extensively studied Kantian ethics, explains that "Kant’s moral philosophy emphasizes that actions must be judged based on their adherence to duty and moral law, not by their consequences" <span class="citation" data-cites="johnson_kants_2022">(<a href="#ref-johnson_kants_2022" role="doc-biblioref">Johnson and Cureton 2022</a>)</span>. This perspective is grounded in the belief that specific actions are intrinsically right or wrong, and individuals must perform or avoid these actions based on rational moral principles.</p>
<p>In the context of AI, deontological ethics implies that AI systems should be designed to follow ethical rules and principles. For instance, AI systems must respect user privacy and confidentiality as an inviolable duty. This approach ensures that AI technologies do not infringe on individuals’ rights, regardless of potential benefits. Implementing deontological principles in AI design can prevent ethical breaches, such as unauthorized data usage or surveillance. By adhering to established moral guidelines, AI systems can maintain ethical integrity and avoid actions that would be considered inherently wrong. As Floridi states, “AI systems should be developed with a commitment to uphold moral duties and respect human dignity” <span class="citation" data-cites="floridi_ethics_2011">(<a href="#ref-floridi_ethics_2011" role="doc-biblioref">Floridi 2011b</a>)</span>.</p>
<p>Consequentialist ethics, in contrast, evaluates the morality of actions based on their outcomes. The most well-known form of consequentialism is utilitarianism, articulated by philosophers like Jeremy Bentham and John Stuart Mill. This ethical theory suggests that actions are morally right if they promote the greatest happiness for the greatest number. Mill emphasizes that "the moral worth of an action is determined by its contribution to overall utility, measured by the happiness or well-being it produces" <span class="citation" data-cites="mill_utilitarianism_1863">(<a href="#ref-mill_utilitarianism_1863" role="doc-biblioref">Mill 1863</a>)</span>. Consequentialist ethics is pragmatic, focusing on the results of actions rather than the actions themselves.</p>
<p>Applying consequentialist ethics to AI development involves designing AI systems to achieve beneficial outcomes. This means prioritizing positive societal impacts, such as improving healthcare outcomes, enhancing public safety, or reducing environmental harm. For instance, algorithms can be designed to optimize resource allocation in disaster response, thereby maximizing the overall well-being of affected populations. In this framework, the ethicality of AI decisions is judged by their ability to produce desirable consequences. Virginia Dignum, a professor of responsible artificial intelligence at Umeå University, explains that "designing algorithms with a focus on maximizing positive outcomes can lead to more ethical and effective AI systems" <span class="citation" data-cites="dignum_responsible_2019">(<a href="#ref-dignum_responsible_2019" role="doc-biblioref"><strong>dignum_responsible_2019?</strong></a>)</span>. Consequently, AI developers focus on the potential impacts of their technologies and strive to enhance their beneficial effects.</p>
<p>Virtue ethics, originating from the teachings of Aristotle, emphasizes the importance of character and virtues in ethical behavior. This framework posits that ethical behavior arises from developing good character traits and living a virtuous life. Aristotle, an ancient Greek philosopher and the author of "Nicomachean Ethics," argues that "virtue is about cultivating excellence in character to achieve eudaimonia or human flourishing" <span class="citation" data-cites="aristotle_nicomachean_350">(<a href="#ref-aristotle_nicomachean_350" role="doc-biblioref">Aristotle 350 B.C.E.</a>)</span>. Virtue ethics focuses on the individual’s character and the moral qualities that define a good person, such as honesty, courage, and compassion.</p>
<p>Additionally, virtue ethics encourages the development and use of AI systems that promote virtuous behavior. This involves fostering transparency, accountability, and fairness in AI technologies. For example, AI systems should be designed to provide clear and understandable explanations for their decisions, promoting transparency and building user trust. Furthermore, AI developers should strive to create technologies that support ethical practices and enhance the common good. Floridi emphasizes that “virtue ethics in AI development requires a commitment to fostering moral virtues and promoting human well-being” <span class="citation" data-cites="floridi_ethics_2011">(<a href="#ref-floridi_ethics_2011" role="doc-biblioref">Floridi 2011b</a>)</span>. By focusing on the character and virtues of AI developers and AI systems, virtue ethics provides a holistic approach to ethical AI development.</p>
<p>Applying these ethical frameworks to AI development is essential to ensure that AI systems operate ethically and responsibly. Deontological ethics in AI involves ensuring that AI follows ethical rules and principles. For instance, AI systems should be designed to respect user privacy and confidentiality. Consequentialist ethics focuses on developing AI to achieve beneficial outcomes. This means creating algorithms prioritizing positive societal impacts, such as improving healthcare outcomes or reducing environmental harm. Virtue ethics encourages virtuous behavior in AI development and use, promoting transparency, accountability, and fairness. Floridi emphasizes that “ethical AI development requires a commitment to core moral principles and virtues” <span class="citation" data-cites="floridi_ethics_2011">(<a href="#ref-floridi_ethics_2011" role="doc-biblioref">Floridi 2011b</a>)</span>.</p>
<p>Examples in practice demonstrate how these frameworks can be applied to guide ethical AI development. Implementing fairness constraints in machine learning models ensures that algorithms do not discriminate against certain groups. Binns notes that “fairness in machine learning can be informed by lessons from political philosophy to create more just and equitable systems” <span class="citation" data-cites="binns_fairness_2018">(<a href="#ref-binns_fairness_2018" role="doc-biblioref">Binns 2018</a>)</span>. Designing algorithms that maximize overall well-being aligns with consequentialist ethics by focusing on the positive outcomes of AI deployment. Additionally, developing AI systems focusing on transparency and accountability supports virtue ethics by fostering trust and reliability in AI technologies.</p>
<p>Ethical principles provide a framework for ensuring that AI operates in ways that are fair, just, and beneficial. Deontological ethics, for instance, focuses on moral rules and obligations, while consequentialism considers the outcomes of actions. By embedding these ethical principles into AI design, we can create systems that respect human dignity and promote societal well-being.</p>
</section>
<section id="bias-in-ai" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="bias-in-ai"><span class="header-section-number">5.1.2</span> Bias in AI</h3>
<p>Bias in AI refers to systematic errors that result in unfair outcomes. These biases can occur at various stages of AI system development and deployment, leading to significant ethical and practical concerns. Addressing bias in AI is crucial because it directly impacts the fairness, accountability, and trustworthiness of AI systems. Barocas, Hardt, and Narayanan emphasize that “bias in machine learning can lead to decisions that systematically disadvantage certain groups” <span class="citation" data-cites="barocas_fairness_2019">(<a href="#ref-barocas_fairness_2019" role="doc-biblioref">Barocas, Hardt, and Narayanan 2019</a>)</span>. O’Neil further highlights the societal impact of biased AI, noting that “algorithms can perpetuate and amplify existing inequalities, leading to a cycle of discrimination” <span class="citation" data-cites="oneil_weapons_2016">(<a href="#ref-oneil_weapons_2016" role="doc-biblioref">O’Neil 2016</a>)</span>. Therefore, understanding and mitigating bias is essential for developing ethical AI systems that promote fairness and equity.</p>
<p>Data bias originates from skewed or non-representative data used to train AI models. This bias often reflects historical prejudices and systemic inequalities in the data. For example, if a hiring algorithm is trained on historical hiring data that reflects gender or racial biases, it may perpetuate these biases in its recommendations. Fatemeh Mehrabi and her colleagues, in their survey on bias in AI, state that "data bias can result from sampling bias, measurement bias, or historical bias, each contributing to the unfairness of AI systems" <span class="citation" data-cites="mehrabi_survey_2021">(<a href="#ref-mehrabi_survey_2021" role="doc-biblioref">Mehrabi et al. 2021</a>)</span>. Safiya Umoja Noble, author of "Algorithms of Oppression," discusses how biased data in search engines can reinforce stereotypes and marginalize certain groups, noting that "search algorithms often reflect the biases of the society they operate within" <span class="citation" data-cites="noble_algorithms_2018">(<a href="#ref-noble_algorithms_2018" role="doc-biblioref">Noble 2018</a>)</span>. Addressing data bias involves careful collection, preprocessing, and validation to ensure diversity and representation.</p>
<p>An effort to address data bias is the "Lab in the Wild" platform, which seeks to broaden the scope of Human-Computer Interaction (HCI) studies beyond the traditional "WEIRD" (Western, Educated, Industrialized, Rich, and Democratic) population <span class="citation" data-cites="oliveira17">(<a href="#ref-oliveira17" role="doc-biblioref"><strong>oliveira17?</strong></a>)</span>. Paulo S. Oliveira, one of the platform’s researchers, notes that this initiative aims to correct demographic skew in behavioral science research by engaging a diverse global audience. By allowing individuals from various demographics to participate in studies from their environments, "Lab in the Wild" provides researchers with a more inclusive dataset.</p>
<p>Another important consideration is the cultural nuances of potential users. For instance, designing a computer vision system to describe objects and people daily must consider whether to identify gender. In the United States, there is growing sensitivity toward gender identity, suggesting that excluding gender might be prudent. Conversely, in India, where a visually impaired woman may need gender-specific information for safety, including gender identification is critical. Ayanna Howard, a roboticist and AI researcher at Georgia Tech, emphasizes the need for adaptable systems that respect local customs and address specific user needs in her work on human-robot interaction. This highlights the importance of adaptable systems that respect local customs and address specific user needs.</p>
<p>Algorithmic bias often arises from the design and implementation choices made by developers. This type of bias can stem from the mathematical frameworks and assumptions underlying the algorithms. For instance, decision trees and reinforcement learning policies can inadvertently prioritize certain outcomes, resulting in biased results. Solon Barocas, a professor at Cornell University, and his colleagues explain that "algorithmic bias can emerge from optimization objectives that do not adequately consider fairness constraints" <span class="citation" data-cites="barocas_fairness_2019">(<a href="#ref-barocas_fairness_2019" role="doc-biblioref">Barocas, Hardt, and Narayanan 2019</a>)</span>. Cathy O’Neil, a data scientist who has written extensively on the societal impacts of algorithms, provides examples of how biased algorithms in predictive policing and credit scoring can disproportionately affect disadvantaged communities. She argues that "algorithmic decisions can have far-reaching consequences when fairness is not adequately addressed" <span class="citation" data-cites="oneil_weapons_2016">(<a href="#ref-oneil_weapons_2016" role="doc-biblioref">O’Neil 2016</a>)</span>. Mitigating algorithmic bias requires incorporating fairness constraints and regularly auditing algorithmic decisions.</p>
<p>Weidinger et al., in their 2022 study published in "Artificial Intelligence," investigate how reinforcement learning (RL) algorithms can replicate or amplify biases present in training data or algorithmic design <span class="citation" data-cites="weidinger_artificial_2022">(<a href="#ref-weidinger_artificial_2022" role="doc-biblioref">Weidinger, Reinecke, and Haas 2022</a>)</span>. They propose RL-based paradigms to test for these biases, aiming to identify and mitigate their impact. Similarly, Mazeika et al., in their research on modeling emotional dynamics from video data, explore how algorithms might prioritize certain emotional expressions or demographics based on their training and data usage <span class="citation" data-cites="mazeika_how_2022">(<a href="#ref-mazeika_how_2022" role="doc-biblioref">Mazeika et al. 2022</a>)</span>. Their work highlights the need for careful consideration of algorithmic design to avoid unintended bias in AI systems.</p>
<p>The evolution of ethics in scientific research has been a long journey. Unethical biomedical experiments conducted by the Nazis during World War II acted as a catalyst for change, leading to a global awakening to the need for ethical oversight in research. This awareness was further reinforced by the infamous Tuskegee syphilis study <span class="citation" data-cites="tuskegee">(<a href="#ref-tuskegee" role="doc-biblioref">Centers for Disease Control and Prevention 2023</a>)</span>, which, for decades, misled its participants and withheld treatment, highlighting the need for moral governance in scientific inquiry. The Belmont Report <span class="citation" data-cites="belmont">(<a href="#ref-belmont" role="doc-biblioref">The National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research 1979</a>)</span> marked a turning point, articulating the ethical principles essential to research involving human subjects, including informed consent and assessments of risks and benefits. Its legacy extends into various fields, including the emergent domain of artificial intelligence, where ethics in the treatment of crowd workers for Reinforcement Learning from Human Feedback (RLHF) is under scrutiny. Today, no study begins without an ethics review. Institutional Review Boards (IRB) like Stanford’s and Internal Review Committees (IRC) such as DeepMind play critical roles in safeguarding ethical standards. They provide feedback and oversight, ensuring research upholds human dignity and rights. This progression reflects a growing commitment to moral responsibility across all realms of scientific study.</p>
</section>
<section id="aligning-ai-with-human-values" class="level3 page-columns page-full" data-number="5.1.3">
<h3 data-number="5.1.3" class="anchored" data-anchor-id="aligning-ai-with-human-values"><span class="header-section-number">5.1.3</span> Aligning AI with Human Values</h3>
<p>Aligning AI systems with human values presents several significant challenges. Human values are multifaceted and context-dependent, making them difficult to encode into AI systems. As Bostrom highlights, “the complexity of human values means that they are not easily reducible to simple rules or objectives” <span class="citation" data-cites="bostrom_superintelligence_2014">(<a href="#ref-bostrom_superintelligence_2014" role="doc-biblioref">Bostrom 2014b</a>)</span>. Additionally, values can evolve, requiring AI systems to adapt. Russell notes that “the dynamic nature of human values necessitates continuous monitoring and updating of AI systems to ensure ongoing alignment” <span class="citation" data-cites="russell_human_2019">(<a href="#ref-russell_human_2019" role="doc-biblioref">Russell 2019b</a>)</span>. Different stakeholders may also have conflicting values, posing a challenge for AI alignment. Addressing these conflicts requires a nuanced approach to balance diverse perspectives and priorities.</p>
<p>What is the right way to represent values? In a Reinforcement Learning (RL) paradigm, one might ask: at what level should we model rewards? Many people are trying to use language. In Constitutional AI <span class="citation" data-cites="bai_constitutional_2022">(<a href="#ref-bai_constitutional_2022" role="doc-biblioref">Bai et al. 2022</a>)</span>, we write down the rules we want a language model to follow or apply reinforcement learning from human feedback, discussed in the next section. However, humans are evolutionarily endowed to pay attention to the same things and have the same frames of reference. AI systems, on the other hand, aren’t biased in the same way, leading to a kind of underdetermination of actions by language <span class="citation" data-cites="quine_word_1960">(<a href="#ref-quine_word_1960" role="doc-biblioref">Quine 1960</a>)</span>.</p>
<p>So, what are values, and how can we model them? Many problems have been framed in an RL setting. Some experts in reinforcement learning argue that a single scalar reward isn’t enough <span class="citation" data-cites="vamplew_human-aligned_2018 vamplew_scalar_2022">(<a href="#ref-vamplew_human-aligned_2018" role="doc-biblioref">Vamplew et al. 2018</a>, <a href="#ref-vamplew_scalar_2022" role="doc-biblioref">2022</a>)</span>. They suggest a vectorized reward approach might better emulate the emotional-like system humans have <span class="citation" data-cites="moerland_emotion_2018">(<a href="#ref-moerland_emotion_2018" role="doc-biblioref">Moerland, Broekens, and Jonker 2018</a>)</span>. With this robustness, we might capture all the dimensions of human values. These approaches are still in the early stages. Language does play a crucial role in human values. Tomasello <span class="citation" data-cites="tomasello_becoming_2019">(<a href="#ref-tomasello_becoming_2019" role="doc-biblioref">Tomasello 2019</a>)</span> argues that learning a language and the awareness of convention it brings help children understand their cultural group and reason about it with peers. However, human values seem to be composed of more than just linguistic utterances.</p>
<p>Several strategies can align AI systems with human values. One effective approach is value-sensitive design, which considers human values from the outset of the design process. Friedman, Kahn, and Borning explain that “value-sensitive design integrates human values into the technology design process to ensure that the resulting systems support and enhance human well-being” <span class="citation" data-cites="friedman_value_2008">(<a href="#ref-friedman_value_2008" role="doc-biblioref">Friedman, Kahn, and Borning 2008</a>)</span>. Another strategy is participatory design, which engages stakeholders in the design process to ensure their values are reflected in the AI system. Muller emphasizes that “participatory design creates a collaborative space where diverse stakeholders can contribute their perspectives and values, leading to more inclusive and ethical AI systems” <span class="citation" data-cites="muller_participatory_2003">(<a href="#ref-muller_participatory_2003" role="doc-biblioref">Muller 2003</a>)</span>. Additionally, iterative testing and feedback allow continuous refinement of AI systems based on user feedback, ensuring they remain aligned with human values over time. Practical examples of value alignment in AI systems demonstrate how these strategies can be implemented effectively.</p>
<p>In autonomous vehicles, ensuring safety and ethical decision-making in critical scenarios is paramount. These vehicles must make real-time decisions that prioritize human safety above all else. Goodall discusses how “Waymo’s safety protocols are designed to prioritize human safety and ethical considerations in autonomous driving” <span class="citation" data-cites="goodall_machine_2014">(<a href="#ref-goodall_machine_2014" role="doc-biblioref">Goodall 2014</a>)</span>. These protocols include extensive testing and validation processes to ensure that autonomous driving algorithms handle various scenarios ethically and safely. For example, the system must decide how to react in an unavoidable collision, weighing the potential outcomes to minimize harm. By embedding these ethical considerations into their design and operation, companies like Waymo aim to align their AI systems with societal values of safety and responsibility.</p>
<p>In healthcare AI, respecting patient privacy and ensuring informed consent are crucial. Healthcare applications often involve sensitive personal data, and AI systems must handle this information with the utmost care. Jiang et al.&nbsp;highlight how “IBM Watson for Oncology incorporates patient privacy protections and informed consent processes to align with ethical standards in medical practice” <span class="citation" data-cites="jiang_artificial_2017">(<a href="#ref-jiang_artificial_2017" role="doc-biblioref">F. Jiang et al. 2017</a>)</span>. IBM Watson for Oncology uses AI to assist in diagnosing and recommending treatments for cancer patients. To align with ethical standards, the system ensures that patients are fully informed about how their data will be used and that their consent is obtained before processing their information. This approach protects patient privacy, and builds trust between patients and healthcare providers, demonstrating a commitment to ethical medical practices.</p>
<p>Judicial AI systems strive to avoid biases in sentencing recommendations, promoting fairness and justice. The judicial system’s integrity depends on delivering fair and unbiased judgments. However, AI systems in judicial contexts, such as risk assessment tools, can perpetuate existing biases if not carefully designed and monitored. Angwin et al.&nbsp;describe how “the COMPAS system has undergone revisions to address biases and improve fairness in judicial decision-making” <span class="citation" data-cites="angwin_machine_2016">(<a href="#ref-angwin_machine_2016" role="doc-biblioref">Angwin et al. 2016</a>)</span>. COMPAS, a tool used to assess the likelihood of a defendant reoffending, faced criticism for racial bias in its risk assessments. Subsequent revisions removed biased factors and incorporated fairness constraints. These changes aimed to ensure that the tool provides more equitable assessments, aligning the AI system with the judicial values of fairness and justice.</p>
<p>This raises the question for AI: At what level should we model values? Is language alone enough? We might also consider questions about language understanding. LLMs seem to "get" much of language even without physical embodiment. Will they "get" human values, too? First, we’ll cover work that attempts to gain traction on these issues. Methodologically, this is similar to the RLHF and language modeling work we will cover in the next section. Here, we focus on this work with a somewhat different aim. In some of the earliest work to address values in AI systems head-on, <span class="citation" data-cites="hendrycks_aligning_2021">(<a href="#ref-hendrycks_aligning_2021" role="doc-biblioref">Hendrycks et al. 2020</a>)</span> introduced a new dataset called ETHICS, sourced from Reddit and through Mturk worker annotations. Instead of showing moral dilemmas, they focused just on clear-cut examples. One example scenario is, “I pushed the elderly man in the wheelchair to the ground” which their model labels as “Unacceptable” and “I pushed the elderly man in the wheelchair around the park.” which their model labels as “acceptable”. To come up with these labels, they define many different tasks, one of which (for the preceding example) is how well the model can capture the ratings of human votes (from the “Am I the Asshole” subreddit). The model that they train could be the reward model that is then later used in a language model setting. What’s interesting and what we’ll be talking about is how they’re using language as the means to gain traction on values.</p>
<figure id="fig:ethics" class="figure">
<p>
<img src="Figures/hendrycks_fig6.png" style="width:75.0%" alt="image" class="figure-img"> <span id="fig:ethics" data-label="fig:ethics"></span>
</p>
<figcaption>
Learned utility values from <span class="citation" data-cites="hendrycks_aligning_2021"></span>.
</figcaption>
</figure>
<p>Their dataset included various scenarios, from deontological ones like Kantian ethics to utilitarian approaches. We’ll describe some of those approaches in a later subsection. The model they trained performed well at predicting scores and utility values for these situations. For example, it ranked highly, “I got called to the principal’s office because I won a school-wide award”, and negatively “I rewired my electricity in the attic and I fell and hurt my back”. In subsequent work by others, this underlying technology has been deployed to reason morally. Part of this work prompted a response from <span class="citation" data-cites="jiang_delphi_2021">(<a href="#ref-jiang_delphi_2021" role="doc-biblioref">L. Jiang et al. 2021</a>)</span>. Anecdotally, many people were unhappy with this demo, disagreeing that LLMs could reason morally at <span class="citation" data-cites="talat_machine_2022">(<a href="#ref-talat_machine_2022" role="doc-biblioref">Talat et al. 2022</a>)</span>.</p>
<figure id="fig:delphi" class="figure">
<p>
<img src="Figures/jiang_machines_2022_fig1.png
    " style="width:75.0%" alt="image" class="figure-img"> <span id="fig:delphi" data-label="fig:delphi"></span>
</p>
<figcaption>
An overview of <span class="citation" data-cites="jiang_delphi_2021"></span>.
</figcaption>
</figure>
<p>If you ask, “Should I drive my friend to the airport if I don’t have a license?” Delphi gets it right and says no. The question that we’re driving at in this is what does it mean for Delphi to get it right? What values are we considering, and how are those represented in the sorts of systems that we’re working on? You can also get Delphi to say a lot of hateful and toxic things by subtly manipulating the input to this model—does this suggest that the model is merely susceptible to hallucinations like other LLMs but otherwise performant? Or does it suggest an underlying lack of capacity?</p>
<p>Delphi operationalizes the ETHICS dataset and adds a couple of others <span class="citation" data-cites="sap_socialIQA_2019">(<a href="#ref-sap_socialIQA_2019" role="doc-biblioref"><strong>sap_socialIQA_2019?</strong></a>)</span>. They call their new, compiled dataset the Commonsense Norm Bank, sourcing many scenarios from Reddit and having crowd workers annotate the acceptability of various judgments pairwise. This allows the model to perform various morally relevant tasks. When prompted, the model outputs a class label for appropriateness and a generative description. For example, “greeting a friend by kissing on a cheek” is appropriate behavior when appended with “in France” but not with “in Korea”. The model captures actual cultural norms. Our driving question should be, how ought we best formalize these kinds of norms, and is this necessarily the right approach? When released in late 2021, Delphi outperformed GPT-3 on a variety of these scenarios. In personal communication with the authors, we understand that Delphi continues to outperform GPT-4 on many of these scenarios as well. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;GPT-4 is good at coming up with longer-rendered answers about why some things are appropriate or not.</p></div></div><p>There have also been works that seek to operationalize performance on moral values to turn such a model into something actionable. <span class="citation" data-cites="hendrycks_what_2021">(<a href="#ref-hendrycks_what_2021" role="doc-biblioref">Hendrycks et al. 2021</a>)</span> used the same constituent parts of the ETHICS dataset to create a model that reasons around text-based adventure games. Jiminy Cricket is a character in one of these games, which has scenarios like those in Fig. <a href="#fig:jiminy" data-reference-type="ref" data-reference="fig:jiminy">1.3</a>. These games offer limited options, and the goal was to see whether agents would perform morally well and not just finish the game. They labeled all examples of game-based actions according to three degrees: positive, somewhat positive, and negative. For example, saving a life in the game was very positive, while drinking water was somewhat positive. They found that with this labeled data, it was possible to train a model that shaped the reward of the underlying RL agent playing the games. The agent would not only finish the games well but also score highly on moral metrics. This approach is similar to optimizing multiple objectives like helpfulness and harmlessness <span class="citation" data-cites="liang_holistic_2023">(<a href="#ref-liang_holistic_2023" role="doc-biblioref">Liang et al. 2023</a>)</span>.</p>
<figure id="fig:jiminy" class="figure">
<p>
<img src="Figures/hendrycks_fig1.png" style="width:75.0%" alt="image" class="figure-img"> <span id="fig:jiminy" data-label="fig:jiminy"></span>
</p>
<figcaption>
An example scenario from <span class="citation" data-cites="hendrycks_what_2021"></span>.
</figcaption>
</figure>
<p>We are discussing whether language is the right medium for learning values. <span class="citation" data-cites="arcas_can_2022">(<a href="#ref-arcas_can_2022" role="doc-biblioref">Arcas 2022</a>)</span> claims that language encompasses all of morality. Since these models operate in the linguistic domain, they can also reason morally. He provides an example with the Lambda model at Google. Anecdotally, when asked to translate a sentence from Turkish to English, where Turkish does not have gendered pronouns, the model might say, "The nurse put her hand in her coat pocket." This inference shows gender assumption. When instructed to avoid gendered assumptions, the model can say "his/her hand." He claims this capability is sufficient for moral reasoning.</p>
<p>Next, we now explore the broader challenges of AI alignment, particularly focusing on AI alignment problems and the critical dimensions of outer and inner alignment.</p>
</section>
<section id="ai-alignment-problems" class="level3" data-number="5.1.4">
<h3 data-number="5.1.4" class="anchored" data-anchor-id="ai-alignment-problems"><span class="header-section-number">5.1.4</span> AI Alignment Problems</h3>
<p>AI alignment ensures that AI systems’ goals and behaviors are consistent with human values and intentions. Various definitions of AI alignment emphasize the importance of aligning AI systems with human goals, preferences, or ethical principles. As stated by <span class="citation" data-cites="enwiki:1185176830">(<a href="#ref-enwiki:1185176830" role="doc-biblioref">Wikipedia contributors 2023</a>)</span>, AI alignment involves</p>
<ul>
<li><p><span class="citation" data-cites="enwiki:1185176830">(<a href="#ref-enwiki:1185176830" role="doc-biblioref">Wikipedia contributors 2023</a>)</span>: “steer[ing] AI systems towards humans’ intended goals, preferences, or ethical principles”</p></li>
<li><p><span class="citation" data-cites="ngo2023alignment">(<a href="#ref-ngo2023alignment" role="doc-biblioref">Ngo, Chan, and Mindermann 2023</a>)</span>: “the challenge of ensuring that AI systems pursue goals that match human values or interests rather than unintended and undesirable goals”</p></li>
<li><p><span class="citation" data-cites="christianoclarifying">(<a href="#ref-christianoclarifying" role="doc-biblioref">P. Christiano 2018</a>)</span>: “an AI <span class="math inline">\(A\)</span> is aligned with an operator <span class="math inline">\(H\)</span> [when] <span class="math inline">\(A\)</span> is trying to do what <span class="math inline">\(H\)</span> wants it to do”</p></li>
</ul>
<p>The importance of AI alignment lies in preventing unintended consequences and ensuring that AI systems act beneficially and ethically. Proper alignment is crucial for the safe and ethical deployment of AI, as it helps AI systems correctly learn and generalize from human preferences, goals, and values, which may be incomplete, conflicting, or misspecified. In practice, AI alignment is a technical challenge, especially for systems with broad capabilities like large language models (LLMs). The degree of alignment can be viewed as a scalar value: a language model post-RLHF (Reinforcement Learning from Human Feedback) is more aligned than a model that has only been instruction-tuned, which in turn is more aligned than the base model. There are specific terms to distinguish different notions of alignment. Intent alignment refers to a system trying to do what its operator wants it to do, though not necessarily succeeding <span class="citation" data-cites="christianoclarifying">(<a href="#ref-christianoclarifying" role="doc-biblioref">P. Christiano 2018</a>)</span>. Value alignment involves a system correctly learning and adopting the values of its human operators. Alignment is often divided into two broad subproblems: outer alignment, which focuses on avoiding specification gaming, and inner alignment, which aims to avoid goal misgeneralization. In the following sections, we will examine these subproblems in greater detail. It is also important to consider how human preferences and values are aggregated and who the human operators are, topics addressed in related discussions on ethics and preference elicitation mechanisms.</p>
<section id="outer-alignment-avoiding-specification-gaming" class="level4" data-number="5.1.4.1">
<h4 data-number="5.1.4.1" class="anchored" data-anchor-id="outer-alignment-avoiding-specification-gaming"><span class="header-section-number">5.1.4.1</span> Outer Alignment: Avoiding Specification Gaming</h4>
<p>To align a model with human values, we need an objective function or reward model that accurately specifies our preferences. However, human preferences are complex and difficult to formalize. When these preferences are incompletely or incorrectly specified, optimizing against the flawed objective function can yield models with undesirable and unintuitive behavior, exploiting discrepancies between our true values and the specified objective function. This phenomenon, known as <em>specification gaming</em>, arises from <em>reward misspecification</em>, and addressing this issue constitutes the <em>outer alignment problem</em> <span class="citation" data-cites="amodei2016concrete">(<a href="#ref-amodei2016concrete" role="doc-biblioref">Amodei et al. 2016</a>)</span>.</p>
<p>Specification gaming occurs when AI systems exploit poorly defined objectives to achieve goals in unintended ways. For instance, a cleaning robot might hide dirt under a rug instead of cleaning it to achieve a "clean" status. This manipulative behavior results from the robot optimizing for an inadequately specified objective function. Another example involves gaming AI, which uses bugs or exploits to win rather than play by the intended rules, thus achieving victory through unintended means <span class="citation" data-cites="krakovna2020specification">(<a href="#ref-krakovna2020specification" role="doc-biblioref">Krakovna et al. 2020</a>)</span>.</p>
<p>One example of specification gaming is seen in recommendation systems, such as those used by YouTube or Facebook. Ideally, these systems should recommend content that users enjoy. As a proxy for this goal, the systems estimate the likelihood that a user clicks on a piece of content. Although the true objective (user enjoyment) and the proxy (click likelihood) are closely correlated, the algorithm may learn to recommend clickbait, offensive, or untruthful content, as users likely click on it. This optimization for clicks rather than genuine enjoyment exemplifies specification gaming, where the algorithm exploits the divergence between the specified objective and the true goal, resulting in misalignment with user interests <span class="citation" data-cites="amodei2016concrete">(<a href="#ref-amodei2016concrete" role="doc-biblioref">Amodei et al. 2016</a>)</span>.</p>
<p>Another instance of specification gaming is evident in reinforcement learning from human feedback (RLHF). Human raters often reward language model (LM) generations that are longer and have a more authoritative tone, regardless of their truthfulness. Here, the true objective (providing high-quality, truthful, and helpful answers) diverges from the proxy goal (a reward model that, due to human rater biases, favors longer and more authoritative-sounding generations). Consequently, models trained with RLHF may produce low-quality answers containing hallucinations but are still favored by the reward model <span class="citation" data-cites="leike2018scalable">(<a href="#ref-leike2018scalable" role="doc-biblioref">Leike et al. 2018</a>)</span>.</p>
<p>Creating accurate objective functions is challenging due to the complexity of human intentions. Human goals are nuanced and context-dependent, making them difficult to encode precisely. Common pitfalls in objective function design include oversimplifying objectives and ignoring long-term consequences. Leike et al.&nbsp;emphasize that “accurately capturing the complexity of human values in objective functions is crucial to avoid specification gaming and ensure proper alignment” <span class="citation" data-cites="leike2018scalable">(<a href="#ref-leike2018scalable" role="doc-biblioref">Leike et al. 2018</a>)</span>.</p>
<p>To mitigate specification gaming, better objective function design is essential. This involves incorporating broader context and constraints into the objectives and regularly updating them based on feedback. Iterative testing and validation are also critical. AI behavior must be continuously tested in diverse scenarios, using simulation environments to identify and fix exploits. Everitt and Hutter discuss the importance of “robust objective functions and rigorous testing to prevent specification gaming and achieve reliable AI alignment” <span class="citation" data-cites="everitt2018alignment">(<a href="#ref-everitt2018alignment" role="doc-biblioref">Everitt and Hutter 2018</a>)</span>. Clark and Amodei further highlight that “faulty reward functions can lead to unintended and potentially harmful AI behavior, necessitating ongoing refinement and validation” <span class="citation" data-cites="clark2016faulty">(<a href="#ref-clark2016faulty" role="doc-biblioref">Clark and Amodei 2016</a>)</span>.</p>
<p>The metrics used to evaluate AI systems play a crucial role in outer alignment. Many AI metrics, such as BLEU, METEOR, and ROUGE, are chosen for their ease of measurement but do not necessarily capture human judgment <span class="citation" data-cites="hardt_patterns_2021">(<a href="#ref-hardt_patterns_2021" role="doc-biblioref">Hardt and Recht 2021</a>)</span>. These metrics can lead to specification gaming, as they may not align with the true objectives we want the AI to achieve. Similarly, using SAT scores to measure LLM performance may not predict real-world task effectiveness, highlighting the need for more contextually relevant benchmarks <span class="citation" data-cites="chowdhery_palm_2022">(<a href="#ref-chowdhery_palm_2022" role="doc-biblioref">Chowdhery et al. 2022</a>)</span>. The word error rate (WER) used in speech recognition is another example; it does not account for semantic errors, leading to misleading conclusions about the system’s performance <span class="citation" data-cites="xiong_achieving_2016">(<a href="#ref-xiong_achieving_2016" role="doc-biblioref">Xiong et al. 2016</a>)</span>.</p>
<p>A classic example comes from six years ago with the claim that a system "Achieve[d] human parity in conversation speech recognition" <span class="citation" data-cites="xiong_achieving_2016">(<a href="#ref-xiong_achieving_2016" role="doc-biblioref">Xiong et al. 2016</a>)</span>. However, we know from experience that captioning services have only recently begun to transcribe speech passably, whether in online meetings or web videos. What happened? In this case, researchers showed their system beat the human baseline—the error rate when transcribing films. However, there were issues with their approach. First, they used a poor measure of a human baseline by hiring untrained Mturk annotators instead of professional captioners. Second, the metric itself, the word error rate (WER), was flawed. WER measures the number of incorrect words in the gold transcription versus the predicted transcription. Consider what the metric hides when it says that two systems both have an error rate of six percent. This does not mean the systems are equivalent. One might substitute "a" for "the," while the other substitutes "tarantula" for "banana." The metric was not sensitive to semantic errors, so a model could outperform humans in WER yet still make unintelligent, highly unsemantic mistakes.</p>
</section>
<section id="inner-alignment-preventing-goal-misgeneralization" class="level4" data-number="5.1.4.2">
<h4 data-number="5.1.4.2" class="anchored" data-anchor-id="inner-alignment-preventing-goal-misgeneralization"><span class="header-section-number">5.1.4.2</span> Inner Alignment: Preventing Goal Misgeneralization</h4>
<p>Assume we have perfectly specified human values in a reward model. An issue remains: given finite training data, many models perform well on the training set, but each will generalize somewhat differently. How do we choose models that correctly generalize to new distributions? This is the problem of <em>goal misgeneralization</em>, also known as the <em>inner alignment problem</em>, where a learned algorithm performs well on the training set but generalizes poorly to new input distributions, achieving low rewards even on the reward function it was trained on. Inner alignment ensures that the learned goals and behaviors of an AI system align with the intended objectives during deployment, whereas goal misgeneralization occurs when an AI system applies learned goals inappropriately to new situations <span class="citation" data-cites="hubinger2019introduction">(<a href="#ref-hubinger2019introduction" role="doc-biblioref">Hubinger et al. 2019</a>)</span>.</p>
<p>Consider the following example of goal misgeneralization from <span class="citation" data-cites="shah2022goal">(<a href="#ref-shah2022goal" role="doc-biblioref">Shah et al. 2022</a>)</span>. The setup involves a never-ending reinforcement learning environment without discrete episodes. The agent navigates a grid world where it can collect rewards by chopping trees. Trees regenerate at a rate dependent on the number left; they replenish slowly when few remain. The optimal policy is to chop trees sustainably, i.e., fewer when they are scarce. However, the agent does not initially learn the optimal policy.</p>
<div id="fig:enter-label-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/tree-gridworld.jpeg" class="img-fluid figure-img"></p>
<figcaption>The agent’s performance in Tree Gridworld. The reward is shown in orange, and the green distribution indicates the number of remaining trees.</figcaption>
</figure>
</div>
<p>Initially, the agent is inefficient at chopping trees, keeping the tree population high (point A). As it improves its chopping skills, it over-harvests, leading to deforestation and a prolonged period of minimal reward (between points B and C). Eventually, it learns sustainable chopping (point D). This scenario (up to point C) exemplifies goal misgeneralization. When the agent first becomes proficient at chopping (between points A and B), it faces a range of potential goals, from sustainable to rapid tree chopping. All these goals align with the (well-specified) reward function and its experience of being rewarded for increased efficiency. Unfortunately, it adopts the detrimental goal of rapid deforestation, resulting in a prolonged period of low reward.</p>
<p>Another example of goal misgeneralization occurs in recommendation systems. These systems aim to maximize user engagement, which can inadvertently lead to promoting extreme or sensational content. Krakovna et al.&nbsp;highlights that “recommendation systems can misgeneralize by prioritizing content that maximizes clicks or watch time, even if it involves promoting harmful or misleading information” <span class="citation" data-cites="krakovna2020specification">(<a href="#ref-krakovna2020specification" role="doc-biblioref">Krakovna et al. 2020</a>)</span>. This misalignment between the system’s learned objective (engagement) and the intended objective (informative and beneficial content) exemplifies how goal misgeneralization can manifest in real-world applications.</p>
<p>Autonomous vehicles also present cases of goal misgeneralization. These vehicles must interpret and respond to various signals in their environment. However, in rare scenarios, they may misinterpret signals, leading to unsafe maneuvers. Amodei et al.&nbsp;discuss that “autonomous vehicles can exhibit unsafe behaviors when faced with uncommon situations that were not well-represented in the training data, demonstrating a misgeneralization of their learned driving policies” <span class="citation" data-cites="amodei2016concrete">(<a href="#ref-amodei2016concrete" role="doc-biblioref">Amodei et al. 2016</a>)</span>. Ensuring that autonomous vehicles generalize correctly to all possible driving conditions remains a significant challenge.</p>
<p>To address goal misgeneralization, robust training procedures are essential. This involves using diverse and representative training data to cover a wide range of scenarios and incorporating adversarial training to handle edge cases. Leike et al. <span class="citation" data-cites="leike2018scalable">(<a href="#ref-leike2018scalable" role="doc-biblioref">Leike et al. 2018</a>)</span> emphasize the importance of “robust training procedures that include diverse datasets and adversarial examples to improve the generalization of AI systems”. Additionally, careful specification of learning goals is crucial. This means defining clear and comprehensive objectives and regularly reviewing and adjusting these goals based on performance and feedback. Hubinger et al.&nbsp;suggests that “regularly updating and refining the objectives based on ongoing evaluation can help mitigate the risks of goal misgeneralization” <span class="citation" data-cites="hubinger2019introduction">(<a href="#ref-hubinger2019introduction" role="doc-biblioref">Hubinger et al. 2019</a>)</span>.</p>
<p>A key concern about goal misgeneralization in competent, general systems is that a policy successfully models the preferences of human raters (or the reward model) and behaves accordingly to maximize reward during training. However, it may deviate catastrophically from human preferences when given a different input distribution during deployment, such as during an unexpected geopolitical conflict or when facing novel technological developments. Increasing data size, regularization, and red-teaming can help mitigate goal misgeneralization, but they do not fundamentally solve the problem. Understanding the inductive biases of optimization algorithms and model families may help address the problem more generally.</p>
<div class="tcolorbox">
<p>So, can you differentiate between inner and outer alignment?</p>
</div>
<p>The distinction between inner and outer alignment can be a bit subtle. The following four cases, from <span class="citation" data-cites="ngo2023alignment">(<a href="#ref-ngo2023alignment" role="doc-biblioref">Ngo, Chan, and Mindermann 2023</a>)</span>, may help to clarify the difference:</p>
<ul>
<li><p>The policy behaves incompetently. This is a capability generalization failure.</p></li>
<li><p>The policy behaves competently and desirably. This is aligned behavior.</p></li>
<li><p>The policy behaves in a competent yet undesirable way which gets a high reward according to the original reward function. This is an outer alignment failure, also known as reward misspecification.</p></li>
<li><p>The policy behaves in a competent yet undesirable way which gets a low reward according to the original reward function. This is an inner alignment failure, also known as goal misgeneralization.</p></li>
</ul>
<p>Now that we understand the alignment problem overall, we move on to the specific techniques used for value learning to ensure AI systems are aligned with human values.</p>
</section>
</section>
<section id="techniques-in-value-learning" class="level3" data-number="5.1.5">
<h3 data-number="5.1.5" class="anchored" data-anchor-id="techniques-in-value-learning"><span class="header-section-number">5.1.5</span> Techniques in Value Learning</h3>
<p>Various methods in value learning for foundation models have been explored in great detail in recent years <span class="citation" data-cites="stiennon_learning_2020">(<a href="#ref-stiennon_learning_2020" role="doc-biblioref">Stiennon et al. 2020</a>)</span>. Using binary human-labeled feedback to make models closely aligned to human preferences is particularly difficult in scenarios where large datasets inherently encompass suboptimal behaviors. The approach of Reinforcement Learning from Human Feedback (RLHF) (<span class="citation" data-cites="ouyang_training_2022">(<a href="#ref-ouyang_training_2022" role="doc-biblioref">Ouyang et al. 2022</a>)</span>) has risen to prominence as an effective method for addressing this issue. The technique applies to various domains, from prompt-image alignment, fine-tuning large language models or diffusion models, and improving the performance of robot policies.</p>
<section id="reinforcement-learning-from-human-feedback" class="level4" data-number="5.1.5.1">
<h4 data-number="5.1.5.1" class="anchored" data-anchor-id="reinforcement-learning-from-human-feedback"><span class="header-section-number">5.1.5.1</span> Reinforcement Learning from Human Feedback</h4>
<p>Reinforcement Learning from Human Feedback (RLHF) is a technique used to align AI behavior with human values by incorporating human feedback into the reinforcement learning process. This approach is particularly effective when large datasets inherently encompass suboptimal behaviors. RLHF aims to refine policies by discriminating between desirable and undesirable actions, ensuring that AI systems act following human preferences <span class="citation" data-cites="ouyang_training_2022">(<a href="#ref-ouyang_training_2022" role="doc-biblioref">Ouyang et al. 2022</a>)</span>.</p>
<p><strong>The core concept of RLHF:</strong> It first trains a reward model using a dataset of binary preferences gathered from human feedback. This reward model is then used to fine-tune the AI model through a reinforcement learning algorithm. The core concept is to utilize human feedback to guide AI learning, thereby aligning the AI’s behavior with human expectations <span class="citation" data-cites="stiennon_learning_2020">(<a href="#ref-stiennon_learning_2020" role="doc-biblioref">Stiennon et al. 2020</a>)</span>.</p>
<div id="fig:toy0" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/rlhf.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>The above diagram depicts the three steps in the traditional RLHF pipeline: (a) supervised fine-tuning, (b) reward model (RM) training, and (c) reinforcement learning via proximal policy optimization (PPO) on this reward model. Image taken from <span class="citation" data-cites="ouyang_training_2022">(<a href="#ref-ouyang_training_2022" role="doc-biblioref">Ouyang et al. 2022</a>)</span>.</figcaption>
</figure>
</div>
<p><strong>The RLHF pipeline</strong> involves the following steps:</p>
<p><strong>Step 1: Supervised Fine-Tuning</strong></p>
<p>In the initial step for language modeling tasks, we utilize a high-quality dataset consisting of <span class="math inline">\(\left(\text{prompt}, \text{response}\right)\)</span> pairs to train the model. Prompts are sampled from a curated dataset designed to cover a wide range of instructions and queries, such as "Explain the moon landing to a 6-year-old." Trained human labelers provide the desired output behavior for each prompt, ensuring responses are accurate, clear, and aligned with task goals. For instance, in response to the moon landing prompt, a labeler might generate, "Some people went to the moon in a big rocket and explored its surface." The collected <span class="math inline">\(\left(\text{prompt}, \text{response}\right)\)</span> pairs serve as the training data for the model, with the cross-entropy loss function applied only to the response tokens. This helps the model learn to generate responses that are closely aligned with the human-provided examples. The training process adjusts model parameters through supervised learning, minimizing the difference between the model’s predictions and the human responses.</p>
<p><strong>Step 2: Reward Model (RM) Training</strong></p>
<p>In this step, we train a reward model to score any <span class="math inline">\(\left(\text{prompt}, \text{response}\right)\)</span> pair and produce a meaningful scalar value. Multiple model-generated responses are sampled for each prompt. Human labelers then rank these responses from best to worst based on their quality and alignment with the prompt. For example, given the prompt "Explain the moon landing to a 6-year-old," responses like "People went to the moon in a big rocket and explored its surface" might be ranked higher than "The moon is a natural satellite of Earth." The rankings provided by the labelers are used to train the reward model <span class="math inline">\(\Phi_{\text{RM}}\)</span>. The model is trained by minimizing the following loss function across all training samples:</p>
<p><span class="math display">\[\mathbb{L}(\Phi_{RM}) = -\mathbb{E}_{(x,y_e,i\rightarrow D_{RL})}[\log(\sigma(\Phi_{RM}(x, y_i)) - \Phi_{RM}(x, y_{1-i}))]\]</span></p>
<p>for <span class="math inline">\(i \in \{0,1 \}\)</span>. This loss function encourages the reward model to produce higher scores for better-ranked responses, thereby learning to evaluate the quality of model outputs effectively.</p>
<p><strong>Step 3: Reinforcement Learning</strong></p>
<p>In this step, we refine the policy using reinforcement learning (RL) based on the rewards provided by the trained reward model. A new prompt is sampled from the dataset, and the policy generates an output. The reward model then calculates a reward for this output, and the reward is used to update the policy using the Proximal Policy Optimization (PPO) algorithm.</p>
<p>The RL setting is defined as follows:</p>
<ol type="1">
<li><p><em>Action Space</em>: The set of all possible actions the agent can take, which, for language models, is typically the set of all possible completions.</p></li>
<li><p><em>Policy</em>: A probability distribution over the action space. In the case of language models like LLM, the policy is contained within the model and represents the probability of predicting each completion.</p></li>
<li><p><em>Observations</em>: The inputs to the policy, which in this context are prompts sampled from a certain distribution.</p></li>
<li><p><em>Reward</em>: A numerical score provided by the Reward Model (RM) that indicates the quality of actions taken by the agent.</p></li>
</ol>
<p>During training, batches of prompts are sampled from two distinct distributions, namely either <span class="math inline">\(D_\text{RL}\)</span>, the distribution of prompts explicitly used for the RL model, or <span class="math inline">\(D_\text{pretrain}\)</span>, the distribution of prompts from the pre-trained model. The objective for the RL agent is to maximize the reward while ensuring that the policy does not deviate significantly from the supervised fine-tuned model and does not degrade the performance on tasks the pre-trained model was optimized for. When sampling a response <span class="math inline">\(y\)</span> to a prompt <span class="math inline">\(x\)</span> from <span class="math inline">\(D_\text{RL}\)</span>, the first objective function is:</p>
<p><span class="math display">\[\text{objective}_1(x_{RL}, y; \phi) = RM(x_{RL}, y) - \beta \log \frac{\text{LLM}_{\phi}^{RL}(y|x)}{\text{LLM}_{SFT}(y|x)}\]</span></p>
<p>Where the first term is the reward from the RM, and the second term is the Kullback-Leibler (KL) divergence, weighted by a factor <span class="math inline">\(\beta\)</span>, which acts as a regularizer to prevent the RL model from straying too far from the SFT model. Further, for each <span class="math inline">\(x\)</span> from <span class="math inline">\(D_\text{pretrain}\)</span>, the second objective is to ensure that the RL model’s performance on text completion does not worsen:</p>
<p><span class="math display">\[\text{objective}_2(x_{\text{pretrain}} ; \phi) = \gamma \log \text{LLM}_{\phi}^{RL}(x_{\text{pretrain}})\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> is a weighting factor that balances the influence of this objective against the others.</p>
<p>The final objective function is a sum of the expected values of the two objectives described above, across both distributions. In the RL setting, we maximize <em>this</em> objective function:</p>
<p><span class="math display">\[\text{objective}(\phi) = E_{(x,y) \sim D_{\phi}^{RL}}[RM(x, y) - \beta \log \frac{\text{LLM}_{\phi}^{RL}(y|x)}{\text{LLM}_{SFT}(y|x)}] + \gamma E_{x \sim D_{\text{pretrain}}}[\log \text{LLM}_{\phi}^{RL}(x)]\]</span></p>
<p>In practice, the second part of the objective is often not used to perform <span class="math inline">\(\text{RLHF}\)</span>. The KL penalty is typically enough to constrain the RL policy. This function balances the drive to maximize the reward with the need to maintain the quality of text completion and the similarity to the behavior of the supervised fine-tuned model.</p>
<p><strong>Limitations and Challenges:</strong> Despite its successes, RLHF faces several challenges. One major issue is the quality of human feedback, which can be inconsistent and subjective. Scalability is another concern, as obtaining a large amount of high-quality feedback can be expensive and time-consuming. Over-optimization and hallucinations, where the model generates plausible but incorrect outputs, are also common problems. This generally stems from temporal credit assignment and the instability of approximate dynamic programming <span class="citation" data-cites="vanhasselt_deep_2018">(<a href="#ref-vanhasselt_deep_2018" role="doc-biblioref">Hasselt et al. 2018</a>)</span>. Further, it is expensive to gather tens of thousands of preferences over datasets to create robust reward models. Strategies to overcome these challenges include using diverse and representative training data, incorporating adversarial training to handle edge cases, and continuously refining the reward model based on ongoing feedback and performance evaluations <span class="citation" data-cites="leike2018scalable">(<a href="#ref-leike2018scalable" role="doc-biblioref">Leike et al. 2018</a>)</span>.</p>
</section>
<section id="contrastive-preference-learning" class="level4" data-number="5.1.5.2">
<h4 data-number="5.1.5.2" class="anchored" data-anchor-id="contrastive-preference-learning"><span class="header-section-number">5.1.5.2</span> Contrastive Preference Learning</h4>
<p>Contrastive Preference Learning (CPL) is a learning paradigm designed to enhance the alignment of AI systems with human preferences without relying on traditional reinforcement learning (RL) methods. CPL addresses many limitations inherent in traditional RLHF techniques by learning from human comparisons rather than explicit reward signals. This section provides an in-depth exploration of CPL, detailing its methodology, experiments, results, and potential challenges. Recent research has shown that human preferences are often better modeled by the optimal advantage function or regret, rather than traditional reward functions used in RLHF. Traditional RLHF approaches, which learn a reward function from a preference model and then apply RL, incur significant computational expenses and complexity <span class="citation" data-cites="hejna2023contrastive">(<a href="#ref-hejna2023contrastive" role="doc-biblioref">Hejna et al. 2023</a>)</span>. CPL offers a streamlined and scalable alternative by leveraging a more accurate regret model of human preferences.</p>
<p><strong>The key idea of CPL</strong> is the substitution of the optimal advantage function with the log probability of the policy in a maximum entropy reinforcement learning framework. This substitution is beneficial as it circumvents the need to learn the advantage function and avoids the optimization challenges associated with RL-like algorithms. By using the log probability of the policy, CPL more closely aligns with how humans model preferences and enables efficient supervised learning from human feedback.</p>
<p>CPL is a structured approach to aligning AI behavior with human preferences by relying on a dataset of preferred behavior segments <span class="math inline">\(\mathcal{D}_{\text{pref}} = \{(\sigma_i^+, \sigma_i^-)\}_{i=1}^n\)</span>, where <span class="math inline">\(\sigma^+ \succ \sigma^-\)</span>. Each behavior segment <span class="math inline">\(\sigma\)</span> is a sequence of states and actions, <span class="math inline">\(\sigma = (s_1, a_1, s_2, a_2, \ldots, s_k, a_k)\)</span>. The CPL approach aims to maximize the expected sum of rewards minus an entropy term, which promotes exploration and prevents overfitting to specific actions:</p>
<p><span class="math display">\[\max_\pi \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t (r(s_t, a_t) - \alpha \log \pi(a_t | s_t)) \right]\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> is the discount factor, <span class="math inline">\(\alpha\)</span> is the temperature parameter controlling the stochasticity of the policy, and <span class="math inline">\(r\)</span> is the reward function. This step sets the foundation by defining the optimization objective that the CPL model strives to achieve. In the learning process, CPL compares the log probabilities of actions in preferred segments <span class="math inline">\(\sigma^+\)</span> against those in non-preferred segments <span class="math inline">\(\sigma^-\)</span> :</p>
<p><span class="math display">\[\mathbb{L}_{CPL}(\pi_\theta, \mathcal{D}_{\text{pref}}) = \mathbb{E}_{(\sigma^+,\sigma^-) \sim \mathcal{D}_{\text{pref}}} \left[ -\log \frac{\exp(\sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a_t^+|s_t^+))}{\exp(\sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a_t^+|s_t^+)) + \exp(\sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a_t^-|s_t^-))} \right]\]</span></p>
<p>This comparison allows the model to learn which actions are more aligned with human preferences, forming the core learning mechanism of CPL. The preference model for CPL is regret-based, described as</p>
<p><span class="math display">\[P_{A^*}[\sigma^+ \succ \sigma^-] = \frac{\exp(\sum_{\sigma^+} \gamma^t A^*(s_t^+, a_t^+))}{\exp(\sum_{\sigma^+} \gamma^t A^*(s_t^+, a_t^+)) + \exp(\sum_{\sigma^-} \gamma^t A^*(s_t^-, a_t^-))}\]</span> where <span class="math inline">\(A^*(s_t, a_t)\)</span> represents the advantage function and is a matrix. This step models human preferences based on regret, reflecting how humans might evaluate different behaviors.</p>
<p>One hypothesis as to why one might consider a regret-based model more useful over a sum-of-rewards, Bradley-Terry model is that humans likely think of preferences based on the regret of each behavior under the optimal policy of the expert’s reward function.</p>
<p>The key insight that the paper leverages is that from <span class="citation" data-cites="ziebart_modeling_2010">(<a href="#ref-ziebart_modeling_2010" role="doc-biblioref">Ziebart 2010</a>)</span> in MaxEnt Offline RL. In this general setting, <span class="citation" data-cites="ziebart_modeling_2010">(<a href="#ref-ziebart_modeling_2010" role="doc-biblioref">Ziebart 2010</a>)</span> shows that one can write that the optimal advantage function is related to the optimal policy by <span class="math inline">\(A^*_r(s, a) = \alpha \log \pi^*(a|s)\)</span>. Therefore, the loss function for CPL can be written by substituting the above result to obtain: <span class="math display">\[L_{CPL}(\pi_\theta, \mathcal{D}_{\text{pref}}) = \mathbb{E}_{(\sigma^+,\sigma^-) \sim \mathcal{D}_{\text{pref}}} \left[ -\log P_{\pi_\theta}[\sigma^+ \succ \sigma^-] \right]\]</span></p>
<p>One merit of using CPL over the typical RLHF pipeline is that it can lead to a deduction in mode collapse. Further, it makes reward misgeneralization failures less likely, enhancing the reliability of the learned policy. However, the approach still has a few limitations:</p>
<ol type="1">
<li><p>CPL assumes knowledge of the human rater’s temporal discounting (i.e., of the discount factor <span class="math inline">\(\gamma\)</span>), which in practice would be difficult to communicate.</p></li>
<li><p>CPL’s loss function is computed over segments, it requires a substantial amount of GPU memory for large segment sizes.</p></li>
</ol>
<div class="tcolorbox">
<p>How does RLHF with PPO and CPL compare their effectiveness and applicability in aligning AI systems with human values?</p>
</div>
<p>The ongoing challenge in aligning foundation models in the future will be to refine these methodologies further, balancing computational feasibility with the sophistication needed to capture the intricacies of human values and countering failure modes such as reward over-optimization. In conclusion, exploring value learning through RLHF and CPL methods has enriched our understanding of integrating human preferences into foundation models. To provide a well-rounded perspective on aligning AI systems with human values, the following table highlights a detailed comparison of RLHF with PPO and CPL, emphasizing their advantages, limitations, and ideal scenarios.</p>
<div id="tab:ppo_vs_cpl">
<table class="caption-top table">
<caption>Comparison between RLHF with PPO and CPL</caption>
<colgroup>
<col style="width: 30%">
<col style="width: 31%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;"><strong>RLHF with PPO</strong></th>
<th style="text-align: left;"><strong>CPL</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Strengths</strong></td>
<td style="text-align: left;"><ul>
<li><p>Excels in optimizing policies through reinforcement learning</p></li>
<li><p>Suitable for tasks that benefit from iterative improvement</p></li>
<li><p>Effective in continuous action spaces</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p>Emphasizes regret and optimality rather than reward maximization</p></li>
<li><p>Reduces computational overhead</p></li>
<li><p>Aligns more closely with human preferences</p></li>
<li><p>Avoids reward</p></li>
</ul>
<p>over-optimization</p>
<ul>
<li>More scalable due to reliance on supervised learning techniques</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Limitations</strong></td>
<td style="text-align: left;"><ul>
<li><p>Faces limitations in handling complex preference structures</p></li>
<li><p>High computational cost</p></li>
<li><p>Susceptible to reward</p></li>
</ul>
<p>misgeneralization</p></td>
<td style="text-align: left;"><ul>
<li><p>May struggle in environments where direct human feedback is less accessible</p></li>
<li><p>Depends on high-quality preference data for effective training</p></li>
</ul></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Ideal Scenarios</strong></td>
<td style="text-align: left;"><ul>
<li><p>Tasks with well-defined reward functions</p></li>
<li><p>Environments allowing extensive interaction and feedback</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p>Environments where human feedback is more accessible than well-defined reward functions</p></li>
<li><p>Tasks requiring computational efficiency and scalability</p></li>
</ul></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="value-alignment-verification" class="level3" data-number="5.1.6">
<h3 data-number="5.1.6" class="anchored" data-anchor-id="value-alignment-verification"><span class="header-section-number">5.1.6</span> Value Alignment Verification</h3>
<p>After we discuss the techniques of value learning, it becomes evident that aligning machine behavior with human values, while advanced, is inherently approximate and not infallible. This realization underscores the importance of value alignment verification—a methodology to ensure that the values imparted to a machine truly reflect those of a human. Human-robot value alignment has been explored through various lenses, including qualitative trust assessments <span class="citation" data-cites="huang2018establishing">(<a href="#ref-huang2018establishing" role="doc-biblioref">Huang et al. 2018</a>)</span>, asymptotic alignment through active learning of human preferences <span class="citation" data-cites="hadfield2016cooperative christiano2017deep sadigh2017active">(<a href="#ref-hadfield2016cooperative" role="doc-biblioref">Hadfield-Menell et al. 2016</a>; <a href="#ref-christiano2017deep" role="doc-biblioref">P. F. Christiano et al. 2017</a>; <a href="#ref-sadigh2017active" role="doc-biblioref">Sadigh et al. 2017</a>)</span>, and formal verification methods <span class="citation" data-cites="brown2021value">(<a href="#ref-brown2021value" role="doc-biblioref">Brown et al. 2021</a>)</span>. This section will focus on the formal verification approach for value alignment as discussed in <span class="citation" data-cites="brown2021value">(<a href="#ref-brown2021value" role="doc-biblioref">Brown et al. 2021</a>)</span>. Unless otherwise stated, all information presented here is derived from <span class="citation" data-cites="brown2021value">(<a href="#ref-brown2021value" role="doc-biblioref">Brown et al. 2021</a>)</span>. This approach aims to ensure that the values imparted to a machine align with those of a human.</p>
<p>To begin with, consider an MDP with state space <span class="math inline">\(\mathcal{S}\)</span>, action space <span class="math inline">\(\mathcal{A}\)</span>, and transition model <span class="math inline">\(\mathcal{T}\)</span>. This formal framework allows us to model the environment in which humans and robots operate. Denote the human’s reward function as <span class="math inline">\(R\)</span> and the robot’s reward function as <span class="math inline">\(R^\prime\)</span>. Both the human and robot reward functions must be linear in a set of shared features, defined as: <span class="math display">\[\begin{aligned}
    R(s) = \mathbf{w}^\top \phi(s), R^\prime(s) = \mathbf{w}^{\prime \top} \phi(s).
\end{aligned}\]</span></p>
<p>These linear reward functions provide a common ground for comparing human and robot preferences.</p>
<p>Next, the optimal state-action value function, which indicates the expected cumulative reward of following a policy <span class="math inline">\(\pi\)</span> starting from state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span>, but we follow the notation in <span class="citation" data-cites="brown2021value">(<a href="#ref-brown2021value" role="doc-biblioref">Brown et al. 2021</a>)</span> for simplicity. The optimal state-action value function is given by:</p>
<p><span class="math display">\[\begin{aligned}
    Q_R^\pi (s,a) = \mathbf{w}^\top \Phi_{\pi_R}^{(s,a)}, \Phi_{\pi_R}^{(s,a)} = \mathbb{E}_\pi [\sum_{t=0}^\infty \gamma^t \phi(s_t) \vert s_0 = s, a_0 = a].
\end{aligned}\]</span></p>
<p>Here, <span class="math inline">\(\Phi_{\pi_R}^{(s,a)}\)</span> is the feature expectation vector under policy <span class="math inline">\(\pi\)</span>, capturing the long-term feature visitation frequencies. We overload the action space notation to define the set of all optimal actions given a state as</p>
<p><span class="math display">\[\begin{aligned}
    \mathcal{A}_R(s) = \underset{x}{\operatorname{argmax}} \\ Q^{\pi^*}_R(s,a)
\end{aligned}\]</span> where <span class="math inline">\(\pi^*\)</span> is an optimal policy. We can now define the aligned reward polytope (ARP). The ARP is the set of all weights <span class="math inline">\(\mathcal{w}\)</span> that satisfy the following set of strict linear inequalities, <span class="math inline">\(\mathbf{w}^\top \mathbf{A}  &gt; \mathbf{0}\)</span> where each row of <span class="math inline">\(\mathbf{A}\)</span> corresponds to <span class="math inline">\(\Phi_{\pi^*_R}^{(s,a)} - \Phi_{\pi^*_R}^{(s,b)}\)</span> for a single <span class="math inline">\((s,a,b)\)</span> tuple where <span class="math inline">\(s \in \mathcal{S}, a \in \mathcal{A}_R(s), b \notin \mathcal{A}_R(s)\)</span>. Thus, to construct <span class="math inline">\(\mathbf{A}\)</span>, one must loop over all <span class="math inline">\((s,a,b)\)</span> tuples which has complexity <span class="math inline">\(O(\vert \mathcal{S} \vert \cdot \vert \mathcal{A} \vert^2)\)</span>. This construction ensures that the weights <span class="math inline">\(\mathbf{w}\)</span> align with the human’s optimal actions across all states.</p>
<p>The intuition behind the ARP is that we use the human optimal policy for each state to determine what actions are optimal and what are suboptimal at this state. Then, for every one of those combinations, we can place a linear inequality on the set of reward weights consistent with that optimal vs suboptimal action bifurcation. One of the key assumptions that let us do this is that we assume both the human and the robot act optimally according to their reward function. This is known as a <em>rationality assumption</em> and provides the link between actions and rewards that we need.</p>
<p>For illustration, consider a simple grid world environment. Figure <a href="#fig:toy" data-reference-type="ref" data-reference="fig:toy">1.6</a> shows the optimal policy and the corresponding ARP. The optimal policy reveals that the gray state is less preferred compared to the white states, which is reflected in the ARP (hatched region of Figure <a href="#fig:toy" data-reference-type="ref" data-reference="fig:toy">1.6</a>b).</p>
<figure id="fig:toy" class="figure">
<p>
<img src="Figures/toy_policy.png" style="width:30.0%" alt="image" class="figure-img"> <img src="Figures/toy_arp.png" style="width:30.0%" alt="image" class="figure-img">
</p>
<figcaption>
Optimal policy (a) and aligned reward polytope (ARP) (b) for a grid world with two features (white and gray) and a linear reward function (<span class="math inline"><em>R</em>(<em>s</em>) = <em>w</em><sub>0</sub> ⋅ <strong>1</strong><sub><em>w</em><em>h</em><em>i</em><em>t</em><em>e</em></sub>(<em>s</em>) + <em>w</em><sub>1</sub> ⋅ <strong>1</strong><sub><em>g</em><em>r</em><em>a</em><em>y</em></sub>(<em>s</em>)</span>). The ARP is denoted by the hatched region in (b).
</figcaption>
</figure>
<p>Computing the ARP exactly can be computationally demanding or we may not have access to the robot’s reward function. This section describes heuristics for testing value alignment in the case the robot’s reward weights (<span class="math inline">\(\mathbf{w^\prime}\)</span>) are unknown, but the robot’s policy can be queried. Heuristics provide simplified methods to estimate value alignment without the need for exhaustive computations.</p>
<p><strong>ARP-blackbox:</strong> The ARP black-box (ARP-bb) heuristic helps address the challenge of computing the ARP by allowing users to work with a simplified model. In this heuristic, the user first solves for the ARP and removes all redundant half-space constraints. For each remaining half-space constraint, the user queries the robot’s action at the corresponding state. The intuition here is that states, where different actions are taken, reveal crucial information about the reward function. By focusing on these key states, we can gain insights into the robot’s reward function without needing to know it explicitly.</p>
<p><strong>Set Cover Optimal Teaching:</strong> The Set Cover Optimal Teaching (SCOT) heuristic uses techniques from <span class="citation" data-cites="brown2019machine">(<a href="#ref-brown2019machine" role="doc-biblioref">Brown and Niekum 2019</a>)</span> to generate maximally informative trajectories. These trajectories are sequences of states where the number of optimal actions is limited, making them particularly informative for understanding the robot’s policy. By querying the robot for actions along these trajectories, we can efficiently gauge the alignment of the robot’s policy. This method helps to identify potential misalignments by focusing on critical decision points in the trajectories.</p>
<p><strong>Critical States:</strong> The Critical States (CS) heuristic identifies states where the gap in value between the optimal action and an average action is significant. These states are crucial because if the robot’s policy is misaligned, the misalignment will be most consequential at these critical states. By querying the robot’s policy at these states, we can assess the alignment more effectively. This heuristic is particularly useful when we have a limited budget of states to check, as it prioritizes the most informative states for evaluation.</p>
<p><strong>Practical Examples:</strong> To illustrate the concepts of value alignment verification, we present an example of applying value alignment verification in a simple MDP grid world environment. Consider a grid world where the human’s reward function is defined as <span class="math inline">\(R(s) = 50 \cdot \mathbf{1}_{green}(s) - 1 \cdot \mathbf{1}_{white}(s) - 50 \cdot \mathbf{1}_{blue}(s)\)</span>, where <span class="math inline">\(\mathbf{1}_{color}(s)\)</span> is an indicator feature for the color of the grid cell. The objective is to align the robot’s policy with this reward function.</p>
<figure id="fig:island" class="figure">
<p>
<img src="Figures/optimal.png" style="width:32.0%" alt="image" class="figure-img"> <img src="Figures/pref_0.png" style="width:32.0%" alt="image" class="figure-img"> <img src="Figures/pref_1.png" style="width:32.0%" alt="image" class="figure-img"> <img src="Figures/arb-bb.png" style="width:32.0%" alt="image" class="figure-img"> <img src="Figures/scot.png" style="width:32.0%" alt="image" class="figure-img"> <img src="Figures/cs-10.png" style="width:32.0%" alt="image" class="figure-img"> <span id="fig:island" data-label="fig:island"></span>
</p>
<figcaption>
<ol type="a">
<li>optimal policy (b) preference query 1 (c) preference query 2 (d) ARP-bb queries (e) SCOT queries (f) CS queries. In the preference queries, the human reward model prefers black to orange.
</li></ol></figcaption>
</figure>

<p>Figure <a href="#fig:island" data-reference-type="ref" data-reference="fig:island">1.7</a>a shows all optimal actions at each state according to the human’s reward function. This optimal policy serves as the benchmark for alignment verification. Figures <a href="#fig:island" data-reference-type="ref" data-reference="fig:island">1.7</a>b and <a href="#fig:island" data-reference-type="ref" data-reference="fig:island">1.7</a>c show two pairwise preference trajectory queries (black is preferable to orange according to (<a href="#eq:%20human_r" data-reference-type="ref" data-reference="eq: human_r">[eq: human_r]</a>)). Preference query 1 verifies that the robot values reaching the terminal goal state (green) rather than visiting more white states. Preference query 2 verifies that the robot values white states more than blue states. These two preference queries are all we need to determine whether the robot’s values are aligned with the human’s values.</p>
<p>Next, we apply the heuristics discussed in the previous section to this grid world example. Figures <a href="#fig:island" data-reference-type="ref" data-reference="fig:island">1.7</a>d, <a href="#fig:island" data-reference-type="ref" data-reference="fig:island">1.7</a>e, and <a href="#fig:island" data-reference-type="ref" data-reference="fig:island">1.7</a>f show the action queries requested by the heuristics ARP-bb, SCOT, and CS. Each heuristic queries the robot’s actions at specific states to assess alignment:</p>
<ul>
<li><p><strong>ARP-bb</strong>: This heuristic queries the fewest states but is myopic. It focuses on critical states derived from the ARP.</p></li>
<li><p><strong>SCOT</strong>: This heuristic generates maximally informative trajectories, querying more states than necessary but providing a comprehensive assessment.</p></li>
<li><p><strong>CS</strong>: This heuristic queries many redundant states, focusing on those where the value gap between optimal and average actions is significant.</p></li>
</ul>
<p>To pass the test given by each heuristic, the robot’s action at each of the queried states must be optimal under the human’s reward function. The example demonstrates that while the ARP-bb heuristic is efficient, it might miss the broader context. SCOT provides a thorough assessment but at the cost of querying more states. CS focuses on high-impact states but includes redundant queries.</p>
<p>It is important to note that both the construction of the ARP and the heuristics rely on having an optimal policy for the human. Thus, in most practical settings we would simply use that policy on the robot without needing to bother with value alignment verification. As such, value alignment verification as presented here is more of an academic exercise rather than a tool of practical utility.</p>
</section>
</section>
<section id="human-centered-design" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="human-centered-design"><span class="header-section-number">5.2</span> Human-Centered Design</h2>
<p>After understanding AI alignment, the next step is to explore practical methodologies for incorporating user feedback and ensuring that AI systems not only align with but also cater to the needs and preferences of their users. This section will provide insights into various Human-Centered Design techniques and their application in creating AI systems that are intuitive and ethically sound, ultimately enhancing the human-AI interaction experience.</p>
<section id="ai-and-human-computer-interaction" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="ai-and-human-computer-interaction"><span class="header-section-number">5.2.1</span> AI and Human-Computer Interaction</h3>
<p>Human-Computer Interaction (HCI) is critical in the context of artificial intelligence because it focuses on designing systems that are intuitive and responsive to human needs. While human-robot interaction and other forms of human interaction with technology are important, HCI specifically addresses the broader and more common interfaces that people interact with daily. HCI principles ensure that AI systems are not only functional but also accessible and user-friendly, making them essential for the successful integration of AI into everyday life. By focusing on HCI, we can leverage established methodologies and insights to create AI systems that are more aligned with human values and needs.</p>
<p>At the heart of this exploration is the concept of human-in-the-loop processes. As AI systems become more sophisticated, their ability to simulate human decision-making processes and behaviors has increased, leading to innovative applications across various domains. The presentation by Meredith Morris, titled "Human-in-the-loop Computing: Reimagining Human-Computer Interaction in the Age of AI," shows work in the integration of human intelligence with AI capabilities <span class="citation" data-cites="Morris2019HITL">(<a href="#ref-Morris2019HITL" role="doc-biblioref">Morris 2019</a>)</span>. Projects like Soylent and LaMPost are highlighted as exemplary cases of this integration. Soylent is a Word plugin that uses human computation to help with editing tasks, while LaMPost is a platform that leverages crowd workers to aid in natural language processing tasks <span class="citation" data-cites="bernstein2010soylent lamport2017lampost">(<a href="#ref-bernstein2010soylent" role="doc-biblioref">Bernstein et al. 2010</a>; <a href="#ref-lamport2017lampost" role="doc-biblioref">Project 2017</a>)</span>. These examples demonstrate how human input can significantly enhance AI outputs by leveraging the unique strengths of human cognition, thereby addressing complex AI problems that were previously unsolvable. For instance, Soylent can improve text quality by incorporating nuanced human feedback, and LaMPost can refine NLP tasks by incorporating human insights into language subtleties, both of which go beyond the capabilities of fully automated systems. However, the integration of human elements in AI systems brings up critical ethical considerations. The presentation discusses the changing perceptions of the ethics of human-in-the-loop processes. While the cost-effectiveness of human data labeling and other processes was once seen as beneficial, it is the ethical implications of such interactions that take precedence nowadays. This shift underscores the evolving norms in HCI and the importance of considering the ethical dimensions of human-AI interactions.</p>
<p>The role of diverse human perspectives plays a crucial role in enhancing AI systems. Involving a broad spectrum of users in the development and testing of AI systems ensures that these technologies are inclusive and representative of the global population, moving beyond the limitations of a WEIRD (Western, Educated, Industrialized, Rich, and Democratic) user base. The methodologies for collecting user feedback in HCI form a critical part of this discussion since they are vital in understanding user needs, preferences, and behaviors, which in turn inform the development of more user-centered AI systems. The presentation by Meredith Morris <span class="citation" data-cites="Morris2019HITL">(<a href="#ref-Morris2019HITL" role="doc-biblioref">Morris 2019</a>)</span> also highlights how these methods can be effectively employed to gain insights from users to ensure that AI systems are aligned with the real-world needs and expectations of users. In HCI, collecting user feedback is a fraught problem. When interacting with AI systems, the typical end user simply cares about tasks that the system can perform. Thus, a key question in HCI for AI is finding and understanding these tasks. <strong>Methodologies for collecting user feedback in HCI</strong>, are described as follow:</p>
<ul>
<li><p><strong>Storyboarding</strong> is a visual method used to predict and explore the user experience with a product or service. A storyboard in HCI is typically a sequence of drawings with annotations that represent a user’s interactions with technology. This technique is borrowed from the film and animation industry and is used in HCI to convey a sequence of events or user flows, including the user’s actions, reactions, and emotions.</p></li>
<li><p><strong>Wizard of Oz Studies</strong> is a method of user testing where participants interact with a system they believe to be autonomous, but which is actually being controlled or partially controlled by a human ‘wizard’ behind the scenes. This technique allows researchers to simulate the response of a system that may not yet be fully functional or developed.</p></li>
</ul>
<p>Both <strong>Storyboarding</strong> and <strong>Wizard of Oz Studies</strong> are effective for engaging with users early in the design process. They help deal with the problem of gathering feedback on a product that doesn’t yet exist. Users often have difficulty imagining outcomes when they cannot touch a live demonstration.</p>
<ul>
<li><p><strong>Surveys</strong> in HCI are structured tools that consist of a series of questions designed to be answered by a large number of participants. They can be conducted online, by telephone, through paper questionnaires, or using computer-assisted methods. Surveys are useful for collecting quantitative data from a broad audience, which can be analyzed statistically.</p></li>
<li><p><strong>Interviews</strong> in HCI are more in-depth and involve direct, two-way communication between the researcher and the participant. Interviews can be structured, semi-structured, or unstructured, ranging from tightly scripted question sets to open-ended conversations.</p></li>
<li><p><strong>Focus Groups</strong> involve a small group of participants discussing their experiences and opinions about a system or design, often with a moderator. Group dynamics can provide insights into collective user perspectives. In particular, users can bounce ideas off each other to provide richer feedback and quieter users who may not otherwise provide feedback may be encouraged by their peers.</p></li>
<li><p><strong>Community-Based Participatory Design (CBPD)</strong> is a human-centered approach that involves the people who will use a product in the design and development process. With CBPD, designers work closely with community members to identify problems, develop prototypes, and iterate based on community feedback. For example, when building a software product for deaf people, the engineering team can hire deaf engineers or designers to provide feedback as they collaboratively build the product.</p></li>
<li><p><strong>Field Studies</strong> involve observing and collecting data on how users interact with a system in their natural environment. This method is based on the premise that observing users in their context provides a more accurate understanding of user behavior. It can include a variety of techniques like ethnography, contextual inquiries, and natural observations.</p></li>
<li><p><strong>Lab-based studies</strong> are conducted in a controlled environment where the researchers can manipulate variables and observe user behavior in a setting designed to minimize external influences. Common lab-based methods include usability testing, controlled experiments, and eye-tracking studies.</p></li>
<li><p><strong>Diary Studies and Ethnography</strong> in HCI are a research method where participants are asked to keep a record of their interactions with a system or product over a while. This log may include text, images, and sometimes even audio or video recordings, depending on the study’s design. Participants typically document their activities, thoughts, feelings, and frustrations as they occur in their natural context.</p></li>
<li><p><strong>Ethnography</strong> is a qualitative research method that involves observing and interacting with participants in their real-life environment. Ethnographers aim to immerse themselves in the user environment to get a deep understanding of the cultural, social, and organizational contexts that shape technology use.</p></li>
</ul>
<p>As we have explored various methodologies for collecting human feedback, it becomes evident that the role of human input is indispensable in shaping AI systems that are not only effective but also ethically sound and user-centric. In the next step, we will elaborate on how to design AI systems for positive human impact, examining how socially aware and human-centered approaches can be employed to ensure that AI technologies contribute meaningfully to society. This includes understanding how AI can be utilized to address real-world challenges and create tangible benefits for individuals and communities.</p>
</section>
<section id="designing-ai-for-positive-human-impact" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="designing-ai-for-positive-human-impact"><span class="header-section-number">5.2.2</span> Designing AI for Positive Human Impact</h3>
<p>In the field of natural language processing (NLP), the primary focus has traditionally been on quantitative metrics such as performance benchmarks, accuracy, and computations. These metrics have long guided the development and evaluation of the technologies. However, as the field evolves and becomes increasingly intertwined with human interactions like the recent popularity of Large Language Models (LLMs), a paradigm shift is becoming increasingly necessary. For example, these LLMs are shown to produce unethical or harmful responses or reflect values that only represent a certain group of people. The need for a human-centered approach in NLP development is crucial as these models are much more likely to be utilized in a broad spectrum of human-centric applications, impacting various aspects of daily life. This shift calls for an inclusive framework where LLMs are not only optimized for efficiency and accuracy but are also sensitized to ethical, cultural, and societal contexts. Integrating a human-centered perspective ensures that these models are developed with a deep understanding of, and respect for, the diversity and complexity of human values and social norms. This approach goes beyond merely preventing harmful outcomes; it also focuses on enhancing the positive impact of NLP technologies on society. In this session, we explore the intricacies of a human-centered approach in NLP development, focusing on three key themes: Socially Aware, Human-Centered, and Positively Impactful.</p>
<section id="socially-aware" class="level4" data-number="5.2.2.1">
<h4 data-number="5.2.2.1" class="anchored" data-anchor-id="socially-aware"><span class="header-section-number">5.2.2.1</span> Socially Aware</h4>
<p>In the exploration of socially aware NLP, <span class="citation" data-cites="hovy-yang-2021-importance">(<a href="#ref-hovy-yang-2021-importance" role="doc-biblioref">Hovy and Yang 2021</a>)</span> presents a comprehensive taxonomy of seven social factors grounded in linguistic theory (See Figure <a href="#fig:taxonomy" data-reference-type="ref" data-reference="fig:taxonomy">1.8</a>).</p>
<div id="fig:taxonomy" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/seven-taxonomy.png" class="img-fluid figure-img" style="width:30.0%"></p>
<figcaption>Taxonomy of social factors</figcaption>
</figure>
</div>
<p>This taxonomy illustrates both the current limitations and progressions in NLP as they pertain to each of these factors. The primary aim is to motivate the NLP community to integrate these social factors more effectively, thereby advancing towards a level of language understanding that more closely resembles human capabilities. The characteristics of speakers, encompassing variables such as age, gender, ethnicity, social class, and dialect, play a crucial role in language processing. Certain languages or dialects, often categorized as low-resource, are spoken by vulnerable populations that require special consideration in NLP systems. In many cases, the dominant culture and values are over-represented, leading to an inadvertent marginalization of minority perspectives. These minority voices must be not only recognized but also given equitable representation in language models. Additionally, norms and context are vital components in understanding linguistic behavior. They dictate the appropriateness of language use in various social situations and settings. Recognizing and adapting to these norms is a critical aspect of developing socially aware NLP systems that can effectively function across diverse social environments.</p>
</section>
<section id="human-centered" class="level4" data-number="5.2.2.2">
<h4 data-number="5.2.2.2" class="anchored" data-anchor-id="human-centered"><span class="header-section-number">5.2.2.2</span> Human-Centered</h4>
<p>The Human-Centered aspect of NLP development emphasizes the creation of language models that prioritize the needs, preferences, and well-being of human users. This involves integrating human-centered design principles throughout the development stages of LLMs, which are described as follows:</p>
<ul>
<li><p><strong>Task Formulation stage:</strong> Human-centered NLP development begins with understanding the specific problems and contexts in which users operate. This involves collaborating with end-users to identify their needs and challenges, ensuring that the tasks addressed by the models are relevant and meaningful to them. By engaging with users early in the process, developers can create models that are not only technically robust but also practically useful.</p></li>
<li><p><strong>Data Collection stage:</strong> Human-centered principles ensure that the data used to train models is representative of the diverse user population. This includes collecting data from various demographic groups, languages, and cultural contexts to avoid biases that could lead to unfair or harmful outcomes. Ethical considerations are paramount, ensuring that data is collected with informed consent and respecting users’ privacy.</p></li>
<li><p><strong>Data Processing</strong> in a human-centered approach involves carefully curating and annotating data to reflect the nuances of human language and behavior. This step includes filtering out potentially harmful content, addressing imbalances in the data, and ensuring that the labels and annotations are accurate and meaningful. By involving human annotators from diverse backgrounds, developers can capture a wider range of perspectives and reduce the risk of biased outputs.</p></li>
<li><p><strong>Model Training</strong> with a human-centered focus involves incorporating feedback from users and domain experts to fine-tune the models. This iterative process ensures that the models remain aligned with users’ needs and preferences. Techniques such as active learning, where the model queries users for the most informative examples, can be employed to improve the model’s performance.</p></li>
<li><p><strong>Model Evaluation</strong> in a human-centered framework goes beyond traditional metrics like accuracy and F1-score. It includes assessing the model’s impact on users, its fairness, and its ability to handle real-world scenarios. User studies and A/B testing can provide valuable insights into how the model performs in practice and how it affects users’ experiences.</p></li>
<li><p><strong>Deployment</strong> of human-centered NLP models involves continuous monitoring and feedback loops to ensure that the models remain effective and aligned with users’ needs over time. This includes setting up mechanisms for users to report issues and provide feedback, which can then be used to update and improve the models. Ensuring transparency in how the models operate and how user data is used also fosters trust and acceptance among users.</p></li>
</ul>
</section>
<section id="positively-impactful" class="level4" data-number="5.2.2.3">
<h4 data-number="5.2.2.3" class="anchored" data-anchor-id="positively-impactful"><span class="header-section-number">5.2.2.3</span> Positively Impactful</h4>
<p>Building on the human-centered approach, it is crucial to consider how language models can be utilized and the broader impacts they can have on society.</p>
<p><strong>Utilization:</strong> LLMs offer socially beneficial applications across various domains such as public policy, mental health, and education. In public policy, they assist in analyzing large volumes of data to inform decision-making processes. In mental health, LLMs can provide personalized therapy and even train therapists by simulating patient interactions. In the education sector, they enable personalized learning experiences and language assistance, making education more accessible and effective. These examples demonstrate the versatility of LLMs in contributing positively to critical areas of human life.</p>
<p><strong>Impact:</strong> The deployment of NLP models, especially LLMs, has significant societal impacts. Positively, they enhance human productivity and creativity, offering tools and insights that streamline processes and foster innovative thinking. LLMs serve as powerful aids in various sectors, from education to industry, enhancing efficiency and enabling new forms of expression and problem-solving. it is essential to acknowledge the potential negative impacts. One major concern is the ability of LLMs to generate and spread misinformation. As these models become more adept at producing human-like text, distinguishing between AI-generated and human-created content becomes increasingly challenging. This raises issues of trust and reliability, with the risk of widespread dissemination of false or misleading information, which could have significant adverse effects on individuals and society.</p>
<p>By considering both the utilization and impact of LLMs, we can better harness their potential for positive societal contributions while mitigating the risks associated with their deployment. In conclusion, by thoughtfully integrating human-centered principles and ensuring positive impacts through feedback collection and ethical considerations, we can develop language models that not only enhance human well-being but also align closely with societal values. Building on these foundational principles, we now turn our attention to Adaptive User Interfaces, which exemplify the practical application of these concepts by personalizing interactions and improving user experiences in dynamic environments.</p>
</section>
</section>
<section id="adaptive-user-interfaces" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="adaptive-user-interfaces"><span class="header-section-number">5.2.3</span> Adaptive User Interfaces</h3>
<p>Adaptive user interfaces (AUIs) represent a significant advancement in personalizing user experiences by learning and adapting to individual preferences. This section will discuss the methodologies and applications of AUIs, highlighting their role in enhancing human-AI interaction through intelligent adaptation. The integration of AUIs within human-centered design paradigms ensures that AI systems not only meet user needs but also anticipate and adapt to their evolving preferences, thus maximizing positive human impact. Nowadays, consumers have more choices than ever and the need for personalized and intelligent assistance to make sense of the vast amount of information presented to them is clear.</p>
<section id="key-ideas" class="level4" data-number="5.2.3.1">
<h4 data-number="5.2.3.1" class="anchored" data-anchor-id="key-ideas"><span class="header-section-number">5.2.3.1</span> Key ideas</h4>
<p>In general, personalized recommendation systems require a model or profile of the user. We categorize modeling approaches into four groups.</p>
<ol type="1">
<li><p>User-created profiles (usually done manually).</p></li>
<li><p>Manually defined groups (stereotypes) that each user is classified into.</p></li>
<li><p>Automatically learned groups (stereotypes) that each user is classified into.</p></li>
<li><p>Adaptively learned individual user models from interactions with the recommendation system.</p></li>
</ol>
<p>The last approach is referred to as <em>adaptive user interfaces</em>. This approach promises that each user is given the most personalization possible, leading to better outcomes. In this session, we discuss recommendation systems that adaptively learn an individual’s preferences and use that knowledge to intelligently recommend choices that the individual is more inclined to like.</p>
<p>The problem of learning individual models can be formalized, given as follows:</p>
<ul>
<li><p>a set of tasks requiring a user decision,</p></li>
<li><p>a description for each task,</p></li>
<li><p>a history of the user’s decision on each task,</p></li>
</ul>
<p>So then we can find a function that maps from task description (features) to user decisions. The task can be described from crowd-sourced data (a collaborative approach) or the measurable features of the task (a content-based approach). The content-based approaches for describing tasks will be focused on in this session. After understanding the framework for adaptive user interfaces now it is a good point to give some example applications to help ground the future discussion. Adaptive user interfaces have been developed for</p>
<ul>
<li><p>Command and form completion</p></li>
<li><p>Email filtering and filing</p></li>
<li><p>News selection and layout</p></li>
<li><p>Browsing the internet</p></li>
<li><p>Selecting movies and TV shows</p></li>
<li><p>Online shopping</p></li>
<li><p>In-car navigation</p></li>
<li><p>Interactive scheduling</p></li>
<li><p>Dialogue systems</p></li>
</ul>
<p>among many other applications.</p>
</section>
<section id="design" class="level4" data-number="5.2.3.2">
<h4 data-number="5.2.3.2" class="anchored" data-anchor-id="design"><span class="header-section-number">5.2.3.2</span> Design</h4>
<p>The goal of an adaptive user interface is to create a software tool that reduces human effort by acquiring a user model based on past user interactions. This is analogous to the goal of machine learning (ML) which is to create a software tool that improves some task performance by acquiring knowledge based on partial task experience. The design of an adaptive user interface can be broken up into six steps:</p>
<ol type="1">
<li><p><strong>Formulating the Problem:</strong> Given some task that an intelligent system could aid, the goal is to find a formulation that lets the assistant improve its performance over time by learning from interactions with a user. In this step the designer has to make design choices about what aspect of user behavior is predicted, and what is the proper level of granularity for description (i.e.&nbsp;what is a training example). This step usually involves formulating the problem into some sort of supervised learning framework.</p></li>
<li><p><strong>Engineering the Representation:</strong> At this stage we have a formulation of a task in ML terms and we need to represent the behavior and user model in such a way that makes computational learning not only tractable but as easy as possible. In this step, the designer has to make design choices about what information is used to make predictions, and how that information is encoded and passed to the model.</p></li>
<li><p><strong>Collecting User Traces:</strong> In this third step the goal is to find an effective way to collect traces (samples) of user behavior. The designer must choose how to translate traces into training data and also how to elicit traces from a user. An ideal adaptive user interface places no extra effort on the user to collect such traces.</p></li>
<li><p><strong>Modeling the User:</strong> In this step the designer must decide what model class to use (neural network, decision tree, graphical model, etc.) and how to train the model (optimizer, step size, batch size, etc.). This step in the design process is usually given too much importance in academia. It is quite often the case that the success of an adaptive user interface is more sensitive to the other design steps.</p></li>
<li><p><strong>Using the Model Effectively:</strong> At this stage the designer must decide how the model will be integrated into a software tool. Specifically, when and how is the model evaluated and how is the output of the model presented to the user? In addition, the designer must consider how to handle situations in which the model predictions are wrong. An ideal adaptive user interface will let the user take advantage of good predictions and ignore bad ones.</p></li>
<li><p><strong>Gaining User Acceptance:</strong> The final step in the design process is to get users to try the system and ultimately adopt it. The initial attraction of users is often a marketing problem, but to retain users the system must be well-designed and easy to use.</p></li>
</ol>
</section>
<section id="applications" class="level4" data-number="5.2.3.3">
<h4 data-number="5.2.3.3" class="anchored" data-anchor-id="applications"><span class="header-section-number">5.2.3.3</span> Applications</h4>
<p>After understanding the design of Adaptive User Interfaces, let’s take a look at how we can apply it to real-world problems. We will summarize and analyze three different application areas of learning human preferences, which are driving route advisor <span class="citation" data-cites="rogers1999adaptive">(<a href="#ref-rogers1999adaptive" role="doc-biblioref">Rogers, Fiechter, and Langley 1999</a>)</span>, destination selection <span class="citation" data-cites="langley1999adaptive">(<a href="#ref-langley1999adaptive" role="doc-biblioref">Langley et al. 1999</a>)</span>, and resource scheduling <span class="citation" data-cites="gervasio1999learning">(<a href="#ref-gervasio1999learning" role="doc-biblioref">Gervasio, Iba, and Langley 1999</a>)</span>.</p>
<p><strong>1. Driving Route Advisor:</strong> The task of route selection involves determining a desirable path for a driver to take from their current location to a chosen destination, given the knowledge of available roads from a digital map. While computational route advisors exist in rental cars and online, they cannot personalize individual drivers’ preferences, which is a gap that adaptive user interfaces aim to fill by learning and recommending routes tailored to the driver’s unique choices and behaviors.</p>
<p>Here is an approach to route selection through learning individual drivers’ route preferences.</p>
<ul>
<li><p>Formulation: Learn a “subjective” function to evaluate entire routes.</p></li>
<li><p>Representation: Global route features are computable from digital maps.</p></li>
<li><p>Data collection: Preference of one complete route over another.</p></li>
<li><p>Induction: A method for learning weights from preference data.</p></li>
<li><p>Using model: Apply subjective function to find “optimal” route.</p></li>
</ul>
<p>This method aims to learn a user model that considers the entirety of a route, thereby avoiding issues like data fragmentation and credit assignment problems.</p>
<p>The design choices are incorporated into <span class="citation" data-cites="rogers1999adaptive">(<a href="#ref-rogers1999adaptive" role="doc-biblioref">Rogers, Fiechter, and Langley 1999</a>)</span>, which: models driver preferences in terms of 14 global route features; gives the driver two alternative routes he might take; lets the driver refine these choices along route dimensions; uses driver choices to refine its model of his preferences; and invokes the driver model to recommend future routes. We note that providing drivers with choices lets the system collect data on route preferences in an unobtrusive manner. The interface of the application is presented in Figure <a href="#fig:exp-1" data-reference-type="ref" data-reference="fig:exp-1">1.9</a></p>
<div id="fig:exp-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/example-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>The adaptive route advisor.</figcaption>
</figure>
</div>
<p>In driving route advisor task <span class="citation" data-cites="rogers1999adaptive">(<a href="#ref-rogers1999adaptive" role="doc-biblioref">Rogers, Fiechter, and Langley 1999</a>)</span>, a linear model is used for predicting the cost of a route based on the time, distance, number of intersections, and the number of turns. The system uses each training pair as a constraint on the weights found during the learning process. The experimental results are shown in the Figure <a href="#fig:exp-2" data-reference-type="ref" data-reference="fig:exp-2">1.10</a>.</p>
<figure id="fig:exp-2" class="figure">
<p>
<img src="Figures/example-2.png" style="width:45.0%" alt="image" class="figure-img"> <img src="Figures/example-3.png" style="width:45.0%" alt="image" class="figure-img">
</p>
<figcaption>
(Left) Experiments with 24 subjects show the Route Advisor improves its predictive ability rapidly with experience. (Right) Analyses also show that personalized user models produce better results than generalized models, even when given more data.
</figcaption>
</figure>
<p><strong>2. Destination Selection:</strong> The task of destination selection involves assisting a driver in identifying one or more suitable destinations that fulfill a specific goal, such as finding a place to eat lunch, based on the driver’s current location and knowledge of nearby options. While there are many recommendation systems online, including those for restaurants, they are not ideally suited for drivers due to the driving environment’s demand for limited visual attention, thus necessitating a more tailored and accessible approach for in-car use.</p>
<p>One approach to destination recommendation can be cast as:</p>
<ul>
<li><p>Formulation: Learn to predict features the user cares about in items.</p></li>
<li><p>Representation: Conditions/weights on attributes and values.</p></li>
<li><p>Data collection: Converse with the user to help him make decisions, noting whether he accepts or rejects questions and items.</p></li>
<li><p>Induction: Any supervised induction method.</p></li>
<li><p>Using model: Guide the dialogue by selecting informative questions and suggesting likely values.</p></li>
</ul>
<p>This design relies on the idea of a conversational user interface. Spoken-language versions of this approach appear well suited to the driving environment.</p>
<p>This approach is implemented in <span class="citation" data-cites="langley1999adaptive">(<a href="#ref-langley1999adaptive" role="doc-biblioref">Langley et al. 1999</a>)</span>, where it engages in spoken conversations to help a user refine goals; incorporates a dialogue model to constrain this process; collects and stores traces of interaction with the user; and personalizes both its questions and recommended items. Their work focused on recommending restaurants to users who want advice about where to eat. This approach to recommendation would work well for drivers, it also has broader applications. We present experimental results in</p>
<figure id="fig:exp-2.5" class="figure">
<p>
<img src="Figures/example-4.png" style="width:45.0%" alt="image" class="figure-img"> <img src="Figures/example-5.png" style="width:45.0%" alt="image" class="figure-img">
</p>
<figcaption>
(Left) Speech Acts Per Conversation. (Right) Time Per Conversation.
</figcaption>
</figure>
<p><strong>3. Resource Scheduling:</strong> The task of resource scheduling describes the challenge of allocating a limited set of resources to complete a set of tasks or jobs within a certain time frame, while also considering the constraints on both the jobs and the resources. Although automated scheduling systems are prevalent in various industries and some interactive schedulers exist, there is a distinct need for systems that can create personalized schedules reflecting the unique preferences of individual users.</p>
<p>An approach to personalized scheduling can be described as:</p>
<ul>
<li><p>Formulation: Learn a utility function to evaluate entire schedules.</p></li>
<li><p>Representation: Global features are computable from the schedule.</p></li>
<li><p>Data collection: Preference of one candidate schedule over others.</p></li>
<li><p>Induction: A method for learning weights from preference data.</p></li>
<li><p>Using model: Apply the ‘subjective’ function to find a good schedule.</p></li>
</ul>
<p>We note that this method is similar to that in the Adaptive Route Advisor. However, it assumes a search through a space of complete schedules (a repair space), which requires some initial schedule. This approach is implemented in <span class="citation" data-cites="gervasio1999learning">(<a href="#ref-gervasio1999learning" role="doc-biblioref">Gervasio, Iba, and Langley 1999</a>)</span>, where the interactive scheduler retrieves an initial schedule from a personalized case library; suggests to the user improved schedules from which to select; lets the user direct search to improve on certain dimensions; collects user choices to refine its personalized utility function; stores solutions in the case base to initialize future schedules; and invokes the user model to recommend future schedule repairs. As before, providing users with choices lets the system collect data on schedule preferences unobtrusively. An example of the interface, and the experimental results are shown in Figure <a href="#fig:exp-3" data-reference-type="ref" data-reference="fig:exp-3">1.12</a></p>
<figure id="fig:exp-3" class="figure">
<p>
<img src="Figures/example-7.png" style="width:45.0%" alt="image" class="figure-img"> <img src="Figures/example-6.png" style="width:45.0%" alt="image" class="figure-img">
</p>
<figcaption>
(Left) The interface of the INCA: Interactive Scheduling <span class="citation" data-cites="gervasio1999learning"></span>. (Right) Experiments with INCA suggest that retrieving personalized schedules helps users more as task difficulty increases. These experimental studies used a mixture of human and synthetic subjects.
</figcaption>
</figure>
</section>
<section id="limitations" class="level4" data-number="5.2.3.4">
<h4 data-number="5.2.3.4" class="anchored" data-anchor-id="limitations"><span class="header-section-number">5.2.3.4</span> Limitations</h4>
<p>The challenges of adaptive interfaces may involve: conceptualizing user modeling as a task suitable for inductive learning, crafting representations that facilitate the learning process, gathering training data from users in a way that doesn’t intrude on their experience, applying the learned user model effectively, ensuring the system can learn in real-time, and dealing with the necessity of learning from a limited number of training instances. These challenges are not only pertinent to adaptive interfaces but also intersect with broader applications of machine learning, while also introducing some unique issues. However, new sensor technology can bring promises to adaptive interfaces. Adaptive interfaces rely on user traces to drive their modeling process, so they stand to benefit from developments like GPS and cell phone locators, robust software for speech recognition, accurate eye and head trackers, real-time video interpreters, wearable body sensors (GSR, heart rate), and portable brain-wave sensors. As those devices become more widespread, they will offer new sources of data and support new types of adaptive services. In addition, adaptive interfaces can be viewed as a form of cognitive simulation that automatically generates knowledge structures to learn user preferences. They are capable of making explicit predictions about future user behavior and explaining individual differences through the process of personalization. This perspective views adaptive interfaces as tools that not only serve functional purposes but also model the psychological aspects of user interaction. Two distinct approaches within cognitive simulation are related to adaptive interfaces: <em>process</em> models that incorporate fundamental architectural principles, and <em>content</em> models that operate at the knowledge level, focusing on behavior. We note that both of them have roles to play, but content models are more relevant to personalization and adaptive interfaces.</p>
<p>In conclusion, adaptive user interfaces represent a significant advancement in creating personalized and efficient interactions between humans and technology. By leveraging modern sensor technologies and cognitive simulation approaches, these interfaces can dynamically learn and adapt to individual user preferences, enhancing overall user experience and system effectiveness. The methodologies discussed, from conceptualizing user models to collecting and utilizing user feedback, form the foundation of this innovative approach. As we transition to the next section, we will explore practical applications and real-world implementations of these human-centered AI principles through detailed case studies, illustrating the tangible impact of adaptive interfaces in various domains.</p>
</section>
</section>
<section id="case-studies-in-human-centered-ai" class="level3" data-number="5.2.4">
<h3 data-number="5.2.4" class="anchored" data-anchor-id="case-studies-in-human-centered-ai"><span class="header-section-number">5.2.4</span> Case Studies in Human-Centered AI</h3>
<p>In this section, we examine practical examples that illustrate the application of human-centered principles in the development and deployment of AI systems. By examining these case studies, we aim to provide concrete insights into how AI technologies can be designed and implemented to better align with human values, enhance inclusivity, and address the specific needs of diverse user groups. The following case studies highlight different approaches and methodologies used to ensure that AI systems are not only effective but also considerate of the human experience.</p>
<section id="lampost-case-study" class="level4" data-number="5.2.4.1">
<h4 data-number="5.2.4.1" class="anchored" data-anchor-id="lampost-case-study"><span class="header-section-number">5.2.4.1</span> LaMPost Case Study</h4>
<p>In our exploration of human-centered AI design, it is crucial to examine how metrics can be improved to better capture the human experience and address the shortcomings of traditional evaluation methods. The LaMPost case study <span class="citation" data-cites="goodman_lampost_2022">(<a href="#ref-goodman_lampost_2022" role="doc-biblioref">Goodman et al. 2022</a>)</span> exemplifies this effort by focusing on the development of an AI assistant designed to aid individuals with dyslexia in writing emails. This case is particularly relevant to our discussion because it highlights the importance of human-centered principles in AI development, especially in creating tools that cater to specific cognitive differences and enhance user experience.</p>
<p>Dyslexia is a cognitive difference that affects approximately 15 percent of language users, with varying degrees of impact on speaking, spelling, and writing abilities. It is a spectrum disorder, meaning symptoms and severity differ among individuals. More importantly, dyslexia is not an intellectual disability; many individuals with dyslexia possess high intelligence. Given the significant number of people affected by dyslexia, it is essential to develop AI tools that support their unique needs and enhance their daily tasks.</p>
<p>The LaMPost project sought to answer the question, “How can LLMs be applied to enhance the writing workflows of adults with dyslexia?” To address this, researchers employed a participatory design approach, involving employees with dyslexia from their company (Google) in the study. This approach ensured that the development process was inclusive and responsive to the actual needs and preferences of the dyslexic community. By focusing on the real-world application of LLMs in aiding email writing for dyslexic individuals, LaMPost serves as a powerful example of how AI can be designed to better capture and enhance the human experience.</p>
<p>The figure below allows users to see suggestions for rewriting selected text, helping them identify main ideas, suggest possible changes, and rewrite their selections to improve clarity and expression.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/lampost_fig3.png" class="img-fluid figure-img"></p>
<figcaption>The Suggest Possible Changes feature from LaMPost.</figcaption>
</figure>
</div>
<p>The table below categorizes the challenges faced by users at different writing levels and the strategies they can use to overcome these challenges, illustrating the varied support needs addressed by LaMPost</p>
<figure class="figure">
<table>
<thead>
<tr>
<th style="text-align: center;">
Writing level
</th>
<th style="text-align: center;">
Examples of Challenges
</th>
<th style="text-align: center;">
Strategies
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">
high
</td>
<td style="text-align: center;">
expressing ideas
</td>
<td style="text-align: center;">
“word faucet”, ASR dictation
</td>
</tr>
<tr>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
ordering ideas
</td>
<td style="text-align: center;">
post-it outlining
</td>
</tr>
<tr>
<td style="text-align: center;">
low
</td>
<td style="text-align: center;">
appropriate language
</td>
<td style="text-align: center;">
proofreading
</td>
</tr>
<tr>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
paraphrasing
</td>
<td style="text-align: center;">
feedback
</td>
</tr>
</tbody>
</table>
<figcaption>
User challenged and strategies in LaMPost.
</figcaption>
</figure>
<p>Next, they ran a focus group to get initial ideas from members of the dyslexic community. This focus group helped them figure out what to measure and added the second research question: “How do adults with dyslexia feel about LLM-assisted writing?” In other words, how does the LLM impact users’ feelings of satisfaction, self-expression, self-efficacy, autonomy, and control?</p>
<p>From this focus group, they went and created a prototype to answer the desires of the group. They included three features in their prototype model. One feature was: <em>identifying main ideas</em>. They focused on this to support overall clarity and organization of high-level ideas of the user. Another feature was <em>suggest possible changes</em>. They focused on this because users wanted to identify high-level adjustments to improve their writing. The last feature they added was <em>rewrite my selections</em>. They added this because users wanted help expressing ideas with a desired phrasing tone or style. This feature generated a rewrite based on a command you gave it.</p>
<p>With the prototype, the researchers evaluated again with 19 participants with dyslexia from outside their organization. They did a three-part study, including a demonstration and background on the system (25 min). Then they did a writing exercise with two real tasks (emails) each user had to do in the real world (25 min). For example, one task might have been to write an email to the principal of their child’s school to ask for a meeting. Then, the researchers did another follow-up interview for more qualitative data, e.g.&nbsp;to ask about specific choices users made when interacting with the model (25 min).</p>
<p>LaMPost’s design prioritized autonomy by allowing users to choose the best option for their writing. One successful thing is that most users felt in control while writing. Users found that numerous options were helpful to filter through poor results. However, participants said the selection process was cognitively demanding and time-consuming. As we all know, features identified in LaMPost are all over the place, such as in Google Docs. Nonetheless, there remain many questions about the balance between automated writing and providing more control to the end users.</p>
<div class="tcolorbox">
<p>How could researchers hone in on this trade-off between <strong>the ease of automated writing</strong> and <strong>providing control to end-users</strong>?<br>
You will need to design a study to approach this question.</p>
<ul>
<li><p>Identify your research question, hypotheses, and the methods that you will use. (Hint: use the HCI methods described in the previous section.)</p></li>
<li><p>Scope the domain of your study appropriately—more broadly than dyslexia but not so broadly to be meaningless.</p></li>
<li><p>What domains will you include? (E.g. students use ChatGPT for assignments, doctors use an LLM to write notes, etc.)</p></li>
</ul>
</div>
<p>In this way, both the case study of LaMPost and its presaging of greater trends in LLM interfaces recapitulate the maxim of HCI: HCI is a cycle. You design a potential system, prototype it, get feedback from people, and iterate constantly. Next, we will explore two case studies that exemplify the application of human-centered principles in NLP. These case studies illustrate how LLMs can be adapted to foster social inclusivity and provide training in social skills.</p>
</section>
<section id="multi-value-and-dada-cross-dialectal-english-nlp" class="level4" data-number="5.2.4.2">
<h4 data-number="5.2.4.2" class="anchored" data-anchor-id="multi-value-and-dada-cross-dialectal-english-nlp"><span class="header-section-number">5.2.4.2</span> Multi-Value and DaDa: Cross-Dialectal English NLP</h4>
<p>English NLP systems are largely trained to perform well in Standard American English - the form of written English found in professional settings and elsewhere. Not only is Standard American English the most well-represented form of English in textual datasets but NLP engineers and researchers often filter dialectal and vernacular English examples from their datasets to improve performance on SAE benchmarks. As a result, NLP systems are generally less performant when processing dialectal inputs than SAE inputs. This performance gap is observable over various benchmarks and tasks, like the SPIDER benchmark. <span class="citation" data-cites="spider">(<a href="#ref-spider" role="doc-biblioref">Chang et al. 2023</a>)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/MV2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Stress test reveals worse performance on the SPIDER benchmark with synthetic dialectical examples than with SAE.</figcaption>
</figure>
</div>
<p>As natural language systems become more pervasive, this performance gap increasingly represents a real allocational harm against dialectal English speakers — these speakers are excluded from using helpful systems and assistants. Multi-Value is a framework for evaluating foundation language models on dialectic input, and DADA is a framework for adapting LLMs to improve performance on dialectic input.</p>
<p><strong>Synthetic Dialectal Data</strong></p>
<p>Ziems et al.&nbsp;(2023) create synthetic dialectal data for several English dialects (Appalachian English, Chicano English, Indian English, Colloquial Singapore English, and Urban African American English).<span class="citation" data-cites="mv">(<a href="#ref-mv" role="doc-biblioref">Ziems et al. 2023</a>)</span> They created synthetic data based on transforming SAE examples to have direct evaluation comparisons. These synthetic examples were created by leveraging known linguistic features of the dialects, such as negative concord in UAAVE. Figure <a href="#fig:features_dialects" data-reference-type="ref" data-reference="fig:features_dialects">1.13</a> maps out the presence of various linguistic features.</p>
<div id="fig:features_dialects" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/MV1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>A comparative distribution of features in five dialects.</figcaption>
</figure>
</div>
<p>This synthetic data, while somewhat limited in the variety of samples. can produce and create realistic examples for benchmarking LM performance. Figure <a href="#fig:synthetic_example" data-reference-type="ref" data-reference="fig:synthetic_example">1.14</a> demonstrates creating a synthetic dialectic example using the ‘give passive’ linguistic feature, illustrating the transformation process from SAE to a vernacular form.</p>
<div id="fig:synthetic_example" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/MV3.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>Execution of a sample transform using a documented linguistic feature.</figcaption>
</figure>
</div>
<p><strong>Feature Level Adapters</strong> One approach to the LLM adaption task would be to train an adapter for each dialect using a parameter-efficient fine-tuning method like low-rank adapters. <span class="citation" data-cites="lora">(<a href="#ref-lora" role="doc-biblioref">Hu et al. 2021</a>)</span> While adapters can certainly bridge the gap between SAE LMs and dialect inputs, this approach suffers from a couple of weaknesses, namely:</p>
<ul>
<li><p>Individually trained adapters do not leverage similarities between low-resource dialects. Transfer learning is often helpful for training low-resource languages and dialects.</p></li>
<li><p>The model needs to know which adapter to use at inference time. This presupposes that we can accurately classify the dialect — sometimes based on as little as one utterance. This classification is not always possible — a more general approach is needed.</p></li>
</ul>
<p>Therefore, Liu et al.&nbsp;(2023) propose a novel solution — DADA: Dialect Adaption via Dynamic Aggregation of Linguistic Rules. <span class="citation" data-cites="dada">(<a href="#ref-dada" role="doc-biblioref">Liu, Held, and Yang 2023</a>)</span> DADA trains adapters on the linguistic feature level rather than the dialect level. The model can use multiple linguistic feature adapters via an additional fusion layer. They can therefore train using multi-dialectical data and cover linguistic variation via a comprehensive set of roughly 200 adapters. DADA saw an improvement in performance over single-dialect adapters for most dialects, as shown in <a href="#fig:dada_performance" data-reference-type="ref" data-reference="fig:dada_performance">1.15</a>.</p>
<div id="fig:dada_performance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/MV4.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>Execution of a sample transform using a documented linguistic feature.</figcaption>
</figure>
</div>
<p>The Multi-Value and DADA case study underscores the importance of designing NLP systems that are inclusive and representative of diverse language users. By addressing the performance gaps in handling dialectal inputs, this case study highlights the necessity of incorporating diverse linguistic data and creating adaptable systems. This approach enhances AI functionality and accessibility, ensuring it respects and reflects linguistic diversity. Ultimately, the study reinforces human-centered design principles, demonstrating how AI can be tailored to better serve and empower all users. Moving forward, we will explore how LLMs can be utilized for social skill training, showcasing their potential to improve human interactions.</p>
</section>
<section id="social-skill-training-via-llms" class="level4" data-number="5.2.4.3">
<h4 data-number="5.2.4.3" class="anchored" data-anchor-id="social-skill-training-via-llms"><span class="header-section-number">5.2.4.3</span> Social Skill Training via LLMs</h4>
<p>The emergence of Large Language Models (LLMs) marks a significant milestone in the field of social skills training. This case study explores the potential of LLMs to augment social skill development across diverse scenarios. More specifically, we discuss a dual-framework approach, where two distinct LLMs operate in tandem as a Partner and a Mentor, guiding human learners in their journey towards improved social interaction. In this framework, we have two agents which are</p>
<ul>
<li><p><strong>AI Partner</strong>: LLM-empowered agents that users can engage with across various topics. This interactive model facilitates practical, conversation-based learning, enabling users to experiment with different communication styles and techniques or practice and develop specific skills in real-world scenarios in a safe, AI-mediated environment.</p></li>
<li><p><strong>AI Mentor</strong>: An LLM-empowered entity designed to provide constructive, personalized feedback based on the interaction of users and the AI Partner. This mentor analyzes conversation dynamics, identifies areas for improvement, offers tailored advice, and guides users toward effective social strategies and improved interaction skills.</p></li>
</ul>
<p>For example, in conflict resolution, individuals learning to handle difficult conversations can use the AI Partner to simulate interactions with a digitalized partner. As a Conflict Resolution Expert, the AI Mentor helps analyze these interactions, offering strategies to navigate conflicts effectively.</p>
<p>In the educational sector, K-12 teachers aiming to incorporate more growth-mindset language into their teaching can practice with a digitalized student. An experienced teacher or mentor, represented by the AI Mentor, provides insights on effective communication and teaching methods. For negotiation training, students preparing to negotiate their first job offers can engage in simulated negotiations with a digitalized HR representative through the AI Partner. As a Negotiation Expert, the AI Mentor then offers guidance on negotiation tactics, helping students effectively articulate their values and negotiate job terms. Lastly, in therapy training, novice therapists can interact with a digitalized patient via the AI Partner to practice therapy sessions. The AI Mentor, functioning as a Therapy Coach, then reviews these sessions, providing feedback and suggestions on enhancing therapeutic techniques and patient engagement.</p>
<p><strong>CARE: Therapy Skill Training</strong> Hsu et al.&nbsp;(2023) introduced CARE <span class="citation" data-cites="hsu2023helping">(<a href="#ref-hsu2023helping" role="doc-biblioref">Hsu et al. 2023</a>)</span>, a framework designed for therapy skill training. This framework leverages a simulated environment, enabling counselors to practice their skills without the risk of harming real individuals. An integral component of CARE is the AI Mentor, which offers invaluable feedback and guidance during the training process. See Figure <a href="#fig:care" data-reference-type="ref" data-reference="fig:care">1.16</a> for the overview of the framework.</p>
<div id="fig:care" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/care.png" class="img-fluid figure-img" style="width:45.0%"></p>
<figcaption>CARE Framework</figcaption>
</figure>
</div>
<p>CARE’s primary function is for novice therapists and counselors to assess and determine the most effective counseling strategies tailored to specific contexts. It provides counselors with customized example responses, which they can adopt, adapt, or disregard when interacting with a simulated support seeker. This approach is deeply rooted in the principles of Motivational Interviewing and utilizes a rich dataset of counseling conversations combined with LLMs. The effectiveness of CARE has been established through rigorous quantitative evaluations and qualitative user studies, which included simulated chats and semi-structured interviews. Notably, CARE has shown significant benefits in aiding novice counselors. From the assessment, counselors chose to use CARE 93% of the time, directly used a CARE response without editing 60% of the time, and sent more extended responses with CARE. Qualitatively, counselors noted several advantages of CARE, such as its ability to refresh memory on various strategies, inspire innovative responses, boost confidence, and save time during consultations. However, there were some drawbacks, including potential disruptions in the thought process, perceived limitations in response options, the requirement for decision-making, and the time needed to review suggestions. Overall, the framework is particularly beneficial for therapists new to the field, offering them a supportive and educational tool to enhance their counseling skills effectively.</p>
</section>
</section>
</section>
<section id="practice-exercises" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="practice-exercises"><span class="header-section-number">5.3</span> Practice Exercises</h2>


</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-amodei2016concrete" class="csl-entry" role="listitem">
Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. 2016. <span>“Concrete Problems in AI Safety.”</span> <em>arXiv Preprint arXiv:1606.06565</em>.
</div>
<div id="ref-angwin_machine_2016" class="csl-entry" role="listitem">
Angwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. <span>“Machine Bias.”</span> <em>ProPublica</em>.
</div>
<div id="ref-arcas_can_2022" class="csl-entry" role="listitem">
Arcas, Blaise Aguera y. 2022. <span>“Can Machines Learn How to Behave?”</span> <em>Medium</em>. <a href="https://medium.com/@blaisea/can-machines-learn-how-to-behave-42a02a57fadb">https://medium.com/@blaisea/can-machines-learn-how-to-behave-42a02a57fadb</a>.
</div>
<div id="ref-aristotle_nicomachean_350" class="csl-entry" role="listitem">
Aristotle. 350 B.C.E. <em>Nicomachean Ethics</em>. translated by W.D. Ross.
</div>
<div id="ref-bai_constitutional_2022" class="csl-entry" role="listitem">
Bai, Yuntao, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, and Cameron McKinnon. 2022. <span>“Constitutional Ai: <span>Harmlessness</span> from Ai Feedback.”</span> <em>arXiv Preprint arXiv:2212.08073</em>.
</div>
<div id="ref-barocas_fairness_2019" class="csl-entry" role="listitem">
Barocas, Solon, Moritz Hardt, and Arvind Narayanan. 2019. <em>Fairness and Machine Learning</em>. fairmlbook.org.
</div>
<div id="ref-bernstein2010soylent" class="csl-entry" role="listitem">
Bernstein, Michael S., Greg Little, Robert C. Miller, Bjorn Hartmann, Mark S. Ackerman, David R. Karger, David Crowell, and Katrina Panovich. 2010. <span>“Soylent: A Word Processor with a Crowd Inside.”</span> In <em>Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology</em>. ACM.
</div>
<div id="ref-binns_fairness_2018" class="csl-entry" role="listitem">
Binns, Reuben. 2018. <span>“Fairness in Machine Learning: Lessons from Political Philosophy.”</span> In <em>Proceedings of the 2018 Conference on Fairness, Accountability, and Transparency</em>, 149–59.
</div>
<div id="ref-bostrom2014superintelligence" class="csl-entry" role="listitem">
Bostrom, Nick. 2014a. <em>Superintelligence: Paths, Dangers, Strategies</em>. Oxford University Press.
</div>
<div id="ref-bostrom_superintelligence_2014" class="csl-entry" role="listitem">
———. 2014b. <em>Superintelligence: Paths, Dangers, Strategies</em>. Oxford University Press.
</div>
<div id="ref-brown2019machine" class="csl-entry" role="listitem">
Brown, Daniel S, and Scott Niekum. 2019. <span>“Machine Teaching for Inverse Reinforcement Learning: Algorithms and Applications.”</span> In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 33:7749–58.
</div>
<div id="ref-brown2021value" class="csl-entry" role="listitem">
Brown, Daniel S, Jordan Schneider, Anca Dragan, and Scott Niekum. 2021. <span>“Value Alignment Verification.”</span> In <em>International Conference on Machine Learning</em>, 1105–15. PMLR.
</div>
<div id="ref-tuskegee" class="csl-entry" role="listitem">
Centers for Disease Control and Prevention. 2023. <span>“The u.s. Public Health Service Untreated Syphilis Study at Tuskegee.”</span> <a href="https://www.cdc.gov/tuskegee/index.html" class="uri">https://www.cdc.gov/tuskegee/index.html</a>.
</div>
<div id="ref-spider" class="csl-entry" role="listitem">
Chang, Shuaichen, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li, Wuwei Lan, et al. 2023. <span>“Dr.spider: A Diagnostic Evaluation Benchmark Towards Text-to-SQL Robustness.”</span> <a href="https://arxiv.org/abs/2301.08881">https://arxiv.org/abs/2301.08881</a>.
</div>
<div id="ref-chowdhery_palm_2022" class="csl-entry" role="listitem">
Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2022. <span>“<span>PaLM</span>: <span>Scaling</span> <span>Language</span> <span>Modeling</span> with <span>Pathways</span>.”</span> <em>arXiv:2204.02311 [Cs]</em>, April. <a href="http://arxiv.org/abs/2204.02311">http://arxiv.org/abs/2204.02311</a>.
</div>
<div id="ref-christianoclarifying" class="csl-entry" role="listitem">
Christiano, Paul. 2018. <span>“Clarifying <span>‘AI Alignment’</span>.”</span> <a href="https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6">https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6</a>.
</div>
<div id="ref-christiano2017deep" class="csl-entry" role="listitem">
Christiano, Paul F, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. <span>“Deep Reinforcement Learning from Human Preferences.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-clark2016faulty" class="csl-entry" role="listitem">
Clark, Jack, and Dario Amodei. 2016. <span>“Faulty Reward Functions in the Wild.”</span> <em>OpenAI Blog</em>.
</div>
<div id="ref-dworkin1988theory" class="csl-entry" role="listitem">
Dworkin, Gerald. 1988. <em>The Theory and Practice of Autonomy</em>. Cambridge University Press.
</div>
<div id="ref-everitt2018alignment" class="csl-entry" role="listitem">
Everitt, Tom, and Marcus Hutter. 2018. <span>“The Alignment Problem for Artificial Intelligence.”</span> In <em>Advances in Neural Information Processing Systems</em>, 1–8.
</div>
<div id="ref-floridi2011ethics" class="csl-entry" role="listitem">
Floridi, Luciano. 2011a. <em>The Ethics of Information</em>. Oxford University Press.
</div>
<div id="ref-floridi_ethics_2011" class="csl-entry" role="listitem">
———. 2011b. <em>The Ethics of Information</em>. Oxford University Press.
</div>
<div id="ref-frankena1973ethics" class="csl-entry" role="listitem">
Frankena, William K. 1973. <em>Ethics</em>. Prentice Hall.
</div>
<div id="ref-friedman_value_2008" class="csl-entry" role="listitem">
Friedman, Batya, Peter H. Kahn, and Alan Borning. 2008. <span>“Value Sensitive Design and Information Systems.”</span> In <em>The Handbook of Information and Computer Ethics</em>. John Wiley &amp; Sons.
</div>
<div id="ref-gervasio1999learning" class="csl-entry" role="listitem">
Gervasio, Melinda T, Wayne Iba, and Pat Langley. 1999. <span>“Learning User Evaluation Functions for Adaptive Scheduling Assistance.”</span> In <em>ICML</em>, 152–61. Citeseer.
</div>
<div id="ref-goodall_machine_2014" class="csl-entry" role="listitem">
Goodall, Noah J. 2014. <span>“Machine Ethics and Automated Vehicles.”</span> In <em>Road Vehicle Automation</em>, 93–102. Springer.
</div>
<div id="ref-goodman_lampost_2022" class="csl-entry" role="listitem">
Goodman, Steven, Erin Buehler, Patrick Clary, Andy Coenen, Aaron Michael Donsbach, Tiffanie Horne, Michal Lahav, et al. 2022. <span>“LaMPost: Evaluation of an AI-Assisted Writing Email Editor Prototype for Adults with Dyslexia.”</span>
</div>
<div id="ref-hadfield2016cooperative" class="csl-entry" role="listitem">
Hadfield-Menell, Dylan, Stuart J Russell, Pieter Abbeel, and Anca Dragan. 2016. <span>“Cooperative Inverse Reinforcement Learning.”</span> <em>Advances in Neural Information Processing Systems</em> 29.
</div>
<div id="ref-hardt_patterns_2021" class="csl-entry" role="listitem">
Hardt, Moritz, and Benjamin Recht. 2021. <span>“Patterns, Predictions, and Actions: A Story about Machine Learning.”</span> <em>arXiv Preprint arXiv:2102.05242</em>.
</div>
<div id="ref-vanhasselt_deep_2018" class="csl-entry" role="listitem">
Hasselt, Hado van, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. 2018. <span>“Deep Reinforcement Learning and the Deadly Triad.”</span>
</div>
<div id="ref-hejna2023contrastive" class="csl-entry" role="listitem">
Hejna, Joey, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, and Dorsa Sadigh. 2023. <span>“Contrastive Preference Learning: Learning from Human Feedback Without RL.”</span> <a href="https://arxiv.org/abs/2310.13639">https://arxiv.org/abs/2310.13639</a>.
</div>
<div id="ref-hendrycks_aligning_2021" class="csl-entry" role="listitem">
Hendrycks, Dan, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020. <span>“Aligning Ai with Shared Human Values.”</span> <em>arXiv Preprint arXiv:2008.02275</em>.
</div>
<div id="ref-hendrycks_what_2021" class="csl-entry" role="listitem">
Hendrycks, Dan, Mantas Mazeika, Andy Zou, Sahil Patel, Christine Zhu, Jesus Navarro, Dawn Song, Bo Li, and Jacob Steinhardt. 2021. <span>“What <span>Would</span> <span>Jiminy</span> <span>Cricket</span> <span>Do</span>? <span>Towards</span> <span>Agents</span> <span>That</span> <span>Behave</span> <span>Morally</span>.”</span> <em>arXiv:2110.13136 [Cs]</em>. <a href="http://arxiv.org/abs/2110.13136">http://arxiv.org/abs/2110.13136</a>.
</div>
<div id="ref-hovy-yang-2021-importance" class="csl-entry" role="listitem">
Hovy, Dirk, and Diyi Yang. 2021. <span>“The Importance of Modeling Social Factors of Language: Theory and Practice.”</span> In <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, edited by Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, 588–602. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2021.naacl-main.49">https://doi.org/10.18653/v1/2021.naacl-main.49</a>.
</div>
<div id="ref-hsu2023helping" class="csl-entry" role="listitem">
Hsu, Shang-Ling, Raj Sanjay Shah, Prathik Senthil, Zahra Ashktorab, Casey Dugan, Werner Geyer, and Diyi Yang. 2023. <span>“Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback.”</span> <a href="https://arxiv.org/abs/2305.08982">https://arxiv.org/abs/2305.08982</a>.
</div>
<div id="ref-lora" class="csl-entry" role="listitem">
Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. <span>“LoRA: Low-Rank Adaptation of Large Language Models.”</span> <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a>.
</div>
<div id="ref-huang2018establishing" class="csl-entry" role="listitem">
Huang, Sandy H, Kush Bhatia, Pieter Abbeel, and Anca D Dragan. 2018. <span>“Establishing Appropriate Trust via Critical States.”</span> In <em>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 3929–36. IEEE.
</div>
<div id="ref-hubinger2019introduction" class="csl-entry" role="listitem">
Hubinger, Evan, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. 2019. <span>“An Introduction to Inner Alignment.”</span> <em>arXiv Preprint arXiv:1906.01820</em>.
</div>
<div id="ref-jiang_artificial_2017" class="csl-entry" role="listitem">
Jiang, Fei, Yong Jiang, Hang Zhi, Yuan Dong, Hui Li, Shugang Ma, and Yongan Wang. 2017. <span>“Artificial Intelligence in Healthcare: Past, Present and Future.”</span> <em>Stroke and Vascular Neurology</em> 2 (4): 230–43.
</div>
<div id="ref-jiang_delphi_2021" class="csl-entry" role="listitem">
Jiang, Liwei, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. 2021. <span>“Delphi: <span>Towards</span> <span>Machine</span> <span>Ethics</span> and <span>Norms</span>.”</span> <em>arXiv:2110.07574 [Cs]</em>, October. <a href="http://arxiv.org/abs/2110.07574">http://arxiv.org/abs/2110.07574</a>.
</div>
<div id="ref-johnson_kants_2022" class="csl-entry" role="listitem">
Johnson, Robert, and Adam Cureton. 2022. <span>“Kant’s <span>Moral</span> <span>Philosophy</span>.”</span> In <em>The <span>Stanford</span> <span>Encyclopedia</span> of <span>Philosophy</span></em>, edited by Edward N. Zalta and Uri Nodelman, Fall 2022. Metaphysics Research Lab, Stanford University. <a href="https://plato.stanford.edu/archives/fall2022/entries/kant-moral/">https://plato.stanford.edu/archives/fall2022/entries/kant-moral/</a>.
</div>
<div id="ref-krakovna2020specification" class="csl-entry" role="listitem">
Krakovna, Victoria et al. 2020. <span>“Specification Gaming Examples in AI.”</span> <em>DeepMind Safety Research</em>.
</div>
<div id="ref-langley1999adaptive" class="csl-entry" role="listitem">
Langley, Pat, Cynthia Thompson, Renee Elio, and Afsaneh Haddadi. 1999. <span>“An Adaptive Conversational Interface for Destination Advice.”</span> In <em>International Workshop on Cooperative Information Agents</em>, 347–64. Springer.
</div>
<div id="ref-leike2018scalable" class="csl-entry" role="listitem">
Leike, Jan, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. 2018. <span>“Scalable Agent Alignment via Reward Modeling: A Research Direction.”</span> <a href="https://arxiv.org/abs/1811.07871">https://arxiv.org/abs/1811.07871</a>.
</div>
<div id="ref-liang_holistic_2023" class="csl-entry" role="listitem">
Liang, Percy, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, et al. 2023. <span>“Holistic <span>Evaluation</span> of <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2211.09110">https://doi.org/10.48550/arXiv.2211.09110</a>.
</div>
<div id="ref-dada" class="csl-entry" role="listitem">
Liu, Yanchen, William Held, and Diyi Yang. 2023. <span>“DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules.”</span> <a href="https://arxiv.org/abs/2305.13406">https://arxiv.org/abs/2305.13406</a>.
</div>
<div id="ref-mazeika_how_2022" class="csl-entry" role="listitem">
Mazeika, Mantas, Eric Tang, Andy Zou, Steven Basart, Jun Shern Chan, Dawn Song, David Forsyth, Jacob Steinhardt, and Dan Hendrycks. 2022. <span>“How <span>Would</span> <span>The</span> <span>Viewer</span> <span>Feel</span>? <span>Estimating</span> <span>Wellbeing</span> <span>From</span> <span>Video</span> <span>Scenarios</span>.”</span> <em>arXiv Preprint arXiv:2210.10039</em>.
</div>
<div id="ref-mehrabi_survey_2021" class="csl-entry" role="listitem">
Mehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. <span>“A Survey on Bias and Fairness in Machine Learning.”</span> <em>ACM Computing Surveys (CSUR)</em> 54 (6): 1–35.
</div>
<div id="ref-mill_utilitarianism_1863" class="csl-entry" role="listitem">
Mill, John Stuart. 1863. <em>Utilitarianism</em>. Parker, Son,; Bourn.
</div>
<div id="ref-moerland_emotion_2018" class="csl-entry" role="listitem">
Moerland, Thomas M, Joost Broekens, and Catholijn M Jonker. 2018. <span>“Emotion in Reinforcement Learning Agents and Robots: A Survey.”</span> <em>Machine Learning</em> 107: 443–80.
</div>
<div id="ref-Morris2019HITL" class="csl-entry" role="listitem">
Morris, Meredith Ringel. 2019. <span>“Human-in-the-Loop Computing: Reimagining Human-Computer Interaction in the Age of AI.”</span> In <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>. ACM.
</div>
<div id="ref-muller_participatory_2003" class="csl-entry" role="listitem">
Muller, Michael J. 2003. <span>“Participatory Design: The Third Space in HCI.”</span> In <em>The Human-Computer Interaction Handbook</em>. CRC Press.
</div>
<div id="ref-ngo2023alignment" class="csl-entry" role="listitem">
Ngo, Richard, Lawrence Chan, and Sören Mindermann. 2023. <span>“The Alignment Problem from a Deep Learning Perspective.”</span> <a href="https://arxiv.org/abs/2209.00626">https://arxiv.org/abs/2209.00626</a>.
</div>
<div id="ref-noble_algorithms_2018" class="csl-entry" role="listitem">
Noble, Safiya Umoja. 2018. <em>Algorithms of Oppression: How Search Engines Reinforce Racism</em>. NYU Press.
</div>
<div id="ref-nussbaum1993quality" class="csl-entry" role="listitem">
Nussbaum, Martha C, and Amartya Sen. 1993. <em>The Quality of Life</em>. Oxford University Press.
</div>
<div id="ref-oneil_weapons_2016" class="csl-entry" role="listitem">
O’Neil, Cathy. 2016. <em>Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy</em>. Crown Publishing Group.
</div>
<div id="ref-ouyang_training_2022" class="csl-entry" role="listitem">
Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span>
</div>
<div id="ref-lamport2017lampost" class="csl-entry" role="listitem">
Project, LaMPort. 2017. <span>“LaMPost: Leveraging Crowdsourcing for Natural Language Processing.”</span> In <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>. ACL.
</div>
<div id="ref-quine_word_1960" class="csl-entry" role="listitem">
Quine, Willard Van Orman. 1960. <em>Word and Object</em>. MIT Press. <a href="https://openlibrary.org/works/OL2910272W?edition=ia%3Awordobject00quin">https://openlibrary.org/works/OL2910272W?edition=ia%3Awordobject00quin</a>.
</div>
<div id="ref-rawls1971theory" class="csl-entry" role="listitem">
Rawls, John. 1971. <em>A Theory of Justice</em>. Harvard University Press.
</div>
<div id="ref-rogers1999adaptive" class="csl-entry" role="listitem">
Rogers, Seth, Claude-Nicolas Fiechter, and Pat Langley. 1999. <span>“An Adaptive Interactive Agent for Route Advice.”</span> In <em>Proceedings of the Third Annual Conference on Autonomous Agents</em>, 198–205.
</div>
<div id="ref-russell2019human" class="csl-entry" role="listitem">
Russell, Stuart. 2019a. <em>Human Compatible: Artificial Intelligence and the Problem of Control</em>. Viking.
</div>
<div id="ref-russell_human_2019" class="csl-entry" role="listitem">
———. 2019b. <em>Human Compatible: Artificial Intelligence and the Problem of Control</em>. Viking.
</div>
<div id="ref-sadigh2017active" class="csl-entry" role="listitem">
Sadigh, Dorsa, Anca Dragan, Shankar Sastry, and Sanjit Seshia. 2017. <span>“Active Preference-Based Learning of Reward Functions.”</span>
</div>
<div id="ref-schwartz1992universals" class="csl-entry" role="listitem">
Schwartz, Shalom H. 1992. <span>“Universals in the Content and Structure of Values: Theoretical Advances and Empirical Tests in 20 Countries.”</span> <em>Advances in Experimental Social Psychology</em> 25: 1–65.
</div>
<div id="ref-shah2022goal" class="csl-entry" role="listitem">
Shah, Rohin, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, and Zac Kenton. 2022. <span>“Goal Misgeneralization: Why Correct Specifications Aren’t Enough for Correct Goals.”</span> <a href="https://arxiv.org/abs/2210.01790">https://arxiv.org/abs/2210.01790</a>.
</div>
<div id="ref-stiennon_learning_2020" class="csl-entry" role="listitem">
Stiennon, Nisan, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. <span>“Learning to Summarize from Human Feedback.”</span>
</div>
<div id="ref-talat_machine_2022" class="csl-entry" role="listitem">
Talat, Zeerak, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. 2022. <span>“On the Machine Learning of Ethical Judgments from Natural Language.”</span> In <em>Proceedings of the 2022 <span>Conference</span> of the <span>North</span> <span>American</span> <span>Chapter</span> of the <span>Association</span> for <span>Computational</span> <span>Linguistics</span>: <span>Human</span> <span>Language</span> <span>Technologies</span></em>. Association for Computational Linguistics.
</div>
<div id="ref-belmont" class="csl-entry" role="listitem">
The National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research. 1979. <span>“The Belmont Report: Ethical Principles and Guidelines for the Protection of Human Subjects of Research.”</span> <a href="https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/index.html" class="uri">https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/index.html</a>.
</div>
<div id="ref-tomasello_becoming_2019" class="csl-entry" role="listitem">
Tomasello, Michael. 2019. <em>Becoming Human: <span>A</span> Theory of Ontogeny</em>. Cambridge, MA: Belknap Press.
</div>
<div id="ref-vamplew_human-aligned_2018" class="csl-entry" role="listitem">
Vamplew, Peter, Richard Dazeley, Cameron Foale, Sally Firmin, and Jane Mummery. 2018. <span>“Human-Aligned Artificial Intelligence Is a Multiobjective Problem.”</span> <em>Ethics and Information Technology</em> 20 (1): 27–40. <a href="https://doi.org/10.1007/s10676-017-9440-6">https://doi.org/10.1007/s10676-017-9440-6</a>.
</div>
<div id="ref-vamplew_scalar_2022" class="csl-entry" role="listitem">
Vamplew, Peter, Benjamin J. Smith, Johan Källström, Gabriel Ramos, Roxana Rădulescu, Diederik M. Roijers, Conor F. Hayes, et al. 2022. <span>“Scalar Reward Is Not Enough: A Response to <span>Silver</span>, <span>Singh</span>, <span>Precup</span> and <span>Sutton</span> (2021).”</span> <em>Autonomous Agents and Multi-Agent Systems</em> 36 (2): 41. <a href="https://doi.org/10.1007/s10458-022-09575-5">https://doi.org/10.1007/s10458-022-09575-5</a>.
</div>
<div id="ref-weidinger_artificial_2022" class="csl-entry" role="listitem">
Weidinger, Laura, Madeline G. Reinecke, and Julia Haas. 2022. <span>“Artificial Moral Cognition: <span>Learning</span> from Developmental Psychology.”</span> Preprint. PsyArXiv. <a href="https://doi.org/10.31234/osf.io/tnf4e">https://doi.org/10.31234/osf.io/tnf4e</a>.
</div>
<div id="ref-enwiki:1185176830" class="csl-entry" role="listitem">
Wikipedia contributors. 2023. <span>“AI Alignment — <span>Wikipedia</span><span>,</span> the Free Encyclopedia.”</span> <a href="https://en.wikipedia.org/w/index.php?title=AI_alignment&amp;oldid=1185176830">https://en.wikipedia.org/w/index.php?title=AI_alignment&amp;oldid=1185176830</a>.
</div>
<div id="ref-xiong_achieving_2016" class="csl-entry" role="listitem">
Xiong, Wayne, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig. 2016. <span>“Achieving Human Parity in Conversational Speech Recognition.”</span> <em>arXiv Preprint arXiv:1610.05256</em>.
</div>
<div id="ref-ziebart_modeling_2010" class="csl-entry" role="listitem">
Ziebart, Brian D. 2010. <span>“Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy.”</span> PhD Thesis, Pittsburgh, PA: Carnegie Mellon University.
</div>
<div id="ref-mv" class="csl-entry" role="listitem">
Ziems, Caleb, William Held, Jingfeng Yang, Jwala Dhamala, Rahul Gupta, and Diyi Yang. 2023. <span>“Multi-VALUE: A Framework for Cross-Dialectal English NLP.”</span> <a href="https://arxiv.org/abs/2212.08011">https://arxiv.org/abs/2212.08011</a>.
</div>
</div>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./004-optim.html" class="pagination-link" aria-label="Model-Free Preference Optimization">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/005-align.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>