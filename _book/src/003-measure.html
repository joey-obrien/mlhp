<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Model-Based Preference Optimization – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/004-optim.html" rel="next">
<link href="../src/002-reward_model.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/003-measure.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/002-reward_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/003-measure.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/004-optim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/005-align.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/006-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-to-active-preference-learning" id="toc-introduction-to-active-preference-learning" class="nav-link active" data-scroll-target="#introduction-to-active-preference-learning"><span class="header-section-number">3.1</span> Introduction to Active Preference Learning</a></li>
  <li><a href="#acquisition-function" id="toc-acquisition-function" class="nav-link" data-scroll-target="#acquisition-function"><span class="header-section-number">3.2</span> Acquisition Function</a>
  <ul class="collapse">
  <li><a href="#active-learning-by-variance-reduction" id="toc-active-learning-by-variance-reduction" class="nav-link" data-scroll-target="#active-learning-by-variance-reduction"><span class="header-section-number">3.2.1</span> Active Learning by Variance Reduction</a></li>
  <li><a href="#active-learning-in-ranking-and-comparison" id="toc-active-learning-in-ranking-and-comparison" class="nav-link" data-scroll-target="#active-learning-in-ranking-and-comparison"><span class="header-section-number">3.2.2</span> Active Learning in Ranking and Comparison</a></li>
  <li><a href="#active-preference-based-learning-of-reward-functions" id="toc-active-preference-based-learning-of-reward-functions" class="nav-link" data-scroll-target="#active-preference-based-learning-of-reward-functions"><span class="header-section-number">3.2.3</span> Active Preference-Based Learning of Reward Functions</a></li>
  </ul></li>
  <li><a href="#foundation-models-for-robotics" id="toc-foundation-models-for-robotics" class="nav-link" data-scroll-target="#foundation-models-for-robotics"><span class="header-section-number">3.3</span> Foundation Models for Robotics</a>
  <ul class="collapse">
  <li><a href="#r3m-universal-visual-representation-for-robotics" id="toc-r3m-universal-visual-representation-for-robotics" class="nav-link" data-scroll-target="#r3m-universal-visual-representation-for-robotics"><span class="header-section-number">3.3.1</span> R3M: Universal Visual Representation for Robotics</a></li>
  <li><a href="#voltron-language-driven-representation-learning-for-robotics" id="toc-voltron-language-driven-representation-learning-for-robotics" class="nav-link" data-scroll-target="#voltron-language-driven-representation-learning-for-robotics"><span class="header-section-number">3.3.2</span> Voltron: Language Driven Representation Learning for Robotics</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">3.4</span> Conclusion</a></li>
  <li><a href="#introduction-to-performance-metric-elicitation" id="toc-introduction-to-performance-metric-elicitation" class="nav-link" data-scroll-target="#introduction-to-performance-metric-elicitation"><span class="header-section-number">3.5</span> Introduction to Performance Metric Elicitation</a></li>
  <li><a href="#bayes-optimal-and-inverse-optimal-classifiers" id="toc-bayes-optimal-and-inverse-optimal-classifiers" class="nav-link" data-scroll-target="#bayes-optimal-and-inverse-optimal-classifiers"><span class="header-section-number">3.6</span> Bayes Optimal and Inverse-Optimal Classifiers</a></li>
  <li><a href="#sec:metric-elicitation" id="toc-sec:metric-elicitation" class="nav-link" data-scroll-target="#sec\:metric-elicitation"><span class="header-section-number">3.7</span> Metric Elicitation Problem Setup</a></li>
  <li><a href="#subsec:confusion-matrices" id="toc-subsec:confusion-matrices" class="nav-link" data-scroll-target="#subsec\:confusion-matrices"><span class="header-section-number">3.8</span> Confusion Matrices</a></li>
  <li><a href="#sec:orga500da0" id="toc-sec:orga500da0" class="nav-link" data-scroll-target="#sec\:orga500da0"><span class="header-section-number">3.9</span> LPM and LFPM Metric Elicitation Algorithms</a>
  <ul class="collapse">
  <li><a href="#sec:orgb6dac4e" id="toc-sec:orgb6dac4e" class="nav-link" data-scroll-target="#sec\:orgb6dac4e"><span class="header-section-number">3.9.1</span> LPM Elicitation</a></li>
  <li><a href="#sec:orga500da1" id="toc-sec:orga500da1" class="nav-link" data-scroll-target="#sec\:orga500da1"><span class="header-section-number">3.9.2</span> LFPM Elicitation</a></li>
  </ul></li>
  <li><a href="#sec:orga500da2" id="toc-sec:orga500da2" class="nav-link" data-scroll-target="#sec\:orga500da2"><span class="header-section-number">3.10</span> Guarantees</a></li>
  <li><a href="#summary-and-further-expansions" id="toc-summary-and-further-expansions" class="nav-link" data-scroll-target="#summary-and-further-expansions"><span class="header-section-number">3.11</span> Summary and Further Expansions</a></li>
  <li><a href="#multiclass-performance-metric-elicitation" id="toc-multiclass-performance-metric-elicitation" class="nav-link" data-scroll-target="#multiclass-performance-metric-elicitation"><span class="header-section-number">3.12</span> Multiclass Performance Metric Elicitation</a></li>
  <li><a href="#preliminaries" id="toc-preliminaries" class="nav-link" data-scroll-target="#preliminaries"><span class="header-section-number">3.13</span> Preliminaries</a></li>
  <li><a href="#diagonal-linear-performance-metric-elicitation" id="toc-diagonal-linear-performance-metric-elicitation" class="nav-link" data-scroll-target="#diagonal-linear-performance-metric-elicitation"><span class="header-section-number">3.14</span> Diagonal Linear Performance Metric Elicitation</a>
  <ul class="collapse">
  <li><a href="#dlpm" id="toc-dlpm" class="nav-link" data-scroll-target="#dlpm"><span class="header-section-number">3.14.1</span> DLPM</a></li>
  <li><a href="#bayes-optimal-classifiers" id="toc-bayes-optimal-classifiers" class="nav-link" data-scroll-target="#bayes-optimal-classifiers"><span class="header-section-number">3.14.2</span> Bayes Optimal Classifiers</a></li>
  <li><a href="#geometry-of-space-of-diagonal-confusions-d" id="toc-geometry-of-space-of-diagonal-confusions-d" class="nav-link" data-scroll-target="#geometry-of-space-of-diagonal-confusions-d"><span class="header-section-number">3.14.3</span> Geometry of Space of Diagonal Confusions D</a></li>
  <li><a href="#dlpm-elicitation" id="toc-dlpm-elicitation" class="nav-link" data-scroll-target="#dlpm-elicitation"><span class="header-section-number">3.14.4</span> DLPM Elicitation</a></li>
  <li><a href="#dlpm-elicitation-guarantees" id="toc-dlpm-elicitation-guarantees" class="nav-link" data-scroll-target="#dlpm-elicitation-guarantees"><span class="header-section-number">3.14.5</span> DLPM Elicitation Guarantees</a></li>
  </ul></li>
  <li><a href="#linear-performance-metric-elicitation" id="toc-linear-performance-metric-elicitation" class="nav-link" data-scroll-target="#linear-performance-metric-elicitation"><span class="header-section-number">3.15</span> Linear Performance Metric Elicitation</a>
  <ul class="collapse">
  <li><a href="#geometry-of-space-of-off-diagonal-confusions-c" id="toc-geometry-of-space-of-off-diagonal-confusions-c" class="nav-link" data-scroll-target="#geometry-of-space-of-off-diagonal-confusions-c"><span class="header-section-number">3.15.1</span> Geometry of Space of Off-Diagonal Confusions C</a></li>
  <li><a href="#lpm-elicitation" id="toc-lpm-elicitation" class="nav-link" data-scroll-target="#lpm-elicitation"><span class="header-section-number">3.15.2</span> LPM Elicitation</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">3.16</span> Summary</a></li>
  <li><a href="#linear-reward-estimation" id="toc-linear-reward-estimation" class="nav-link" data-scroll-target="#linear-reward-estimation"><span class="header-section-number">3.17</span> Linear Reward Estimation</a>
  <ul class="collapse">
  <li><a href="#geometry-of-pairwise-comparisons" id="toc-geometry-of-pairwise-comparisons" class="nav-link" data-scroll-target="#geometry-of-pairwise-comparisons"><span class="header-section-number">3.17.1</span> Geometry of Pairwise Comparisons</a></li>
  <li><a href="#driving-simulator-example" id="toc-driving-simulator-example" class="nav-link" data-scroll-target="#driving-simulator-example"><span class="header-section-number">3.17.2</span> Driving Simulator Example</a></li>
  <li><a href="#active-learning-for-pairwise-comparisons" id="toc-active-learning-for-pairwise-comparisons" class="nav-link" data-scroll-target="#active-learning-for-pairwise-comparisons"><span class="header-section-number">3.17.3</span> Active Learning for Pairwise Comparisons</a></li>
  <li><a href="#multi-modal-reward-functions-for-pairwise-comparisons" id="toc-multi-modal-reward-functions-for-pairwise-comparisons" class="nav-link" data-scroll-target="#multi-modal-reward-functions-for-pairwise-comparisons"><span class="header-section-number">3.17.4</span> Multi-Modal Reward Functions for Pairwise Comparisons</a></li>
  </ul></li>
  <li><a href="#guiding-human-demonstrations-in-robotics" id="toc-guiding-human-demonstrations-in-robotics" class="nav-link" data-scroll-target="#guiding-human-demonstrations-in-robotics"><span class="header-section-number">3.18</span> Guiding Human Demonstrations in Robotics</a>
  <ul class="collapse">
  <li><a href="#estimating-compatiblity" id="toc-estimating-compatiblity" class="nav-link" data-scroll-target="#estimating-compatiblity"><span class="header-section-number">3.18.1</span> Estimating Compatiblity</a></li>
  <li><a href="#case-studies-with-fixed-sets" id="toc-case-studies-with-fixed-sets" class="nav-link" data-scroll-target="#case-studies-with-fixed-sets"><span class="header-section-number">3.18.2</span> Case Studies with Fixed Sets</a></li>
  <li><a href="#actively-eliciting-compatible-demonstrations" id="toc-actively-eliciting-compatible-demonstrations" class="nav-link" data-scroll-target="#actively-eliciting-compatible-demonstrations"><span class="header-section-number">3.18.3</span> Actively Eliciting Compatible Demonstrations</a></li>
  <li><a href="#limitations-and-future-work-for-active-elicitation" id="toc-limitations-and-future-work-for-active-elicitation" class="nav-link" data-scroll-target="#limitations-and-future-work-for-active-elicitation"><span class="header-section-number">3.18.4</span> Limitations and Future Work for Active Elicitation</a></li>
  </ul></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1"><span class="header-section-number">3.19</span> Conclusion</a></li>
  <li><a href="#truthful-preference-elicitation-with-adversary" id="toc-truthful-preference-elicitation-with-adversary" class="nav-link" data-scroll-target="#truthful-preference-elicitation-with-adversary"><span class="header-section-number">3.20</span> Truthful Preference Elicitation with Adversary</a>
  <ul class="collapse">
  <li><a href="#auction-theory" id="toc-auction-theory" class="nav-link" data-scroll-target="#auction-theory"><span class="header-section-number">3.20.1</span> Auction Theory</a></li>
  <li><a href="#mutual-information-paradigm" id="toc-mutual-information-paradigm" class="nav-link" data-scroll-target="#mutual-information-paradigm">Mutual Information Paradigm</a></li>
  <li><a href="#auction-theory-2" id="toc-auction-theory-2" class="nav-link" data-scroll-target="#auction-theory-2"><span class="header-section-number">3.20.2</span> Auction Theory 2</a></li>
  </ul></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/003-measure.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<iframe src="https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/" style="width:45%; height:225px;">
</iframe>
<p><a href="https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/" class="btn btn-outline-primary" role="button">Fullscreen Slide</a></p>
<p>Active learning implements the idea of improving one’s current model by strategically querying new data and training on it. The new data starts unlabelled and could be costly to acquire or annotate. The general strategy for query selection is to predict what would happen if a prospective point were collected, annotated, and added to the training data. Within this decision process (Figure <a href="#fig:schema" data-reference-type="ref" data-reference="fig:schema">1.1</a>), the model is never actually trained on those prospective points. The estimate of the added value of a point can be correct, but it can also diverge from reality. If the strategy used is effective, then the model not only improves, but it could also do so using much less data than if it was trained on a dataset built manually, without exploiting model feedback.</p>
<p>Active learning can enhance real-world systems that employ machine learning models. For example, active learning may be used to improve the computer vision models used in autonomous vehicles <span class="citation" data-cites="AL_app_autonomous">(<a href="#ref-AL_app_autonomous" role="doc-biblioref">Jarl et al. 2021</a>)</span>, where driving scenes can take infinitely many forms and gathering an exhaustive data set is impossible. Instead, probing a model and understanding what type of data it would benefit from sounds more practical. In robotics, autonomous agents may query humans when unsure how to act or face new situations <span class="citation" data-cites="AL_app_robotics">(<a href="#ref-AL_app_robotics" role="doc-biblioref">Taylor, Berrueta, and Murphey 2021</a>)</span>. In this space, there is often a significant financial and time cost incurred for collecting data: the robot must act in real-time in the real world, and while parallelization is possible, being strategic about which examples should be collected to best benefit the model helps. In meteorology, active learning can help decide where to place additional sensors for weather predictions <span class="citation" data-cites="AL_app_sensors">(<a href="#ref-AL_app_sensors" role="doc-biblioref">Singh, Nowak, and Ramanathan 2006</a>)</span>. Sensor placement involves deploying teams of people to remote locations and expensive construction for an extra data point. Choosing these locations and where to allocate resources wisely interests governments and businesses. Active learning could also be employed to select data for fine-tuning LLMs for specific downstream tasks <span class="citation" data-cites="AL_app_LLMs">(<a href="#ref-AL_app_LLMs" role="doc-biblioref">Margatina et al. 2023</a>)</span>. In this space, it might be difficult to fully describe an NLP task one might want an LLM to solve. Often, instead of defining a task via a dataset of examples, it may be easier for the human to interact with the LLM for a specific use case, find holes in the model, and cover those using active learning.</p>
<div id="fig:schema" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/active_learning_schema.png" class="img-fluid figure-img" style="width:30.0%"></p>
<figcaption>General active learning system diagram: Current model trained on current data set <span class="math inline">\(\mathcal{D}\)</span>, potential points <span class="math inline">\(\tilde{x}_1...\tilde{x}_m\)</span> are being investigated. One of them will be added to the data set. Relative to the model, a proxy highlights the relative value of each point to model improvement (<span class="math inline">\(v(\tilde{x}_1), \ldots, v(\tilde{x}_m)\)</span>), the point with maximum value is selected and added to <span class="math inline">\(\mathcal{D}\)</span>. The cycle repeats until we collect enough data.</figcaption>
</figure>
</div>
<section id="introduction-to-active-preference-learning" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introduction-to-active-preference-learning"><span class="header-section-number">3.1</span> Introduction to Active Preference Learning</h2>
<p>Imagine you are training a robot to assist people in feeding; how can you possibly allow robots to properly learn the required actions, how far to go, how to detect a mouth, and most importantly, human preferences? In most cases and research, robots learn from human demonstrations, watching many ways a person would do the task. However, such an approach is impractical as expert demonstrations are limited, and properly training a supervised learning model on such data will require huge datasets of many demonstrations. Another disadvantage is that such demonstrations depend on an individual human, so they are variable and difficult to collect. Alternatives to suboptimal demonstrations include approaches using pairwise comparisons, where two trajectories of an action are chosen, and the human evaluates which one is better, and physical corrections, where reward functions are learned from human-robot interactions with the help of a human along the action performed.</p>
<p>In this chapter, we will cover the theory behind pairwise comparisons, extensions of the approach that overcome some of the challenges found in this space, and practical examples where such an approach will be helpful. Further, we will talk about using LLMs and their role in assisting robots through corrections, as well as the applications of such an approach.</p>
</section>
<section id="acquisition-function" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="acquisition-function"><span class="header-section-number">3.2</span> Acquisition Function</h2>
<p>The proxy used to estimate the value of a point in the active learning decision loop can be of various flavors. We summarize some of the known active learning methods and the proxies they rely on:</p>
<ul>
<li><p>Expected model change <span class="citation" data-cites="AL_expmodelchange">(<a href="#ref-AL_expmodelchange" role="doc-biblioref">Cai, Zhang, and Zhou 2013</a>)</span>: This approach focuses on labeling points that would have the most impact on changing the current model parameters.</p></li>
<li><p>Expected error reduction <span class="citation" data-cites="AL_experrorredn">(<a href="#ref-AL_experrorredn" role="doc-biblioref">Mussmann et al. 2022</a>)</span>: Points that would most effectively reduce the model’s generalization error are labeled using this strategy.</p></li>
<li><p>Variance reduction <span class="citation" data-cites="AL_variance">(<a href="#ref-AL_variance" role="doc-biblioref">Cohn, Ghahramani, and Jordan 1996</a>)</span>: This approach labels points that would minimize output variance, which is one component of error. By selecting points that reduce variability in the model’s predictions, it aims to improve its overall performance.</p></li>
<li><p>Uncertainty sampling <span class="citation" data-cites="AL_uncertainty">(<a href="#ref-AL_uncertainty" role="doc-biblioref">Zhu et al. 2010</a>)</span>: Points for which the model is least certain about the correct output are labeled using this strategy.</p>
<ul>
<li><p>Entropy Sampling: This technique computes the entropy of each sample, and the one with the highest entropy is considered the least certain.</p></li>
<li><p>Margin Sampling: The sample with the smallest difference between the two highest class probabilities is deemed the most uncertain.</p></li>
<li><p>Least Confident Sampling: The sample with the smallest best probability is considered the most uncertain.</p></li>
</ul></li>
<li><p>User Centered Labeling Strategies <span class="citation" data-cites="AL_usercentered">(<a href="#ref-AL_usercentered" role="doc-biblioref">Bernard et al. 2018</a>)</span>: This approach involves actively involving the user in the labeling process by visualizing data through dimensionality reduction techniques. The user then provides labels for the compiled data based on their domain expertise and preferences. This strategy leverages user input to improve the quality and relevance of the labeled data.</p></li>
<li><p>Balance exploration and exploitation <span class="citation" data-cites="AL_exploreexploit">(<a href="#ref-AL_exploreexploit" role="doc-biblioref">Bouneffouf et al. 2014</a>)</span>: This strategy involves finding a balance between exploring new data and exploiting existing knowledge. For example, the Active Thompson Sampling (ATS) algorithm assigns a sampling distribution to the data pool, samples a point from this distribution, and queries the oracle for its label.</p></li>
<li><p>Query by committee <span class="citation" data-cites="AL_committee">(<a href="#ref-AL_committee" role="doc-biblioref">Beluch et al. 2018</a>)</span>: This active learning strategy involves training multiple models on labeled data and using them to vote on the label for unlabeled data. Points for which the committee of models disagrees the most are selected for labeling.</p></li>
<li><p>Querying from diverse subspaces or partitions <span class="citation" data-cites="AL_partition">(<a href="#ref-AL_partition" role="doc-biblioref">Ma et al. 2022</a>)</span>: When using a forest of trees as the underlying model, the leaf nodes can represent overlapping partitions of the feature space. This strategy selects instances from non-overlapping or minimally overlapping partitions for labeling.</p></li>
<li><p>Conformal prediction <span class="citation" data-cites="AL_conformal">(<a href="#ref-AL_conformal" role="doc-biblioref">Makili, Sánchez, and Dormido-Canto 2012</a>)</span>: This method predicts that a new data point will have a label similar to old data points in some specified way. The degree of similarity within the old examples is used to estimate the confidence in the prediction.</p></li>
<li><p>Mismatch-first farthest-traversal <span class="citation" data-cites="AL_mismatch">(<a href="#ref-AL_mismatch" role="doc-biblioref">Zhao, Heittola, and Virtanen 2020</a>)</span>: This strategy first prioritizes data points that are wrongly predicted by the current model compared to the nearest-neighbor prediction. The second criterion is the distance to previously selected data, with preference given to those that are farthest away. The goal is to optimize both the correction of mispredictions and the diversity of the selected data.</p></li>
</ul>
<section id="active-learning-by-variance-reduction" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="active-learning-by-variance-reduction"><span class="header-section-number">3.2.1</span> Active Learning by Variance Reduction</h3>
<p><span class="citation" data-cites="AL_variance">(<a href="#ref-AL_variance" role="doc-biblioref">Cohn, Ghahramani, and Jordan 1996</a>)</span> The first paper explored employs active learning to improve models using the proxy of <em>expected variance of the learner</em> for measuring the value of a prospective point if added to the training set. The idea is that at every iteration in the active learning process, in this work, they ask: “Which new point <span class="math inline">\(\tilde{x} \sim P(X=x)\)</span>, for which the model currently believes <span class="math inline">\(P(\tilde{Y}|\tilde{X}=\tilde{x})\)</span>, if annotated as <span class="math inline">\(y(\tilde{x})\)</span> and added to the training data <span class="math inline">\(\mathcal{D}\)</span>, would potentially lower the <em>expected variance of the learner</em> across <span class="math inline">\(P(X=x)\)</span>?" We describe the expected learner variance proxy for model improvement in section <a href="#sec:ELV" data-reference-type="ref" data-reference="sec:ELV">[sec:ELV]</a>.</p>
<p>In other words, what the authors claim is that, without the true label <span class="math inline">\(y(\tilde{x})\)</span> and <em>without retraining</em>, they visualize a <em>hypothetical</em> future where the model is actually trained on <span class="math inline">\((\tilde{x}, f(\tilde{x}))\)</span>, using <span class="math inline">\(P(\tilde{Y}|\tilde{X}=\tilde{x})\)</span> as a proxy for <span class="math inline">\(y(\tilde{x})\)</span>. In the following sections we: define the expected learner variance (ELV) (<a href="#sec:ELV" data-reference-type="ref" data-reference="sec:ELV">[sec:ELV]</a>), derive an active learning algorithm which employs ELV for model improvement (<a href="#sec:algo" data-reference-type="ref" data-reference="sec:algo">[sec:algo]</a>) and discuss emiprical results when this algorithm is used on two models, namely: a Gaussian mixture model and a locally-weighted regression model (fig <a href="#fig:two_models" data-reference-type="ref" data-reference="fig:two_models">1.2</a>), for which the ELV proxy can be computed accurately and efficiently.</p>
<p>The authors derive a notion of expected learner variance starting from machine learning first principles, in a model and data-agnostic fashion to glean a general understanding of the proxy the authors plug into the general active learning process described in figure <a href="#fig:schema" data-reference-type="ref" data-reference="fig:schema">1.1</a>.</p>
<p>If we are probing around a prospective point <span class="math inline">\(x\)</span> that we potentially could add to our dataset, we can consider the expected error at x (across choosable datasets <span class="math inline">\(\mathcal{D}\)</span> and annotations strategies <span class="math inline">\(P(Y|X=x)\)</span>), where <span class="math inline">\(y(x)\)</span> is the true label of the point which we do not posess: <span class="math display">\[\mathbb{E} _{P(Y|X=x), P(\mathcal{D})}[\ (\hat{y}(x; \mathcal{D}) - y(x))^2\ ]\]</span> Note that the expectation wraps around the stochasticity in annotations and in the dataset <span class="math inline">\(\mathcal{D}\)</span> picked so far, used to train the learner <span class="math inline">\(\hat{y}\)</span>. Since <span class="math inline">\(\mathcal{D}\)</span> is being picked, <span class="math inline">\(P(\mathcal{D})\)</span> may be very different from the joint distribution of data points and their labels: <span class="math inline">\(P(X=x,Y=y)\)</span>.</p>
<p>Expanding the expectation to expose the bias-variance tradeoff (see <span class="citation" data-cites="bias_variance_orig_paper">(<a href="#ref-bias_variance_orig_paper" role="doc-biblioref">Geman, Bienenstock, and Doursat 1992</a>)</span>) yields the following expression: <span class="math display">\[\mathbb{E}_{y|x}[(y(x) - \mathbb{E}[y|x])^2] \\ + (\mathbb{E}_{\mathcal{D}}[\hat{y}(x; \mathcal{D})] - \mathbb{E}[y|x])^2 \\+ \mathbb{E}_{\mathcal{D}}[(\hat{y}(x; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[\hat{y}(x; \mathcal{D})])^2].\]</span></p>
<p>The first, second and third additive terms characterize: the noise in the true annotation of <span class="math inline">\(x\)</span> which is independent of the learner, the learner squared bias at <span class="math inline">\(x\)</span> and the learner variance at <span class="math inline">\(x\)</span>, respectively.</p>
<p>Authors isolate the learner variance at <span class="math inline">\(x\)</span> and denote it by <span class="math inline">\(\sigma_{\hat{y}}^2(x, \mathcal{D})\)</span>, namely: <span class="math display">\[\sigma_{\hat{y}}^2(x, \mathcal{D}) = \mathbb{E}_{\mathcal{D}}[(\hat{y}(x; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[\hat{y}(x; \mathcal{D})])^2]\]</span></p>
<p>The authors let <span class="math inline">\(\bar{\mathcal{D}} = \mathcal{D} \cup (\tilde{x}, \tilde{y})\)</span> represent the prospective <em>future</em> dataset where current dataset <span class="math inline">\(\mathcal{D}\)</span> is merged with prospective point <span class="math inline">\(\tilde{x}\)</span> and the label <span class="math inline">\(\tilde{y}\)</span> which is our best estimate of its annotation. The authors estimate learner variance at <span class="math inline">\(x\)</span>, if the learner was trained on <span class="math inline">\(\bar{\mathcal{D}}\)</span> as: <span class="math display">\[\tilde{\sigma}_{\hat{y}}^2(x, \mathcal{\bar{D}})= \mathbb{E}_{\bar{\mathcal{D}}}[(\hat{y}(x; \bar{\mathcal{D}}) - \mathbb{E}_{\bar{\mathcal{D}}}[\hat{y}(x; \bar{\mathcal{D}})])^2]\]</span></p>
<p>The <em>expected</em> learner variance, the proxy the authors are after, where the expectation is taken over our estimated belief <span class="math inline">\(P(\tilde{Y}|\tilde{X}=\tilde{x})\)</span> of the label <span class="math inline">\(\tilde{y}\)</span> is the following: <span class="math display">\[\label{eqn:ELV_at_x}
  \langle\tilde{\sigma}_{\hat{y}}^2(x, \mathcal{\bar{D}})\rangle= \mathbb{E}_{P(\tilde{Y} | \tilde{X}=\tilde{x})}[\tilde{\sigma}_{\hat{y}}^2]. \tag{1}\]</span></p>
<p>We notice here that the key to being able to compute the ELV given a model, hinges on the ability to compute estimated belief <span class="math inline">\(P(\tilde{Y}|\tilde{X}=\tilde{x})\)</span> for the annotation <span class="math inline">\(\tilde{y}\)</span> of any prospective data point <span class="math inline">\(\tilde{x}\)</span>.</p>
<p>Finally, when deciding whether to include data point <span class="math inline">\((\tilde{x}, \tilde{y})\)</span> into the training dataset, we are interested in the expected learner variance across the <em>entire input distribution</em>, which we characterize as the following integral over the input distribution: <span class="math display">\[\int_x P(X=x) \langle\tilde{\sigma}_{\hat{y}}^2(x, \mathcal{\bar{D}})\rangle dx.\]</span></p>
<p>We note to the reader that <span class="math inline">\(P(X=x)\)</span> is a distribution with potentially-infinite support and the authors do not compute this integral exactly. Instead, the computational estimate of that integral consists of sampling several points <span class="math inline">\(x \sim P(X=x)\)</span> and averaging the quantity inside the integral over these points until convergence using Monte-Carlo sampling approaches (see <span class="citation" data-cites="monte-carlo">(<a href="#ref-monte-carlo" role="doc-biblioref">Ghojogh et al. 2020</a>)</span>). We refer the reader to the algorithm in <a href="#sec:algo" data-reference-type="ref" data-reference="sec:algo">[sec:algo]</a> for an exact instantiation of this idea.</p>
<p>We note that the paper derives closed-form solutions to the expected learner variance (equation <a href="#eqn:ELV_at_x" data-reference-type="eqref" data-reference="eqn:ELV_at_x">[eqn:ELV_at_x]</a>) for two models (fig <a href="#fig:two_models" data-reference-type="ref" data-reference="fig:two_models">1.2</a>): (1) a mixture of Gaussian model and (2) a locally-weighted regression model (fig <a href="#fig:two_models" data-reference-type="ref" data-reference="fig:two_models">1.2</a>). We refer the reader to the paper for the mathematical derivation of those quantities. We cite the empirical results seen when training these two models in section <a href="#sec:empirical" data-reference-type="ref" data-reference="sec:empirical">[sec:empirical]</a>, where authors use the algorithm which they describe in <a href="#sec:algo" data-reference-type="ref" data-reference="sec:algo">[sec:algo]</a>, which in turn employs the expected learner variance they derived above, as a proxy for model improvement.</p>
<p><span id="sec:two_models" label="sec:two_models"></span></p>
<div id="fig:two_models" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/1_two_models.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Two models explored empirically in <a href="#sec:empirical" data-reference-type="ref" data-reference="sec:empirical">[sec:empirical]</a>. These two models lead to closed-form, accurately and efficiently-computed expected learner variance which can be plugged into the algorithm described in <a href="#sec:algo" data-reference-type="ref" data-reference="sec:algo">[sec:algo]</a>.</figcaption>
</figure>
</div>
<p>With the expected learner proxy defined (<a href="#sec:ELV" data-reference-type="ref" data-reference="sec:ELV">[sec:ELV]</a>) for estimating how much a prospective data point would improve a model, the general active learning loop (<a href="#fig:schema" data-reference-type="ref" data-reference="fig:schema">1.1</a>) for the ELV proxy can be instantiated, specifically, in the form of the following algorithm:<br>
<br>
<span id="sec:algo" label="sec:algo"></span> <span class="math inline">\(\mathcal{D} = \{\}\)</span>, iterate until <span class="math inline">\(\mathcal{D}\)</span> is large enough:</p>
<ol type="1">
<li><p>Sample candidate points <span class="math inline">\(\tilde{x}_1,\ldots,\tilde{x}_m\)</span> from <span class="math inline">\(P(X=x)\)</span></p></li>
<li><p>Compute current belief <span class="math inline">\(P(\tilde{Y}|\tilde{X}=\tilde{x})\)</span> for each point</p></li>
<li><p>Calculate <span class="math inline">\(\langle\tilde{\sigma}_{\hat{y}}^2(x, \mathcal{D})\rangle\)</span> on each point using current beliefs from last step</p></li>
<li><p>Calculate ELV integral via Monte Carlo sampling from <span class="math inline">\(P(X=x)\)</span></p></li>
<li><p>Label point with lowest ELV: <span class="math inline">\(\tilde{x}^* \rightarrow y(\tilde{x}^*)\)</span></p></li>
<li><p><span class="math inline">\(\mathcal{D} = \mathcal{D} \cup \{(\tilde{x}^*, y(\tilde{x}^*))\}\)</span></p></li>
<li><p>Retrain on <span class="math inline">\(\mathcal{D}\)</span></p></li>
</ol>
<p>While there is no general recipe for the number of iterations to perform, one could imagine relying on some empirical measure like a loss on left-out labelled data to gauge model improvement (as seen in figs <a href="#fig:empirical:gauss" data-reference-type="ref" data-reference="fig:empirical:gauss">1.4</a>, <a href="#fig:empirical:regress" data-reference-type="ref" data-reference="fig:empirical:regress">1.5</a>). Intuitively, the size of the data set and its relationship to the loss is intimately tied to the model complexity which impacts its data-thirstiness.</p>
<p>Arm2D (fig <a href="#fig:arm2D" data-reference-type="ref" data-reference="fig:arm2D">1.3</a>) is a kinematics problem where learner has to predict the tip position of a robotic arm given a set of joint angles <span class="math inline">\(\mathbf{\theta_1}, \mathbf{\theta_2}\)</span>. In this analysis, the two models seen in <a href="#sec:two_models" data-reference-type="ref" data-reference="sec:two_models">[sec:two_models]</a>, namely the Gaussian mixture model and locally-weighted regression, were trained using the algorithm described in <a href="#sec:algo" data-reference-type="ref" data-reference="sec:algo">[sec:algo]</a>.</p>
<div id="fig:arm2D" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/1_experiment_setup.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>The arm kinematics problem. The learner attempts to predict tip position given a set of joint angles <span class="math inline">\(\mathbf{\theta_1}, \mathbf{\theta_2}\)</span></figcaption>
</figure>
</div>
<div id="fig:empirical:gauss" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/1_experiment_results_gaussian.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Variance and MSE learning curves for mixture of 60 Gaussians trained on the Arm2D domain. Dotted lines denote standard error for average of 10 runs, each started with one initial random example.</figcaption>
</figure>
</div>
<p>What is interesting in the results seen (figs <a href="#fig:empirical:gauss" data-reference-type="ref" data-reference="fig:empirical:gauss">1.4</a>, <a href="#fig:empirical:regress" data-reference-type="ref" data-reference="fig:empirical:regress">1.5</a>) is that beyond the variance of the learner decreasing, as we would expect, given that authors chose points to reduce the expected variance, we do also see a related decrease in mean square error (MSE) of the models, as the size of the collected data set increases, for <em>both</em> models. This is a cool result because the expected learner variance for these models can be computed, relative to a new point, accurately and efficiently, and we see that when plugged into the general active learning loop <a href="#fig:schema" data-reference-type="ref" data-reference="fig:schema">1.1</a> to yield algorithm <a href="#sec:algo" data-reference-type="ref" data-reference="sec:algo">[sec:algo]</a>, model performance is enhanced significantly.</p>
<p>What is surprising in the case of the locally-weighted regression model (fig <a href="#fig:empirical:regress" data-reference-type="ref" data-reference="fig:empirical:regress">1.5</a>) is that if points were to be chosen randomly, the MSE would be extremely unstable and have sharp increases and decreases. But when the algorithm described in <a href="#sec:algo" data-reference-type="ref" data-reference="sec:algo">[sec:algo]</a> is used, with the expected learner variance as proxy, the MSE decreases almost smoothly bar some preliminary instabilities.</p>
<div id="fig:empirical:regress" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/1_experiment_results_regression.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Variance and MSE learning curves for LOESS model trained on the Arm2D domain. Dotted lines denote standard error for average of 60 runs, each started with a single initial random example.</figcaption>
</figure>
</div>
</section>
<section id="active-learning-in-ranking-and-comparison" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="active-learning-in-ranking-and-comparison"><span class="header-section-number">3.2.2</span> Active Learning in Ranking and Comparison</h3>
<p>Many researchers have shown that making comparisons is an easier and more convenient method for users than assigning a specific score for each of the items demonstrated. Individual comparisons give out a full ranking over a set of <span class="math inline">\(n\)</span> objects <span class="math inline">\(\Theta = (\theta_1, \theta_2, ..., \theta_n\)</span>). That ranking will be defined as a mapping <span class="math inline">\(\sigma : \{1,...,n\} \rightarrow \{1,...,n\}\)</span> that prescribes an order to the set of objects <span class="math inline">\(\Theta\)</span>. More specifically, for a single <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\sigma(\Theta) = \theta_{\sigma(1)} &lt; \theta_{\sigma(2)} &lt; ... &lt; \theta_{\sigma(n-1)} &lt; \theta_{\sigma(n)}\)</span>, where <span class="math inline">\(\theta_{i} &lt; \theta_{j}\)</span> means that <span class="math inline">\(\theta_{i}\)</span> is worse in rating compared to <span class="math inline">\(\theta_{j}\)</span>.</p>
<p>For any <span class="math inline">\(n\)</span> elements that there are to rank, there are <span class="math inline">\(n!\)</span> possible orderings over these elements that can result in the correct complete ranking. Using the fact that a lower bound on sorting is <span class="math inline">\(n\log n\)</span>, to get a guaranteed true rating over <span class="math inline">\(n\)</span> objects one will need <span class="math inline">\(n\log n\)</span> pairwise comparisons if those comparisons are picked at random. That number can be quite high and costly in many applications, especially since most of the ranking information comes from humans, so the more comparisons they have to make, the more money and time is spent. This also might be inefficient since some comparisons can provide more value to the learning process than others, meaning that some comparisons are a waste to the process of deriving the ranking. This can be detrimental in psychology and market research, as those fields utilize comparisons the most, and a faster process might give a lot of benefits.</p>
<p>The reason the lower bound on the number of comparisons is <span class="math inline">\(n\log n\)</span> is that it does not assume any information about the underlying space and field, so the comparisons are chosen at random. However, if one is to take advantage of the structures that are in the comparison space, it might lead to more information about which comparisons will be most valuable to ask for. For example, if abstracting to humans for a moment, <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> discusses how eye doctors have a big range of options to choose from when assigning prescriptions for glasses, however, patients don’t see them doing that many comparisons before they decide which is the best option for a specific client. That is because eye doctors have domain knowledge about the field which they incorporate into the process, and only ask clients questions for comparisons when needed. That gives rise to using similar knowledge in the ranking field, which results in an active learning approach that selects data based on the relevance of a comparison query toward finding the final <span class="math inline">\(\sigma(\Theta)\)</span>.</p>
<section id="geometric-approach-to-comparisons" class="level4" data-number="3.2.2.1">
<h4 data-number="3.2.2.1" class="anchored" data-anchor-id="geometric-approach-to-comparisons"><span class="header-section-number">3.2.2.1</span> Geometric Approach to Comparisons</h4>
<p>In this part, we will go over a paper <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span>, which dives deep into active learning over the data that can be embedded into a multi-dimensional space. In that case, the comparison that is done between two different objects splits the space into halves, where in each half one object is better than the other. By utilizing such spatial information, the paper creates a geometric approach to ranking and active learning, and this spatial information will be the domain knowledge that will help to determine which comparisons to perform to get the ranking.</p>
<p>For this application, the following terms are defined:</p>
<ol type="1">
<li><p><span class="math inline">\(R^d\)</span> is the space in which objects can be embedded</p></li>
<li><p><span class="math inline">\(\theta_1,...,\theta_n\)</span> the objects, and now will represent the locations of those objects in <span class="math inline">\(R^d\)</span></p></li>
<li><p>For each ranking <span class="math inline">\(\sigma\)</span> there is a reference point <span class="math inline">\(r_{\sigma} \in R^d\)</span>, such that if according to ranking <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\theta_{i} &lt; \theta_{j}\)</span> (object <span class="math inline">\(i\)</span> is worse than <span class="math inline">\(j\)</span>), then <span class="math inline">\(||\theta_i - r_{\sigma}|| &lt; ||\theta_j - r_{\sigma}||\)</span>, or otherwise object <span class="math inline">\(i\)</span> is closer to the reference point <span class="math inline">\(r_\theta\)</span> of the ranking than object <span class="math inline">\(j\)</span>.</p></li>
<li><p><span class="math inline">\(\Sigma_{n,d}\)</span> will be the set of all possible rankings of the <span class="math inline">\(n\)</span> objects that satisfy the embedding distances in the space <span class="math inline">\(R^d\)</span> defined in the previous term. Note that not all possible rankings will satisfy the embedding conditions, but on the other side, there might be multiple rankings that can satisfy all of those conditions too.</p></li>
<li><p>For every ranking <span class="math inline">\(\sigma\)</span> there is <span class="math inline">\(M_n(\sigma)\)</span> as the number of pairwise comparisons needed to identify the ranking. When comparisons are done at random <span class="math inline">\(E[M_n(\sigma)] = n\log n\)</span>, so the paper <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> will reason about this quantity to show that it can be smaller if incorporating the space knowledge.</p></li>
<li><p><span class="math inline">\(q_{i,j}\)</span> will be the query of comparison between objects <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p></li>
</ol>
</section>
<section id="embedding-space" class="level4" data-number="3.2.2.2">
<h4 data-number="3.2.2.2" class="anchored" data-anchor-id="embedding-space"><span class="header-section-number">3.2.2.2</span> Embedding Space</h4>
<div id="fig:dim-space" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/SPACE.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>Objects <span class="math inline">\(\theta_1, \theta_2, \theta_3\)</span> and queries in <span class="math inline">\(R^2\)</span>. The <span class="math inline">\(r_\theta\)</span> lies in the shaded region which represents <span class="math inline">\(\Sigma_{n,2}\)</span>(consistent with the labels of <span class="math inline">\(q_{1,2}, q_{1,3}, q_{2,3}\)</span>). The dotted (dashed) lines represent new queries whose labels are (are not) ambiguous.</figcaption>
</figure>
</div>
<p>Next, we will discuss embedding space. To properly understand how to select the most valuable queries, it is essential to discuss the space where the objects exist and how the queries divide that space to find the proper rankings. For this example, in Figure <a href="#fig:dim-space" data-reference-type="ref" data-reference="fig:dim-space">1.6</a>, the paper <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> works in <span class="math inline">\(R^2\)</span> space, with three objects <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>, and <span class="math inline">\(\theta_3\)</span>. There are pairwise queries <span class="math inline">\(q_{1,3}\)</span>, <span class="math inline">\(q_{2,3}\)</span>, <span class="math inline">\(q_{1,2}\)</span> between them that are denoted by the solid lines that are equidistant from the two objects that they are comparing, those lines respectively split the <span class="math inline">\(R^2\)</span> space into halves, where each of the halves is closer to one of the two objects. The paper starts coloring dark grey on the side of the worse object for each of the queries and take the intersection of those halves, which ends up in the dark grey region on the image, which actually indicates the <span class="math inline">\(\Sigma_{n,2}\)</span> since all of those points follow the embedding conditions. In particular, for every point <span class="math inline">\(r\)</span> in the dark grey area <span class="math inline">\(||\theta_3 - r|| &lt; ||\theta_2 - r|| &lt; ||\theta_1 -  r||\)</span>, which in turn means that <span class="math inline">\(\theta_3 &lt; \theta_2 &lt; \theta_1\)</span>. So that means every point <span class="math inline">\(r\)</span> is one of the <span class="math inline">\(r_\sigma\)</span> that represent their respective rankings <span class="math inline">\(\sigma \in \Sigma_{n,2}\)</span>. In other words, the paper is trying to have the reference points and dark grey region to be closest to the worst object, and furthest from the best object.</p>
<p>Authors also denote the label for each of the queries <span class="math inline">\(q_{i,j}\)</span>, such as label <span class="math inline">\(y_{i,j} = 1\{q_{i,j}\}\)</span> (for example <span class="math inline">\(y_{1,2} = 0, y_{3,2} = 1\)</span>). This allows to decide how to label the new queries that are represented by the dashed and the dotted lines. It depends on which objects each of those queries are comparing. Let’s focus on the dotted line and call it <span class="math inline">\(q_{i,4}\)</span>, where <span class="math inline">\(i={1,2,3}\)</span> and consider potential locations of <span class="math inline">\(\theta_4\)</span>. Since the line has to be equidistant from one of the three objects in the picture and <span class="math inline">\(\theta_4\)</span>, that means <span class="math inline">\(\theta_4\)</span> can be placed in 3 different places. If the query performed is <span class="math inline">\(q_{2,4}\)</span> then <span class="math inline">\(\theta_4\)</span> will lay closer to the dark grey area than <span class="math inline">\(\theta_2\)</span>, and thus <span class="math inline">\(y_{2,4} = 0\)</span>. However, if <span class="math inline">\(q_{1,4}\)</span> or <span class="math inline">\(q_{3,4}\)</span> are done, in both cases <span class="math inline">\(\theta_4\)</span> will be further from dark grey area than <span class="math inline">\(\theta_1\)</span> or <span class="math inline">\(\theta_3\)</span>, which means <span class="math inline">\(y_{1,4} = y_{3,4} = 1\)</span>. In this case, the labels are contradictory and depend on which object they are compared with, which means that such a query <span class="math inline">\(q_{i,4}\)</span> can be called ambiguous.</p>
<p>In contrast, authors analyze the dashed line and call it <span class="math inline">\(q_{i,5}\)</span>, where <span class="math inline">\(i={1,2,3}\)</span> and consider potential locations of <span class="math inline">\(\theta_5\)</span>. Since the line has to be equidistant from one of the three objects in the picture and <span class="math inline">\(\theta_5\)</span>, that means it can be placed in 3 different places. If one of the three potential queries is done, <span class="math inline">\(\theta_5\)</span> will be closer to dark grey area than <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>, and <span class="math inline">\(\theta_3\)</span>, which means <span class="math inline">\(y_{1,5} = y_{2,5} = y_{3,5} = 0\)</span>. In this case, all labels are the same no matter which object isu sed, which means that such a query won’t be contradictory, as all of them agree on the label.</p>
<p>The goal is to try to do as many ambiguous queries as possible and skip the non-ambiguous queries to decrease the total <span class="math inline">\(M_n(\sigma)\)</span>. Intuitively, if there is contradictory information about a query then it needs to be done, so the human can clarify which way it actually leans. Whereas, if all of the sources of information from the domain space agree on the query’s label then that information can be used without asking a human, incorporating the knowledge of the embedding distances.</p>
<p>Lastly, to consider the general case of the <span class="math inline">\(R^d\)</span> space, rather than talk about halves of the image, it is essential to discuss half-spaces, similarly considering the half-space that assigns a label of <span class="math inline">\(1\)</span> to the query, and half-space assigning a label of <span class="math inline">\(0\)</span>. If both half-spaces take place to exist, they have conflicting information on the query, which makes the query ambiguous. However, if one of the half-spaces does not exist, it means that the other is actually the full space, which represents consistency in the label assignment, and a non-ambiguous query.</p>
<section id="algorithms-for-ambiguous-query-selection" class="level5" data-number="3.2.2.2.1">
<h5 data-number="3.2.2.2.1" class="anchored" data-anchor-id="algorithms-for-ambiguous-query-selection"><span class="header-section-number">3.2.2.2.1</span> Algorithms for Ambiguous Query Selection</h5>
<div class="algorithm">
<div class="algorithmic">
<p><strong>input:</strong> <span class="math inline">\(n\)</span> objects in <span class="math inline">\(\mathbb{R}^d\)</span> <strong>initialize:</strong> objects <span class="math inline">\(\theta_1, \dots, \theta_n\)</span> in uniformly random order request <span class="math inline">\(q_{i,j}\)</span>’s label from reference impute <span class="math inline">\(q_{i,j}\)</span>’s label from previously labeled queries <strong>output:</strong> ranking of <span class="math inline">\(n\)</span> objects</p>
</div>
</div>
<p>The standard algorithm in <a href="#fig:qsa_alg" data-reference-type="ref" data-reference="fig:qsa_alg">[fig:qsa_alg]</a> requests labels for <span class="math inline">\(q_{i,j}\)</span> if those queries are ambiguous, but otherwise, it infers the information from prior comparisons and their labels.</p>
<p>Now it is important to demonstrate that the number of comparisons decreases. In particular <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> show that this algorithm has <span class="math inline">\(E[M_n(\sigma)] = O(d\log n)\)</span>, where <span class="math inline">\(d\)</span> is the dimension of the space and <span class="math inline">\(d &lt; n\)</span>, which improves on the <span class="math inline">\(O(n\log n)\)</span> baseline. The proof can be studied deeper in the paper itself, but on a high level, it starts reasoning about the probability of the query being ambiguous and a comparison being requested from a human, thus representing <span class="math inline">\(M_n = \Sigma_{k=1}^{n-1}\Sigma_{i=1}^k 1\{Requestq_{i,k+1}\}\)</span>. For that, authors define <span class="math inline">\(Q(i,j)\)</span>, which represents the number of different rankings that exist for <span class="math inline">\(i\)</span> elements in <span class="math inline">\(j\)</span>-dimensional space (e.g. <span class="math inline">\(Q(1,d) = 1, Q(n,0) = 1, Q(n,1) = n!\)</span>). In that case, <span class="math inline">\(|\Sigma_{n,d}| = Q(n,d)\)</span>. Further, using recurrence relations for <span class="math inline">\(Q(i,j)\)</span>, authors derive that <span class="math inline">\(|\Sigma_{n,d}| = Q(n,d) = O(n^{2d})\)</span>, which is omitted here. Analogously authors define <span class="math inline">\(P(i,j)\)</span> that represents the number of rankings in <span class="math inline">\(\Sigma_{n,d}\)</span> that will still be possible with adding a new element <span class="math inline">\(i+1\)</span> to the ranking objects. Going back to Figure <a href="#fig:dim-space" data-reference-type="ref" data-reference="fig:dim-space">1.6</a>, <span class="math inline">\(P(i,j)\)</span> estimates how much of the dark grey area will still exist after making a query for <span class="math inline">\(i+1\)</span>. As was indicated there, the dotted line ambiguous query did not change the dark grey area at all (<span class="math inline">\(P(n,d) = Q(n,d)\)</span>), whereas the dashed non-ambiguous query would cut a piece from it (<span class="math inline">\(P(n,d) &lt; Q(n,d)\)</span>). Thus, <span class="math inline">\(Request q_{i,k+1} = P(k,d) / Q(k,d)\)</span>, so a higher value indicates more possible rankings and an ambiguous query that needs to be requested to get more useful information. With this in mind, authors derive that <span class="math inline">\(E[M_n(\sigma)] = O(d\log n)\)</span>, showing that fewer queries are needed for effective ranking.</p>
<p>The issue with this algorithm is that only one human provides the answers to the requested queries, which means it does not account for their biases. An alternative approach is a Robust Query Selection Algorithm (RQSA) <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span>, which uses majority voting for every query, which will indicate the ground truth of the query’s label. However, a consideration the authors took is that a group of people can still give incorrect or divided responses. In the case that the votes for each answer are almost equal in number, the authors pushed that query to the end of the algorithm, to see if it can become a non-ambiguous query with more information learned. But if it does not, the odd number of voters is used to determine the final ranking. <span id="section:QSA" label="section:QSA"></span></p>
</section>
<section id="performance-analysis" class="level5" data-number="3.2.2.2.2">
<h5 data-number="3.2.2.2.2" class="anchored" data-anchor-id="performance-analysis"><span class="header-section-number">3.2.2.2.2</span> Performance Analysis</h5>
<div id="fig:rand_n" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/Dim:query_graph.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Mean and standard deviation of requested queries (solid) in the noiseless case for <span class="math inline">\(n = 100\)</span>; <span class="math inline">\(\log_2|\Sigma_{n,d}|\)</span> is a lower bound (dashed).</figcaption>
</figure>
</div>
<div id="fig:geo_acc">
<table class="caption-top table">
<caption>Statistics for the Robust Query Selection Algorithm (RQSA) <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> discussed at the end of <a href="#section:QSA" data-reference-type="ref" data-reference="section:QSA">[section:QSA]</a> and the baseline of conducting all comparisons. <span class="math inline">\(y\)</span> serves as a noisy ground truth, <span class="math inline">\(\tilde{y}\)</span> is the result of all comparisons, and <span class="math inline">\(\hat{y}\)</span> is the output of the RQSA.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Dimension</th>
<th></th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">% of queries</td>
<td>mean</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">18.5</td>
</tr>
<tr class="even">
<td style="text-align: left;">requested</td>
<td>std</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Average error</td>
<td><span class="math inline">\(d(\bar{y}, y)\)</span></td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td><span class="math inline">\(d(\bar{y}, y)\)</span></td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.29</td>
</tr>
</tbody>
</table>
</div>
<p>Figure <a href="#fig:rand_n" data-reference-type="ref" data-reference="fig:rand_n">1.7</a> shows that the number of comparisons fits within the expected bounds, as <span class="math inline">\(\log|\Sigma_{n,d}| = \log(n^d) = d\log n\)</span>. To derive that graph, authors <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> sampled 100 random data points in a <span class="math inline">\(R^d\)</span> space, where <span class="math inline">\(d\)</span> took on 10 different values as indicated on the graph. Each dimension’s experiments were repeated 25 times for consistency.</p>
<p>With regard to the accuracy and performance of the method, the authors did a ranking experiment on 100 different audio signals, results of which can be seen in Table <a href="#fig:geo_acc" data-reference-type="ref" data-reference="fig:geo_acc">1.1</a>. The ground truth labels came from humans, indicated by <span class="math inline">\(y\)</span> in the table. That resulted in the existence of noise and potential errors in the ground truth, which could influence the performance of both the baseline algorithm that does all comparisons (<span class="math inline">\(\Tilde{y}\)</span>) and the Robust Query Selection Algorithm (RQSA) proposed in <a href="#section:QSA" data-reference-type="ref" data-reference="section:QSA">[section:QSA]</a> (<span class="math inline">\(\hat{y}\)</span>). As can be seen in both 2 and 3-dimensional spaces RQSA performed worse by <span class="math inline">\(8\%\)</span> compared to the baseline, which indicates that active learning that uses the domain information can still be erroneous due to the inference of certain comparisons that sometimes may not be entirely correct. However, as can be seen by the upper part of Figure <a href="#fig:geo_acc" data-reference-type="ref" data-reference="fig:geo_acc">1.1</a>, significantly less queries were requested compared to the baseline, which means that the approach can have a significant benefit at a cost of slight loss in accuracy. <span id="sec:geo_app" label="sec:geo_app"></span></p>
</section>
</section>
<section id="user-information-as-domain-knowledge-for-active-learning" class="level4" data-number="3.2.2.3">
<h4 data-number="3.2.2.3" class="anchored" data-anchor-id="user-information-as-domain-knowledge-for-active-learning"><span class="header-section-number">3.2.2.3</span> User Information as Domain Knowledge for Active Learning</h4>
<p>An alternative source of domain knowledge could be users themselves, who can indicate their uncertainty when it comes to comparing two objects. Prior studies have shown <span class="citation" data-cites="unnoisy_humans">(<a href="#ref-unnoisy_humans" role="doc-biblioref">Amershi et al. 2014</a>)</span> that when presented with only two options when selecting which object is better, but not being able to properly decide, users would get frustrated and tend to respond more faultyly, creating noise and incorrect responses in the data. Through feedback and other studies <span class="citation" data-cites="noisy_humans">(<a href="#ref-noisy_humans" role="doc-biblioref">Guillory and Bilmes 2011</a>)</span> it was determined that presenting users with an option of indifference between the two objects can remove those problems. Moreover, in connection to active learning, the authors show that such an option helps to select more informative queries since it provides more domain knowledge that can be used, resulting in a decrease in the number of queries required.</p>
<p>For this problem, the following terms are defined:</p>
<ol type="1">
<li><p><span class="math inline">\(c\)</span> - a cost function that represents user preferences, and the result the model has to determine at the end of training. The preferred items will have lower costs, and less preferred ones will have higher costs. The goal is to determine this function with the fewest possible number of queries using active learning.</p></li>
<li><p><span class="math inline">\(H\)</span> - a set of hypotheses over the possible cost functions, where for each <span class="math inline">\(h \in H\)</span> there is a cost function <span class="math inline">\(c_h\)</span> associated with it.</p></li>
<li><p><span class="math inline">\(h^*\)</span> - a true hypothesis that the model needs to determine, which has cost <span class="math inline">\(c_{h^*}\)</span> associated with it</p></li>
<li><p><span class="math inline">\(t(x,y)\)</span> - a test performed to compare items <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (the user is being asked to provide a response to which item is better). Those tests result in changes and adjustments to <span class="math inline">\(H\)</span> as more information is learned.</p></li>
<li><p><span class="math inline">\(o(x,y)\)</span> - observation or result of <span class="math inline">\(t(x,y)\)</span>, where <span class="math inline">\(o(x,y) \in \{x&lt;y, x&gt;y\}\)</span></p></li>
<li><p><span class="math inline">\(S = \{(t_1, o_1), (t_2, o_2),...,(t_m, o_m)\}\)</span> - a sequence of <span class="math inline">\(m\)</span> pairs of tests and observations</p></li>
<li><p><span class="math inline">\(w(H|S)\)</span> - probability mass of all hypotheses that are still consistent with the observations (similar to the dark grey area from Figure <a href="#fig:dim-space" data-reference-type="ref" data-reference="fig:dim-space">1.6</a> and <span class="math inline">\(Q(i,j)\)</span> discussed in <a href="#section:QSA" data-reference-type="ref" data-reference="section:QSA">[section:QSA]</a>). This means that if <span class="math inline">\(h \in H\)</span> is inconsistent with user responses received, it is removed from <span class="math inline">\(H\)</span>.</p></li>
</ol>
<p>With the key terms defined, let’s consider the noiseless base setting where users only have two options for response. Those components will also later be translated to the setting with the third option so the true cost function can be determined there. <span class="math inline">\(w(H|S)\)</span> is the sum of the weights of all hypotheses that are still consistent with the evidence. <span class="math display">\[\begin{aligned}
    w(H|S) = \sum_{h \in H} w(h | S)\\
\end{aligned}\]</span> Each <span class="math inline">\(w(h|S)\)</span> is a probability of the evidence’s existence given such hypothesis: <span class="math display">\[\begin{aligned}
    w(h|S) = p(S|h)
\end{aligned}\]</span> Such probability comes from the test-observation pairs since they compose the set <span class="math inline">\(S\)</span>. Moreover, each test is independent of other tests, which gives: <span class="math display">\[\begin{aligned}
    p(S|h) = \prod_{(t,o) \in S} p((t,o) | h)
\end{aligned}\]</span> In the noiseless setting, users will select an option that minimizes their cost function (selecting more preferred items), mathematically defined as: <span class="math display">\[\begin{aligned}
    p((t, o = x) | h) =
    \begin{cases}
        1 &amp; c_h(x) &lt; c_h(y)\\
        0 &amp; else
    \end{cases}
    \label{eq:prob_base}
\end{aligned}\]</span></p>
<p><strong>6.3.3.1 User Noise Modeling</strong></p>
<p>As has been discussed, users are not perfect evaluators and even get frustrated if unable to select the better option. Prior work <span class="citation" data-cites="unnoisy_humans">(<a href="#ref-unnoisy_humans" role="doc-biblioref">Amershi et al. 2014</a>)</span> has shown that treating users as perfect can lead to poor performance. That gave rise to accounting for noise in users’ responses, but a majority of such work applies the same noise to all queries and all responses. While those led to great performance results <span class="citation" data-cites="noisy_humans">(<a href="#ref-noisy_humans" role="doc-biblioref">Guillory and Bilmes 2011</a>)</span>, they don’t accurately reflect the real world, which gave rise to the idea of creating query-based noise.</p>
<p>Effectively, for some of the queries it is important to incorporate the fact that the user is unsure and noisy, but for others, if the user is confident, noise in the response is not needed at all. For comparison-based learning, this means that the noise is related to the costs of the two items compared. Specifically for items <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, if <span class="math inline">\(c_{h^*}(x) \simeq c_{h^*}(y)\)</span> then the items are hard to distinguish for the user, so here it is preferred to incorporate user uncertainty and noise. But if <span class="math inline">\(c_{h^*}(x) &gt;&gt; c_{h^*}(y)\)</span>, the user will certainly select <span class="math inline">\(y\)</span> and the other way around, which is where the noise is not needed.</p>
<p>Query-dependent noise is also supported in the psychology literature, which means that such an approach is more related to the real world. In particular, psychologists talk about the Luce-Sheppard Choice rule <span class="citation" data-cites="lus-shep">(<a href="#ref-lus-shep" role="doc-biblioref">Shepard 1957</a>)</span> when talking about comparisons. This rule previously gave rise to a logistic model based on the noise <span class="citation" data-cites="lus-log">(<a href="#ref-lus-log" role="doc-biblioref">Viappiani and Boutilier 2010</a>)</span> where the probability of observation for a given test is: <span class="math display">\[\begin{aligned}
    p((t, o = x) | h) \propto exp(-\gamma * c_h(x))
    \label{eq:noise_model}
\end{aligned}\]</span></p>
<div id="fig:noiseless_1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/Noiseless probs.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>User response model in the noiseless setting</figcaption>
</figure>
</div>
<div id="fig:noiseless_2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/Noise probs.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>User response with Luce Sheppard noise model</figcaption>
</figure>
</div>
<p>Figures <a href="#fig:noiseless_1" data-reference-type="ref" data-reference="fig:noiseless_1">1.8</a> and <a href="#fig:noiseless_2" data-reference-type="ref" data-reference="fig:noiseless_2">1.9</a> demonstrate the difference between the noiseless setting and incorporating the Luce-Sheppard Choice rule. GBS is the baseline model with only 2 response options, and CLAUS is the model with the uncertainty option added. The figures show how incorporating such noise influences and smoothes the probability distribution of the user’s response.</p>
<p><strong>6.3.3.2 User Uncertainty</strong></p>
<p>We will now discuss the functionality of CLAUS, which is an algorithm designed by <span class="citation" data-cites="claus">(<a href="#ref-claus" role="doc-biblioref">Holladay et al. 2016</a>)</span> that allows users to select an uncertain response about the two options that they need to rank. The authors model such uncertainty as <span class="math inline">\(\epsilon\)</span> and it is associated with each <span class="math inline">\(c_h\)</span>, so now every hypothesis <span class="math inline">\(h\)</span> is defined over a pair of <span class="math inline">\((c_h, \epsilon_h)\)</span>. It is important to note that the goal is to still learn and maintain our objective on <span class="math inline">\(c\)</span>, <span class="math inline">\(\epsilon\)</span> is only necessary to model the users’ responses. The uncertainty relates to the cost function in the following way: <span class="math display">\[\begin{aligned}
    |c_h(x) - c_h(y)| &lt; \epsilon_h
\end{aligned}\]</span> this means that the user is uncertain between items <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and their cost difference is negligible such that the user is not able to select which item is better. This in turn gives more information about the real value of the two items, as a binary response would indicate the user’s preference towards one item, which will not be real and will skew the cost functions.</p>
<p>This causes modifications of the problem set-up:</p>
<ol type="1">
<li><p>For test <span class="math inline">\(t(x,y)\)</span> the observation will be <span class="math inline">\(o(x,y) \in \{x&lt;y, x&gt;y, \Tilde{xy}\}\)</span>, where <span class="math inline">\(\Tilde{xy}\)</span> is the uncertain response.</p></li>
<li><p>The probability distribution over the user’s response <a href="#eq:prob_base" data-reference-type="ref" data-reference="eq:prob_base">[eq:prob_base]</a> will now be defined as: $$</p>
<span class="math display">\[\begin{aligned}
        p((t, o = x) | h) =
        \begin{cases}
            1 &amp; c_h(x) &lt; c_h(y) - \epsilon_h\\
            0 &amp; else
        \end{cases}

\end{aligned}\]</span>
<p><span class="math display">\[ \]</span></p>
<span class="math display">\[\begin{aligned}
        p((t, o = \Tilde{xy}) | h) =
        \begin{cases}
            1 &amp; |c_h(x) - c_h(y)|^2 &lt; \epsilon_h^2\\
            0 &amp; else
        \end{cases}

\end{aligned}\]</span>
<p>$$ This means the user confidently selects <span class="math inline">\(x\)</span> when it is better than <span class="math inline">\(y\)</span> by more than <span class="math inline">\(\epsilon\)</span>, but if the squared difference of the cost functions of two items is negligible by <span class="math inline">\(\epsilon\)</span> user will choose the indifferent option.</p></li>
<li><p>Finally this also updates the noise model <a href="#eq:noise_model" data-reference-type="ref" data-reference="eq:noise_model">[eq:noise_model]</a>: $$</p>
<span class="math display">\[\begin{aligned}
            p((t, o = x) | h) \propto \exp(-\gamma * [c_h(x) - c_h(y)]) \label{eq:noise_model}

\end{aligned}\]</span>
<p><span class="math display">\[ \]</span></p>
<span class="math display">\[\begin{aligned}
            p((t, o = \Tilde{xy}) | h) \propto exp(-1/\epsilon_h^2 * [c_h(x) - c_h(y)]^2)

\end{aligned}\]</span>
<p>$$</p></li>
</ol>
<p><strong>6.3.3.3 Performance Analysis</strong></p>
<div id="fig:equiv_c" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/equiv.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>CLAUS using equivalence classes. Each cost function <span class="math inline">\(c\)</span> corresponds to an equivalence class (blue ellipse). Hypotheses (black dots) are <span class="math inline">\(\{c_h,\epsilon_h\}\)</span> pairs. Hypotheses sharing a cost <span class="math inline">\(c\)</span> are said to be inside the equivalence class of <span class="math inline">\(c\)</span>. After performing a test and receiving an observation, the evidence results in downweighting connections among some of the hypotheses.</figcaption>
</figure>
</div>
<p>Before diving deeper into the comparisons of performance, it is important to indicate that rather than predicting a specific pair <span class="math inline">\((c_h, \epsilon_h)\)</span>, the algorithm focuses on predicting a group of pairs that are similar to one another, otherwise called equivalence class (Figure <a href="#fig:equiv_c" data-reference-type="ref" data-reference="fig:equiv_c">1.10</a>), which indicates not essentially different hypothesis for the cost function and uncertainty. That information is learned through each new test, as the algorithm updates the information about <span class="math inline">\(c\)</span> and <span class="math inline">\(\epsilon\)</span> that distinguishes between the distinct <span class="math inline">\(h\)</span>, finding the equivalence groups among them. Moreover, the authors tweaked the parameter responsible for the size of the equivalence class (how many hypotheses can be grouped together at a time).</p>
<div id="fig:claus_num" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/GBS:CLAUS.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>NO CAPTION, ADD CAPTION</figcaption>
</figure>
</div>
<div id="fig:claus_tab">
<table class="caption-top table">
<caption>NO CAPTION, ADD CAPTION</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Query Count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">GBS - About Equal</td>
<td style="text-align: center;"><span class="math inline">\(94.15 \pm 0.52\)</span></td>
<td style="text-align: center;"><span class="math inline">\(36.02 \pm 0.03\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">GBS - Not Sure</td>
<td style="text-align: center;"><span class="math inline">\(\textbf{94.66} \pm \textbf{0.55}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(35.95 \pm 0.04\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">CLAUS - About Equal</td>
<td style="text-align: center;"><span class="math inline">\(91.56 \pm 0.84\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\textbf{25.93} \pm \textbf{0.41}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">CLAUS - Not Sure</td>
<td style="text-align: center;"><span class="math inline">\(90.86 \pm 0.74\)</span></td>
<td style="text-align: center;"><span class="math inline">\(26.98 \pm 0.47\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>The first performance evaluation is done on the number of queries and confirms that it decreases in Figure <a href="#fig:claus_num" data-reference-type="ref" data-reference="fig:claus_num">1.11</a>. The GBS model serves as the baseline, as it will do all of the comparison queries using the binary response options. The CLAUS model is measured over different values of <span class="math inline">\(\epsilon\)</span> on the x-axis and over different sizes of the equivalence sets indicated by different shades of blue. Figure shows that all variants of CLAUS use approximately 10 fewer queries on average compared to GBS. Moreover, using bigger-sized equivalence classes can further decrease the number of needed queries. The most optimal <span class="math inline">\(\epsilon \simeq 0.07\)</span>, after which higher <span class="math inline">\(\epsilon\)</span> does not provide any benefit.</p>
<p>Lastly, the authors considered the performance difference, which is indicated in Figure <a href="#fig:claus_tab" data-reference-type="ref" data-reference="fig:claus_tab">1.2</a>. For that authors used two different labels for the uncertainty button in CLAUS, it was either labeled as "About Equal" or "Not Sure" as those can provoke different responses and feelings in users. Moreover, GBS and CLAUS-type responses were mixed in the same set of questions to the user, which splits the metrics for both in two as can be seen in Figure <a href="#fig:claus_tab" data-reference-type="ref" data-reference="fig:claus_tab">1.2</a>. The performance of CLAUS is lower by <span class="math inline">\(3\%\)</span> on average, indicating similar results to <a href="#sec:geo_app" data-reference-type="ref" data-reference="sec:geo_app">[sec:geo_app]</a>, showing that a smaller number of queries can still lead to a performance loss. However, the second column of Figure <a href="#fig:claus_tab" data-reference-type="ref" data-reference="fig:claus_tab">1.2</a> supports the information in Figure <a href="#fig:claus_num" data-reference-type="ref" data-reference="fig:claus_num">1.11</a>, as it also shows that 10 fewer queries were conducted on average.</p>
</section>
</section>
<section id="active-preference-based-learning-of-reward-functions" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="active-preference-based-learning-of-reward-functions"><span class="header-section-number">3.2.3</span> Active Preference-Based Learning of Reward Functions</h3>
<p>Active learning can be essential in learning within dynamic systems and environments. Say we have an agent in an environment, and we want it to conform to a certain behavior as set by a human. How exactly do we go about doing this? In a traditional RL setting, this is solved by a class of algorithms under Inverse Reinforcement Learning. Techniques such as VICE and GAIL attempt to learn a reward function that can distinguish between states visited by the agent and states desired to be visited as defined by a human. In effect, a human will demonstrate what it would like the agent to do in the environment, and from there, learning is done. However, what if humans do not precisely know how an agent should optimally behave in an environment but still have some opinion on what trajectories would be better than others? This is where a paper like Active Preference-Based Learning of Reward Functions comes into the picture. The paper aims to use human preferences to aid an agent’s learning within a dynamic system.</p>
<p>A dynamic system contains human input, robotic input, and an environment state. The transitions between states is defined by <span class="math inline">\(f_{HR}\)</span>, so that we have: <span class="math display">\[x^{t+1} = f_{HR}(x^t, u_R, u_H)\]</span> At a given time step <span class="math inline">\(t\)</span>, we have <span class="math inline">\(x_t\)</span>, <span class="math inline">\(u_R^t\)</span>, and <span class="math inline">\(u_H^t\)</span>. This can be encapsulated into a single <span class="math inline">\(d\)</span> dimensional feature vector that the authors denote as <span class="math inline">\(\phi\)</span>. The paper then assumes that the underlying reward model we are trying to learn can be represented linearly. If we have our human reward preference function defined as <span class="math inline">\(r_H\)</span>, this means we can write <span class="math inline">\(r_H\)</span> as: <span class="math display">\[r_H(x^t, u_R^t, u_H^t) = w^{\intercal}\phi(x^t, u_R^t, u_H^t)\]</span> Because the reward function is linear, we can take the weight vector out of the summation if we want to calculate the reward over an entire trajectory: <span class="math display">\[R_{H}(x^0, u_R, u_H) = \sum_{t=0}^{N} r_{H}(x^t, u^t, u_H^t)\]</span> <span class="math display">\[\Phi = \sum \phi(x^t, u_R^t, u_H^t)\]</span> <span class="math display">\[R_H(traj) = w\cdot\Phi(traj)\]</span></p>
<section id="properties-of-w" class="level4" data-number="3.2.3.1">
<h4 data-number="3.2.3.1" class="anchored" data-anchor-id="properties-of-w"><span class="header-section-number">3.2.3.1</span> Properties of W</h4>
<p>First, the scale of <span class="math inline">\(w\)</span> does not matter because we only care about the relative rewards produced with <span class="math inline">\(w\)</span> (given two different trajectories, we want to answer the question of which trajectory a human would prefer, i.e.&nbsp;which one has a higher preference reward). This means we can constrain <span class="math inline">\(||w|| &lt;= 1\)</span>, so the initial prior is uniform over a unit ball. From here, we can determine a probabilistic expression to assess whether we should prefer trajectory A or B (because it can be noisy with human input). Let <span class="math inline">\(I_t = +1\)</span> if the human prefers trajectory <span class="math inline">\(A\)</span> and let <span class="math inline">\(I_t = -1\)</span> if the human prefers trajectory <span class="math inline">\(B\)</span>. We get the following for <span class="math inline">\(p(I_t | w)\)</span>.</p>
<p><span class="math display">\[p(I_t = +1|w) = \frac{exp(R_H(traj_A))}{exp(R_H(traj_A)) + exp(R_H(traj_B)}\]</span> <span class="math display">\[p(I_t = -1|w) = \frac{exp(R_H(traj_B))}{exp(R_H(traj_A)) + exp(R_H(traj_B)}\]</span></p>
<p>We can re-write this expression to make it cleaner, using the following substitution: <span class="math display">\[\psi = \Phi(traj_a) - \Phi(traj_b)\]</span> <span class="math display">\[f_{\psi} (w) = p(I_t|w) = \frac{1}{1 + exp(-I_tw^{\intercal}\psi}\]</span></p>
<p>The idea now is that we can update <span class="math inline">\(p(w)\)</span> everytime we get a result from a human preference query using Bayes:</p>
<p><span class="math display">\[p(w|I_t) &lt;- p(w) \cdot p(I_t|w)\]</span></p>
<p>We don’t need to know <span class="math inline">\(p(I_t)\)</span> because we can use an algorithm like the Metropolis algorithm to actually sample.</p>
</section>
<section id="generating-queries" class="level4" data-number="3.2.3.2">
<h4 data-number="3.2.3.2" class="anchored" data-anchor-id="generating-queries"><span class="header-section-number">3.2.3.2</span> Generating Queries</h4>
<p>This is where the interesting part of the paper comes into play. How do we actually generate queries for the user to pick between? This paper synthetically generates queries through an optimization process and then presents them to a human to pick between. The idea is that we want to generate a query that maximizes the conditional entropy <span class="math inline">\(H(I|w)\)</span>. There are a few ways to think about this – intuitively we want to pick a query that we are most uncertain about given our current weights (thus having the highest conditional entropy given the weights). The way the authors of the paper frame this originally in the paper is that "we want to find the next query such that it will help us remove as much volume (the integral of the unnormalized pdf over w) as possible from the space of possible rewards." Mathematically this can be written as:</p>
<p><span class="math display">\[max_{x^0, u_R, u_H^A, u_H^B} min\{E[1-f_{\psi}(w)], E[1 - f_{-\psi}(w)]\}\]</span></p>
<p>But how exactly do we optimize this expression mathematically? After all, we need to use this expression to generate synthetic queries. The answer is to sample <span class="math inline">\(w_1, ... w_m\)</span> from <span class="math inline">\(p(w)\)</span>. We can assume we are sampling points from a point cloud, thus approximating the distribution <span class="math inline">\(p(w)\)</span> as</p>
<p><span class="math display">\[p(w) = \frac{1}{M} \sum \delta (w_i)\]</span>. We can now approximate the expectation expression like so: <span class="math display">\[E[1 - f_{\psi}(w)] = \frac{1}{M} (\sum 1 - f_{\psi}(w_i))\]</span></p>
<p>and now we can optimize the expression to generate a synthetic query! Altogether, the algorithm looks like the following:</p>
<div class="algorithm">
<div class="algorithmic">
<p><strong>input:</strong> features <span class="math inline">\(\phi\)</span>, horizon <span class="math inline">\(N\)</span>, dynamics <span class="math inline">\(f\)</span>, <span class="math inline">\(iter\)</span> <strong>initialize:</strong> <span class="math inline">\(p(w) \sim Uniform(B)\)</span>, for a unit ball <span class="math inline">\(B\)</span> <span class="math inline">\(W \gets M\)</span> samples from <span class="math inline">\(AdaptiveMetropolis(p(w))\)</span> <span class="math inline">\((x^0, u_R, u^A_H, u^B_H) \gets SynthExps(W,f)\)</span> <span class="math inline">\(I_t \gets QueryHuman(x^0, u_R, u^A_H, u^B_H)\)</span> <span class="math inline">\(\varphi = \Phi(x^0, u_R, u^A_H) - \Phi(x^0, u_R, u^B_H)\)</span> <span class="math inline">\(f_\varphi(w) = \min(1, I_t\exp(w^\top \varphi))\)</span> <span class="math inline">\(p(w) \gets p(w) \cdot f_\varphi(w)\)</span> <span class="math inline">\(t \gets t+1\)</span> <strong>output:</strong> distribution of <span class="math inline">\(w: p(w)\)</span></p>
</div>
</div>
</section>
<section id="batching-queries" class="level4" data-number="3.2.3.3">
<h4 data-number="3.2.3.3" class="anchored" data-anchor-id="batching-queries"><span class="header-section-number">3.2.3.3</span> Batching Queries</h4>
<p>The algorithm itself works well, however there ends up being a bottle neck that each query needs to be synthesized before being sent to the human – one at a time. In other words, the human gives their feedback, waits for a query to be synthesized, and then gives another data point of feedback. There is no room for parallelization and so the authors proposed a second algorithm in a separate paper that allows for the batching of queries. Simply put, we change the mathematical expression to the following:</p>
<p><span class="math display">\[max_{\xi_{ib+1_A}, \xi_{ib+1_B}, ... , \xi_{ib+b_A}, \xi_{ib+b_B} H(I_{ib+1}, I_{ib+2}, .., I_{ib+b} | w)}\]</span></p>
<p>Naively, we could consider optimizing this in the greedy fashion. This would mean just synthetically generating <span class="math inline">\(b\)</span> independent queries. The obvious drawback of this method would be that the queries would likely be very similar to each other. The authors propose a few other heuristics that would help guide the algorithm away from generating very similar queries. As an example, the authors propose Medioid Selection where we have to cluster <span class="math inline">\(B\)</span> greedy vectors into <span class="math inline">\(b &lt; B\)</span> groups and pick one vector frmo each group (the medioid). The authors also propose two other methods rooted in providing different queries: boundary medioids selection and successive elimination. They are best visually depicted as:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/greedy.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>image</figcaption>
</figure>
</div>
</section>
<section id="results" class="level4" data-number="3.2.3.4">
<h4 data-number="3.2.3.4" class="anchored" data-anchor-id="results"><span class="header-section-number">3.2.3.4</span> Results</h4>
<p>The authors test both the non-batched and variety of batched learning algorithms on multiple environments:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/activeresults.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>image</figcaption>
</figure>
</div>
<p>What is interesting to note is that when graphed over <span class="math inline">\(N\)</span> the non-batched active learning approach does in the same ball-park of performance as the batched approaches. However, if you graph it over time, we see that learning is a much slower process when not-batched.</p>
</section>
</section>
</section>
<section id="foundation-models-for-robotics" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="foundation-models-for-robotics"><span class="header-section-number">3.3</span> Foundation Models for Robotics</h2>
<p>Modern foundation models have been ubiquitous in discussions of powerful, general purpose AI systems that can accomplish myriad tasks across many disciplines such as programming, medicine, law, open question-answering and much more, with rapidly increasing capabilities <span class="citation" data-cites="bommasani2022opportunities">(<a href="#ref-bommasani2022opportunities" role="doc-biblioref">Bommasani et al. 2022</a>)</span>. However, despite successes from large labs in controlled environments <span class="citation" data-cites="brohan2023rt2">(<a href="#ref-brohan2023rt2" role="doc-biblioref">Brohan et al. 2023</a>)</span> foundation models have not seen ubiquitous use in robotics due to shifting robot morphology, lack of data, and the sim to real gap in robotics <span class="citation" data-cites="walke2023bridgedata">(<a href="#ref-walke2023bridgedata" role="doc-biblioref">Walke et al. 2023</a>)</span>. For this subsection we explore two promising approaches known as R3M and Voltron which are the first to leverage pre-training on vast amounts of data towards performance improvement on downstream robotic tasks despite the aforementioned issues <span class="citation" data-cites="nair2022r3m karamcheti2023languagedriven">(<a href="#ref-nair2022r3m" role="doc-biblioref">Nair et al. 2022</a>; <a href="#ref-karamcheti2023languagedriven" role="doc-biblioref">Karamcheti et al. 2023</a>)</span>.</p>
<section id="r3m-universal-visual-representation-for-robotics" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="r3m-universal-visual-representation-for-robotics"><span class="header-section-number">3.3.1</span> R3M: Universal Visual Representation for Robotics</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/r3m.png" class="img-fluid figure-img" style="width:95.0%"></p>
<figcaption>R3M pipeline</figcaption>
</figure>
</div>
<p>R3M represents a significant advancement in the field of robotic manipulation and learning. This model diverges from traditional approaches that rely on training from scratch within the same domain on the same robot data as instead it leverags pretraining on large datasets, akin to the practices in computer vision and natural language processing (NLP) where models are trained on diverse, large-scale datasets to create reusable, general-purpose representations.</p>
<p>The core principle behind R3M is its training methodology. It is pre-trained on a wide array of human videos, encompassing various activities and interactions. This diverse dataset enables the model to capture a broad spectrum of physical interactions and dynamics, which are crucial for effective robotic manipulation known as EGO4D <span class="citation" data-cites="grauman2022ego4d">(<a href="#ref-grauman2022ego4d" role="doc-biblioref">Grauman et al. 2022</a>)</span>. However, prior papers could not fit this dataset well, and R3M leveraged. The training utilizes a unique objective that combines time contrastive learning, video-language alignment, and a sparsity penalty. This objective ensures that R3M not only understands the temporal dynamics of scenes (i.e., how states transition over time) but also focuses on semantically relevant features, such as objects and their interrelations, while maintaining a compact and efficient representation.</p>
<p>What sets R3M apart in the realm of robotics is its efficiency and effectiveness in learning from a limited amount of data. The model demonstrates remarkable performance in learning tasks in the real world with minimal human supervision – typically less than 10 minutes. This is a stark contrast to traditional models that require extensive and often prohibitively large datasets for training. Furthermore, R3M’s pre-trained nature allows for its application across a variety of tasks and environments without the need for retraining from scratch, making it a versatile tool in robotic manipulation. The empirical results from using R3M are compelling, leading to a 10% improvement over training from a pretrained image-net model, self-supervised approaches such as MoCo or even CLIP <span class="citation" data-cites="deng2009imagenet he2020momentum radford2021learning">(<a href="#ref-deng2009imagenet" role="doc-biblioref">Deng et al. 2009</a>; <a href="#ref-he2020momentum" role="doc-biblioref">He et al. 2020</a>; <a href="#ref-radford2021learning" role="doc-biblioref">Radford et al. 2021</a>)</span>. Note however, that R3m does <strong>not</strong> use any language data which leaves quite a bit of supervision to be desired.</p>
</section>
<section id="voltron-language-driven-representation-learning-for-robotics" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="voltron-language-driven-representation-learning-for-robotics"><span class="header-section-number">3.3.2</span> Voltron: Language Driven Representation Learning for Robotics</h3>
<p>Building off the success of R3M, Voltron proposes a further extension of leveraging self-supervision and advancements in foundation models, and multi-modality. Voltron takes on an intuitive and simple dual use objective, where the trained model alternates between predicting the task in an image through natural language and classifying images based on a natural text label. This forces a nuanced understanding of both modalities <span class="citation" data-cites="radford2021learning">(<a href="#ref-radford2021learning" role="doc-biblioref">Radford et al. 2021</a>)</span>.</p>
<p>Voltron’s approach is distinguished by its versatility and depth of learning. It is adept at handling a wide range of robotic tasks, from low-level spatial feature recognition to high-level semantic understanding required in language-conditioned imitation and intent scoring. This flexibility makes it suitable for various applications in robotic manipulation, from grasping objects based on descriptive language to performing complex sequences of actions in response to verbal instructions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/voltron.png" class="img-fluid figure-img" style="width:95.0%"></p>
<figcaption>Voltron pipeline</figcaption>
</figure>
</div>
<p>The authors rigorously test Voltron in scenarios such as dense segmentation for grasp affordance prediction, object detection in cluttered scenes, and learning multi-task language-conditioned policies for real-world manipulation with up to 15% improvement over baselines. In each of these domains, Voltron has shown a remarkable ability to outperform existing models like MVP and R3M, showcasing its superior adaptability and learning capabilities <span class="citation" data-cites="xiao2022masked">(<a href="#ref-xiao2022masked" role="doc-biblioref">Xiao et al. 2022</a>)</span>.</p>
<p>Moreover, Voltron’s framework allows for a balance between encoding low-level and high-level features, which is critical in the context of robotics. This balance enables the model to excel in both control tasks and those requiring deeper semantic understanding, offering a comprehensive solution in the realm of robotic vision and manipulation.</p>
<p>Voltron stands as a groundbreaking approach in the field of robotics, offering a language-driven, versatile, and efficient approach to learning and manipulation. Its ability to seamlessly integrate visual and linguistic data makes it a potent tool in the ever-evolving landscape of robotic technology, with potential applications that extend far beyond current capabilities. Interesting the authors show Voltron does not beat R3M off the shelf but only when trained on similar amounts of data. Nevertheless, Voltron’s success in diverse tasks and environments heralds a new era in robotic manipulation, where language and vision coalesce to create more intelligent, adaptable, and capable robotic systems.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">3.4</span> Conclusion</h2>
<p>On the note of applying active learning to RL and environment settings, there have been many recent papers that have attempted to extend this to more modern RL environments. For example, the paper "When to Ask for Help" <span class="citation" data-cites="ask_help">(<a href="#ref-ask_help" role="doc-biblioref">Xie et al. 2022</a>)</span> examines the intersection of autonomous and active learning. Instead of just expecting an RL agent to autonomously solve a task, making the assumption that an agent could get stuck and need human input to get "unstuck" is a key insight of the paper. In general, there has been an emphasis in recent literature in robotics on not just blindly using demonstration data as a form of human input, but rather actively querying a human and using this to better synthesize correct actions.</p>
<p>Active learning holds promise for enhancing AI models in real-world scenarios, yet several challenges persist. This discussion aims to provide an overview of these challenges.</p>
<p><strong>Task-Specific Considerations:</strong></p>
<p>For certain tasks, the input space of a model may have some rare yet extremely important pockets which may never be discovered by active learning and may cause severe blindspots in the model. In medical imaging for instance, there can be rare yet critical diseases. Designing AL strategies for medical image analysis must prioritize rare classes, such as various forms of cancers. Oftentimes, collecting data around those rare classes is not a recommendation of the active learning process because these examples constitute heavy distribution drifts from the input distribution a model has seen.</p>
<p><strong>Complex Task Adaptation:</strong></p>
<p>AL has predominantly been adopted for simple classification tasks, leaving more other types of tasks (generative ones for instance), less explored. In Natural Language Processing, tasks like natural language inference, question-answering pose additional complexities that affect the direct application of the active learning process. While machine translation has seen AL applications, generation tasks in NLP require more thorough exploration. Challenges arise in obtaining unlabeled data, particularly for tasks with intricate inputs.</p>
<p><strong>Unsupervised and Semi-Supervised Approaches:</strong></p>
<p>In the presence of large datasets without sufficient labels, unsupervised and semi-supervised approaches become crucial. These methods offer a means to extract information without relying on labeled data for every data point, potentially revolutionizing fields like medical image analysis. There is an ongoing need for methods that combine self/semi-supervised learning with active learning.</p>
<p><strong>Algorithm Scalability:</strong></p>
<p>Scalability is a critical concern for online AL algorithms, particularly when dealing with large datasets and high-velocity data streams. The computational demands of AL can become prohibitive as data volume increases, posing challenges for practical deployment. Issues of catastrophic forgetting and model plasticity further complicate scalability, requiring careful consideration in algorithm design.</p>
<p><strong>Labeling Quality Assurance:</strong></p>
<p>The effectiveness of most online AL strategies hinges on the quality of labeled data. Ensuring labeling accuracy in real-world scenarios is challenging, with human annotators prone to errors, biases, and diverse interpretations. Addressing imperfections in labeling through considerations of oracle imperfections becomes essential in real-life AL applications. Solutions for cleaning up data and verifying its quality need to be more aggressively pursued.</p>
<p><strong>Data Drift Challenges:</strong></p>
<p>Real-world settings introduce data drift, where distributions shift over time, challenging models to adapt for accurate predictions. These shifts can impact the quality of labeled data acquired in the AL process. For example, the criterion or proxy used for selecting informative instances may be thrown off when the distribution a model is trained on, and the distribution we want it to perform well on, are too far away from one another.</p>
<p><strong>Evaluation in Real-Life Scenarios</strong>:</p>
<p>While AL methods are often evaluated assuming access to ground-truth labels, the real motivation for AL lies in label scarcity. Assessing the effectiveness of AL strategies becomes challenging in real-life scenarios where ground-truth labels may be limited. In other words, one may verify the goodness of an AL algorithm within the lab, but once the algorithm is deployed for improving all sorts of models on all sorts of data distributions, verifying whether AL is actually improving a model is tricky, especially when collecting and labeling data from the target distribution is expensive and defeats the purpose of using AL in the first place.</p>
<p>By systematically addressing these challenges, the field of active learning in AI can progress towards more effective and practical applications.</p>
<p>In summary, active learning is a promising modern tool to model training that presents potential benefits. As was mentioned at the start, there are numerous approaches that can be employed by active learning, starting from reducing error of model’s prediction, reducing variance, to more conformal predictions. The flavor of active learning heavily depends on the applications, which include robotics, LLM, autonomous vehicles, and more. We discussed in more detail how to perform active learning for variance reduction in the case of predicting kinematics of the robotic arms, which showed decrease in MSE as well as more stable reduction in it. Next we talked about using active learning for reducing the number of comparisons required to create a ranking of objects, and the examples discussed were able to achieve that but with some loss in the prediction accuracy. Finally, we discussed how active learning can be used for modeling of reward functions within a dynamical system, which demonstrated improvements in performance and time required to achieve it. For a more hands-on experience with active learning and demonstrated example, we encourage the readers to explore a blogpost by Max Halford <span class="citation" data-cites="max_halford">(<a href="#ref-max_halford" role="doc-biblioref">Halford 2023</a>)</span>.</p>
</section>
<section id="introduction-to-performance-metric-elicitation" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="introduction-to-performance-metric-elicitation"><span class="header-section-number">3.5</span> Introduction to Performance Metric Elicitation</h2>
<p>In binary classification problems, one important consideration is the selection of an appropriate performance metric corresponding to the real-world task at hand. The problem of <em>metric elicitation</em> attempts to characterize and discover the performance metric of a practitioner, reflecting the rewards or costs that come with correct or incorrect classification. For example, in medical contexts such as diagnosing a disease or choosing whether a given treatment is appropriate for a certain condition, tradeoffs are made for incorrect decisions; for example, not administering a treatment could lead to the worsening of a disease (a false negative), whereas delivering the wrong kind of treatment could lead to adverse side effects that would be worse than not treating the condition (a false positive).</p>
<p>Rather than choosing from a limited set of default choices like the F1-score or weighted accuracy, metric elicitation considers the process by which we can devise a metric that best matches the preferences of the practitioners or users, by querying an “oracle" who provides feedback on proposed potential metrics in the form of pairwise comparisons. Because queries to humans are often expensive, the aim is to minimize the amount of comparisons needed.</p>
<p><strong>Note:</strong> Almost all of the content in this chapter comes from “Performance Metric Elicitation from Pairwise Classifier Comparisons” by Hiranandani et al. <span class="citation" data-cites="pmlr-v89-hiranandani19a">(<a href="#ref-pmlr-v89-hiranandani19a" role="doc-biblioref">Hiranandani et al. 2019a</a>)</span>, which introduced the problem of metric elicitation and the framework for binary-class metric elicitation from pairwise comparisons. In this chapter, we aim to present their work expository while providing additional motivation and intuitive explanations to supplement their work.</p>
<p>The motivation for the pairwise comparison portion of metric elicitation comes from a rich history of literature in psychology, economics, and computer science <span class="citation" data-cites="pref1 pref2 pref3 pref4 ab">(<a href="#ref-pref1" role="doc-biblioref">Samuelson 1938</a>; <a href="#ref-pref2" role="doc-biblioref">Mas-Colell 1977</a>; <a href="#ref-pref3" role="doc-biblioref">Varian 2006</a>; <a href="#ref-pref4" role="doc-biblioref">Braziunas and Boutilier 2012</a>; <a href="#ref-ab" role="doc-biblioref">Tamburrelli and Margara 2014</a>)</span> demonstrating that humans are often ineffective at providing absolute feedback on things like potential prices, user interfaces, or even ML model outputs (hence the comparison-based structure of RLHF, for instance). In addition, confusion matrices are a way to accurately capture binary metrics such as accuracy, <span class="math inline">\(F_\beta\)</span>, and Jaccard similarity by recording the number of false positives, true positives, false negatives, and true negatives obtained by a certain classifier. The main goal of this chapter is to introduce two binary-search procedures that can be used to approximate the oracle’s performance metric for two types of metrics (linear and linear-fractional performance metrics) by presenting the oracle with confusion matrices generated by various classifiers; in essence, we are learning an optimal threshold for classification given a decision boundary for a binary classification problem.</p>
<p>First, we introduce some relevant notation that will later be used to formalize notions of oracle queries, classifiers, and metrics.</p>
<ul>
<li><p><span class="math inline">\(X \in \mathcal{X}\)</span> is an input random variable.</p></li>
<li><p><span class="math inline">\(Y \in \{0, 1\}\)</span> is the output random variable.</p></li>
<li><p>We learn from a dataset of size <span class="math inline">\(n\)</span>, denoted by <span class="math inline">\(\{(x, y)_i\}^n_{i=1}\)</span>, generated iid from some distribution <span class="math inline">\(\mathbb{P}(X, Y)\)</span>.</p></li>
<li><p><span class="math inline">\(\eta(\vec{x}) = \mathbb{P}(Y=1 | X=x)\)</span> is the conditional probability of the positive class, given some sample <span class="math inline">\(x\)</span>.</p></li>
<li><p><span class="math inline">\(\zeta = \mathbb{P}(Y=1)\)</span> is the unconditional probability of the positive class</p></li>
<li><p>The set of all potential classifiers is <span class="math inline">\(\mathcal{H} = \{h : \mathcal{X} \rightarrow \{0,1\}\}\)</span></p></li>
<li><p>The confusion matrix for some classifier <span class="math inline">\(h\)</span> is <span class="math inline">\(C(h, \mathbb{P}) \in \mathbb{R}^{2 \times 2}\)</span>, where <span class="math inline">\(C_{ij}(h, \mathbb{P}) = \mathbb{P}(Y=i, h=j)\)</span> for <span class="math inline">\(i, j \in \{0,1\}\)</span>; these represent the false positives, true positives, false negatives, and true negatives, so <span class="math inline">\(\sum_{i,j}C_{ij}=1\)</span>.</p></li>
<li><p><span class="math inline">\(\mathcal{C}\)</span> is the set of all confusion matrices</p></li>
<li><p>Note: Since <span class="math inline">\(FN(h, \mathbb{P}) =\zeta - TP(h, \mathbb{P})\)</span> and <span class="math inline">\(FP(h, \mathbb{P}) = 1 - \zeta - TN(h, \mathbb{P})\)</span>; <span class="math inline">\(\mathcal{C}\)</span> is in fact a 2-dimensional space, not a 4-dimensional space</p></li>
<li><p>Any hyperplane (line) in <span class="math inline">\((tp, tn)\)</span> given by <span class="math inline">\(\ell := a \cdot tp + b \cdot tn = c; a, b, c\in \mathbb{R}\)</span></p></li>
<li><p>Given a classifier <span class="math inline">\(h\)</span>, we define a performance metric <span class="math inline">\(\phi : [0, 1]^{2 \times 2} \rightarrow \mathbb{R}\)</span>. We refer to the value <span class="math inline">\(\phi(C(h))\)</span>, which represents the performance of a certain classifier with respect to a certain metric, as the <em>utility</em> of the classifier <span class="math inline">\(h\)</span>. We assume, without loss of generality, that a higher value of <span class="math inline">\(\phi\)</span> means <span class="math inline">\(h\)</span> is a better performance metric.</p></li>
</ul>
<p>Our focus is to recover some metric <span class="math inline">\(\phi\)</span> using comparisons between confusion matrices <span class="math inline">\(C(h)\)</span>, determined by classifiers <span class="math inline">\(h\)</span>, which comes close to the oracle’s “ground-truth" metric <span class="math inline">\(\phi^*\)</span>.</p>
<p>Next, we introduce two classes of performance metrics, for which we will present two elicitation algorithms. A <em>linear performance metric (LPM)</em>, given some constants <span class="math inline">\(\{a_{11}, a_{01}, a_{10}, a_{00}\} \in \mathbb{R}^{4}\)</span>, is of the form <span class="math display">\[\begin{aligned}
\phi(C) &amp; = a_{11} T P + a_{01} F P + a_{10} F N + a_{00} TN  = m_{11} T P + m_{00} T N + m_{0},
\end{aligned}\]</span> where <span class="math inline">\(m_{11} = (a_{11} - a_{10})\)</span>, <span class="math inline">\(m_{00} = (a_{00} - a_{01})\)</span>, and <span class="math inline">\(m_{0} = a_{10} \zeta + a_{01} (1 - \zeta)\)</span>; the second line is a useful parametrization of the metric, constructed using our observation about the dimensionality of <span class="math inline">\(\mathcal{C}\)</span>. For example, one common LPM is weighted accuracy: <span class="math inline">\(WA = w_1TP + w_2TN\)</span>, where varying the proportion between <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> corresponds to different importances afforded to various types of misclassification.</p>
<p>A slightly more complicated class of metrics are the <em>Linear-Fractional Performance Metrics (LFPM)</em>; given constants <span class="math inline">\(\{a_{11}, a_{01}, a_{10}, a_{00}, b_{11}, b_{01}, b_{10}, b_{00}\} \in \mathbb{R}^{8}\)</span>, an LFPM is defined as: <span class="math display">\[\begin{aligned}
\phi(C) &amp; = \frac{a_{11} T P + a_{01} F P + a_{10} F N + a_{00} T N}{b_{11} T P + b_{01} F P + b_{10} F N + b_{00} T N} = \frac{p_{11} T P + p_{00} T N + p_{0}}{q_{11} T P + q_{00} T N + q_{0}}
\end{aligned}\]</span> where <span class="math inline">\(p_{11} = (a_{11} - a_{10})\)</span>, <span class="math inline">\(p_{00} = (a_{00} - a_{01})\)</span>, <span class="math inline">\(q_{11} = (b_{11} - b_{10})\)</span>, <span class="math inline">\(q_{00} = (b_{00} - b_{01})\)</span>, <span class="math inline">\(p_{0} = a_{10} \zeta + a_{01} (1 - \zeta)\)</span>, and <span class="math inline">\(q_{0} = b_{10} \zeta + b_{01} (1 - \zeta)\)</span>; again, these are useful reparametrizations that will simplify the elicitation process by reducing the number of variables to consider. Some commonly-used LFPMs are <span class="math inline">\(F_\beta\)</span> score and Jaccard similarity, given by <span class="math display">\[F_{\beta}=\frac{T P}{\frac{T P}{1+\beta^{2}}-\frac{T N}{1+\beta^{2}}+\frac{\beta^{2} \zeta+1-\zeta}{1+\beta^{2}}}, J A C=\frac{T P}{1-T N};\]</span> Taking <span class="math inline">\(\beta = 1\)</span>, for example, gives the familiar F1 score, which is often used as a metric in ML classification problems.</p>
<p>Defining these notions of LPMs and LFPMs will allow us to consider a far more general array of metrics for learning problems, potentially allowing us to align better with practitioners’ preferences.</p>
</section>
<section id="bayes-optimal-and-inverse-optimal-classifiers" class="level2 page-columns page-full" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="bayes-optimal-and-inverse-optimal-classifiers"><span class="header-section-number">3.6</span> Bayes Optimal and Inverse-Optimal Classifiers</h2>
<p>In addition, we define the notions of Bayes optimal and inverse-optimal classifiers. Given a performance metric <span class="math inline">\(\phi\)</span>, we define:</p>
<ul>
<li><p>the <em>Bayes utility</em> as <span class="math inline">\(\bar{\tau} := \sup_{h \in \mathcal{H}} \phi(C(h)) = \sup_{C \in \mathcal{C}} \phi(C)\)</span>;<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> this is the highest achievable utility (using the metric <span class="math inline">\(\phi\)</span>) over all classifiers <span class="math inline">\(h \in \mathcal{H}\)</span> for a given problem.</p></li>
<li><p>the <em>Bayes classifier</em> as <span class="math inline">\(\bar{h} := \arg \max_{h \in \mathcal{H}} \phi(C(h))\)</span>; this is the classifier <span class="math inline">\(h\)</span> corresponding to the Bayes utility.</p></li>
<li><p>the <em>Bayes confusion matrix</em> as <span class="math inline">\(\bar{C} := \arg \max_{C \in \mathcal{C}} \phi(C)\)</span>; this is the confusion matrix corresponding to the Bayes utility and classifier.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Here, the “<span class="math inline">\(\sup\)</span>" operation refers to the <em>supremum</em>, which is the largest value that a certain set can take on, whereas”<span class="math inline">\(\inf\)</span>" would refer to the <em>infimum</em>, or the smallest value a certain set can take on.</p></div></div><p>Similarly, the inverse Bayes utility, classifier, and confusion matrix can be defined by replacing “<span class="math inline">\(\sup\)</span>" with”<span class="math inline">\(\inf\)</span>"; they represent the classifier and confusion matrix corresponding to the lower bound on utility for a given problem.</p>
<p>We also have the following useful proposition:</p>
<div class="prop">
<p>Let <span class="math inline">\(\phi \in \varphi_{L P M}\)</span>. Then</p>
<p><span class="math display">\[\bar{h}(x)=\left\{\begin{array}{lr}
\mathbbm{1}\left[\eta(x) \geq \frac{m_{00}}{m_{11}+m_{00}}\right], &amp; m_{11}+m_{00} \geq 0 \\
\mathbbm{1}\left[\frac{m_{00}}{m_{11}+m_{00}} \geq \eta(x)\right], &amp; \text { o.w. }
\end{array}\right\}\]</span></p>
<p>is a Bayes optimal classifier w.r.t <span class="math inline">\(\phi\)</span>. The inverse Bayes classifier is given by <span class="math inline">\(\underline{h}=1-\bar{h}\)</span>.</p>
</div>
<p>This is a simple derivation based on the fact that we only get rewards from true positives and true negatives. Essentially, if we recover an LPM, we can use it to determine the best-performing classifier, obtained by placing a threshold on the conditional probability of a given sample, that corresponds to a confusion matrix. Therefore, the three notions of Bayes utility, classifier, and confusion matrix are functionally equivalent in our setting.</p>
</section>
<section id="sec:metric-elicitation" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="sec:metric-elicitation"><span class="header-section-number">3.7</span> Metric Elicitation Problem Setup</h2>
<p>Finally, we will formalize the problem of metric elicitation.</p>
<p>Given two classifiers <span class="math inline">\(h, h'\)</span> (or equivalently, two confusion matrices <span class="math inline">\(C, C'\)</span>), we define an <em>oracle query</em> as the function <span class="math inline">\(\Gamma\left(h, h^{\prime}\right)=\Omega\left(C, C^{\prime}\right)=\mathbbm{1}\left[\phi(C)&gt;\phi\left(C^{\prime}\right)\right]=: \mathbbm{1}\left[C \succ C^{\prime}\right]\)</span>, which represents the classifier that is preferred by the practitioner.</p>
<p>Then, we can define the metric elicitation problem for populations.</p>
<div class="defn">
<p>Suppose the true (oracle) performance metric is <span class="math inline">\(\phi\)</span>. The goal is to recover a metric <span class="math inline">\(\hat{\phi}\)</span> by querying the oracle for as few pairwise comparisons of the form <span class="math inline">\(\Omega\left(C, C^{\prime}\right)\)</span> so that <span class="math inline">\(\|\phi-\hat{\phi}\|_{--}&lt;\kappa\)</span> for a sufficiently small <span class="math inline">\(\kappa &gt; 0\)</span> and for any suitable norm <span class="math inline">\(\|\cdot\|_{--}\)</span>.</p>
</div>
<p>In practice, we would not have access to the true probability distribution or the population, which would give us the true values of <span class="math inline">\(C\)</span> and <span class="math inline">\(C'\)</span>. We can, however, subtly alter this problem description to use <span class="math inline">\(\hat{C}\)</span> and <span class="math inline">\(\hat{C}^{\prime}\)</span>, which come from our dataset of <span class="math inline">\(n\)</span> samples.</p>
<div class="defn">
<p>Suppose the true (oracle) performance metric is <span class="math inline">\(\phi\)</span>. The aim is to recover a metric <span class="math inline">\(\hat{\phi}\)</span> by querying the oracle for as few pairwise comparisons of the form <span class="math inline">\(\Omega\left(\hat{C}, \hat{C}^{\prime}\right)\)</span> so that <span class="math inline">\(\|\phi-\hat{\phi}\|_{--}&lt;\kappa\)</span> for a sufficiently small <span class="math inline">\(\kappa &gt; 0\)</span> and for any suitable norm <span class="math inline">\(\|\cdot\|_{--}\)</span>.</p>
</div>
<p>As is common in theoretical ML research, we solve the population problem and then consider ways to extend this to practical settings where we only have limited datasets of samples. In our case, this would correspond to calculating the confusion matrices from a portion of the dataset we have access to.</p>
</section>
<section id="subsec:confusion-matrices" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="subsec:confusion-matrices"><span class="header-section-number">3.8</span> Confusion Matrices</h2>
<p>Since we are considering all possible metrics in the LPM and LFPM families, we need to make certain assumptions about <span class="math inline">\(\mathcal{C}\)</span>. Particularly, we will assume that <span class="math inline">\(g(t) = \mathbb{P}[\eta(X) \geq t]\)</span> is continuous and strictly decreasing for <span class="math inline">\(t \in [0, 1]\)</span>; essentially, <span class="math inline">\(\eta\)</span> has positive density and zero probability.</p>
<p>In addition, <span class="math inline">\(\mathcal{C}\)</span> is convex, closed, and contained in the rectangle <span class="math inline">\([0, \zeta] \times[0,1-\zeta]\)</span>, and rotationally symmetric around its center, <span class="math inline">\((\frac{\zeta}{2}, \frac{1-\zeta}{2})\)</span>, where the axes represent the proportion of true positives and negatives. Also, the only vertices of <span class="math inline">\(\mathcal{C}\)</span> are <span class="math inline">\((0,1-\zeta)\)</span> and <span class="math inline">\((\zeta, 0)\)</span>, corresponding to predicting all <span class="math inline">\(0\)</span>’s or all <span class="math inline">\(1\)</span>’s on a given dataset. Therefore, <span class="math inline">\(\mathcal{C}\)</span> is strictly convex, and any line that is tangent to it is tangent at exactly one point, corresponding to one particular confusion matrix; these properties can be visually observed in Figure <a href="#fig:c" data-reference-type="ref" data-reference="fig:c">1.12</a>.</p>
<div id="fig:c" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/Screenshot 2023-11-13 at 6.56.44 PM.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Visual representation of <span class="math inline">\(\mathcal{C}\)</span></figcaption>
</figure>
</div>
<p>Next, recall that an LPM is represented in terms of three parameters (<span class="math inline">\(\phi = m_{11}TP + m_{00}TN + m_0\)</span>). We have just seen that this LPM and its corresponding confusion matrix correspond to a certain point on the boundary of <span class="math inline">\(\mathcal{C}\)</span>. We first note that this point is independent of <span class="math inline">\(m_0\)</span>. In addition, we only care about the relative weightings of <span class="math inline">\(m_{11}\)</span> and <span class="math inline">\(m_{00}\)</span>, not their actual values–they are scale invariant. Therefore, we can parametrize the space of LPMs as <span class="math inline">\(\varphi_{L P M}=\{\mathbf{m}=(\cos \theta, \sin \theta): \theta \in[0,2 \pi]\}\)</span>, where <span class="math inline">\(\cos \theta\)</span> corresponds to <span class="math inline">\(m_{00}\)</span> and <span class="math inline">\(\sin \theta\)</span> corresponds to <span class="math inline">\(m_{11}\)</span>. And, as we already know, we can recover the Bayes classifier given <span class="math inline">\(\mathbf{m}\)</span>, and it is unique, corresponding to one point on the boundary of <span class="math inline">\(\mathcal{C}\)</span>, due to its convexity. The supporting hyperplane at this point is defined as</p>
<p><span class="math display">\[\bar{\ell}_{\mathbf{m}}:=m_{11} \cdot tp+m_{00} \cdot tn=m_{11} \overline{TP}_{\mathbf{m}}+m_{00} \overline{TN}_{\mathbf{m}}\]</span></p>
<p>We note that if <span class="math inline">\(m_{00}\)</span> and <span class="math inline">\(m_{11}\)</span> have opposite signs, then <span class="math inline">\(\bar{h}_m\)</span> is the trivial classifier predicting all 1’s or all 0’s, since either predicting true positives or true negatives results in negative reward. This corresponds to a supporting hyperplane with positive slope, so it can only be tangent at the vertices.</p>
<p>In addition, the boundary <span class="math inline">\(\partial \mathcal{C}\)</span> can be split into upper and lower boundaries (<span class="math inline">\(\partial \mathcal{C}_{+}, \partial \mathcal{C}_{-}\)</span>), corresponding to <span class="math inline">\(\theta \in (0, \pi/2)\)</span> and <span class="math inline">\(\theta \in (\pi, 3\pi/2)\)</span> respectively (and whether <span class="math inline">\(m_{00}, m_{11}\)</span> are positive or negative).</p>
</section>
<section id="sec:orga500da0" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="sec:orga500da0"><span class="header-section-number">3.9</span> LPM and LFPM Metric Elicitation Algorithms</h2>
<p>Now that we have established all of this background, we are ready to present the main two results of this section. First, we introduce an algorithm for LPM elicitation, which also forms the backbone for LFPM elicitation.</p>
<section id="sec:orgb6dac4e" class="level3" data-number="3.9.1">
<h3 data-number="3.9.1" class="anchored" data-anchor-id="sec:orgb6dac4e"><span class="header-section-number">3.9.1</span> LPM Elicitation</h3>
<p>For LPM elicitation, we need one more proposition.</p>
<div class="prop">
<p>For a metric <span class="math inline">\(\psi\)</span> (quasiconvex and monotone increasing in TP/TN) or <span class="math inline">\(\phi\)</span> (quasiconcave and monotone increasing), and parametrization <span class="math inline">\(\rho^+\)</span>/<span class="math inline">\(\rho^-\)</span> of upper/lower boundary, composition <span class="math inline">\(\psi \circ \rho^-\)</span> is quasiconvex and unimodal on [0, 1], and <span class="math inline">\(\phi \circ \rho^+\)</span> is quasiconcave and unimodal on [0, 1].</p>
</div>
<p>Quasiconcavity and quasiconvexity are slightly more general variations on concavity and convexity. Their main useful property in our setting is that they are unimodal (they have a singular extremum), so we can devise a binary-search-style algorithm for eliciting the Bayes optimal and inverse-optimal confusion matrices for a given setting, as well as the corresponding <span class="math inline">\(\phi\)</span>’s.</p>
<p>We first note that to maximize a quasiconcave metric, in which <span class="math inline">\(\phi\)</span> is monotonically increasing in <span class="math inline">\(TP\)</span> and <span class="math inline">\(TN\)</span>, we note that the resulting maximizer (and supporting hyperplane) will occur on the upper boundary of <span class="math inline">\(\mathcal{C}\)</span>. We thus set our initial search range to be <span class="math inline">\([0, \pi/2]\)</span> and repeatedly divide it into four regions. Then, we calculate the resulting CM on the 5 resulting boundaries of these regions and query the oracle <span class="math inline">\(4\)</span> times. We repeat this in each iteration of the binary search until a maximizer is found.</p>
<div class="rem">
<p>In the case of quasiconcave and quasiconvex search ranges, a slightly more sophisticated variation on typical binary search must be used. To illustrate this, we provide the following example.</p>
<figure id="bsearch" class="figure">
<figure class="figure">
<img src="Figures/normaldistribution.png" class="figure-img">
</figure>
<figure class="figure">
<img src="Figures/normaldistribution copy.png" class="figure-img">
</figure>
<figcaption>
Binary search algorithm example for quasiconvex functions
</figcaption>
</figure>
<p>Consider the two distributions in Figure <a href="#bsearch" data-reference-type="ref" data-reference="bsearch">1.13</a>. Note that if, for both the symmetric and skewed distributions, we were to divide the search range into two portions and compare <span class="math inline">\(A\)</span>, <span class="math inline">\(C\)</span>, and <span class="math inline">\(E\)</span>, we would find that <span class="math inline">\(C&gt;A\)</span> and <span class="math inline">\(C&gt;E\)</span>. In both of these cases, this does not help us reduce our search range, since the true maximum could lie on either of the two intervals (as in the second case), or at <span class="math inline">\(C\)</span> itself (as in the first case). Therefore, we must make comparisons between all five points <span class="math inline">\(A, B, C, D, E\)</span>. This allows us to correctly restrict our search range to <span class="math inline">\([B, D]\)</span> in the first case and <span class="math inline">\([C, E]\)</span> in the second. These extra search requirements are due to the quasiconcavity of the search space we are considering, in which there exists a maximum but we need to make several comparisons at various points throughout the search space to be able to reduce its size in each iteration.</p>
</div>
<div class="algorithm">
<div class="algorithmic">
<p><strong>input:</strong> <span class="math inline">\(\epsilon &gt; 0\)</span> and oracle <span class="math inline">\(\Omega\)</span> <strong>initialize:</strong> <span class="math inline">\(\theta_a = 0, \theta_b = \frac{\pi}{2}\)</span> set <span class="math inline">\(\theta_c = \frac{3\theta_a+\theta_b}{4}\)</span>, <span class="math inline">\(\theta_d = \frac{\theta_a+\theta_b}{2}\)</span>, and <span class="math inline">\(\theta_e = \frac{\theta_a+3\theta_b}{4}\)</span></p>
<p>obtain <span class="math inline">\(h\theta_a, h\theta_c, h\theta_d, h\theta_e, h\theta_b\)</span> using Proposition 1</p>
<p>Compute <span class="math inline">\(C\theta_a, C\theta_c, C\theta_d, C\theta_e, C\theta_b\)</span> using (1)</p>
<p>Query <span class="math inline">\(\Omega(C\theta_c, C\theta_a), \Omega(C\theta_d, C\theta_c), \Omega(C\theta_e, C\theta_d)\)</span>, and <span class="math inline">\(\Omega(C\theta_b, C\theta_e)\)</span></p>
<p>request <span class="math inline">\(q_{i,j}\)</span>’s label from reference impute <span class="math inline">\(q_{i,j}\)</span>’s label from previously labeled queries</p>
<p>assume the default order <span class="math inline">\(C\theta \prec C\theta' \prec C\theta''\)</span></p>
<p>assume the default order <span class="math inline">\(C\theta \prec C\theta' \prec C\theta''\)</span></p>
<p>Set <span class="math inline">\(\theta_b = \theta_d\)</span> Set <span class="math inline">\(\theta_b = \theta_d\)</span> Set <span class="math inline">\(\theta_a = \theta_c\)</span> Set <span class="math inline">\(\theta_b = \theta_e\)</span> Set <span class="math inline">\(\theta_a = \theta_d\)</span> Set <span class="math inline">\(\theta_a = \theta_d\)</span> <strong>output:</strong> <span class="math inline">\(\vec{m}, C\)</span>, and <span class="math inline">\(\vec{l}\)</span>, where <span class="math inline">\(\vec{m} = m_l(\theta_d), C = C\theta_d\)</span>, and <span class="math inline">\(\vec{l} := (\vec{m}, (tp, tn)) = (\vec{m}, C)\)</span></p>
</div>
</div>
<p>To elicit LPMs, we run Algorithm 1 <a href="#lpm" data-reference-type="ref" data-reference="lpm">[lpm]</a>, querying the oracle in each iteration, and set the elicited metric <span class="math inline">\(\hat{m}\)</span> (which is the maximizer on <span class="math inline">\(\mathcal{C}\)</span>) to be the slope of the resulting hyperplane, since the metric is linear.</p>
<div class="rem">
<p>To find the minimum of a quasiconvex metric, we flip all instances of <span class="math inline">\(\prec\)</span> and <span class="math inline">\(\succ\)</span>, and use an initial search range of <span class="math inline">\([\pi, 3\pi/2]\)</span>; we use this algorithm, which we refer to as Algorithm 2, in our elicitation of LFPMs.</p>
</div>
<p>Next, we provide a Python implementation of Algorithm 1.</p>
<div class="python">
<p>def get_m(theta): """ Inputs: - theta: the value that parametrizes m Outputs: - m_0 and m_1 for the LPM """</p>
<p>return (math.cos(theta), math.sin(theta))</p>
<p>def lpm_elicitation(epsilon, oracle): """ Inputs: - epsilon: some epsilon &gt; 0 representing threshold of error - oracle: some function that accepts 2 confusion matrices and returns true if the first is preferred and false otherwise Outputs: - estimate for m, which is used to compute the LPM as described above """</p>
<p>a = 0 b = math.pi/2 while (b - a &gt; epsilon): c = (3 * a + b) / 4 d = (a + b) / 2 e = (a + 3 * b) / 4</p>
<p>m_a, m_b, m_c, m_d, m_e = (get_m(x) for x in [a,b,c,d,e]) # using definition of m c_a, c_b, c_c, c_d, c_e = (get_c(x) for x in [m_a, m_b, m_c, m_d, m_e]) # compute classifier from m’s then calculate confusion matrices</p>
<p>response_ac = oracle(c_a, c_c) response_cd = oracle(c_c, c_d) response_de = oracle(c_d, c_e) response_eb = oracle(c_e, c_b)</p>
<p># update ranges to keep the peak if response_ac: b = d elif response_cd: b = d elif response_de: a = c b = e elif response_eb: a = d else: a = d return get_m(d), get_c(d)</p>
</div>
</section>
<section id="sec:orga500da1" class="level3" data-number="3.9.2">
<h3 data-number="3.9.2" class="anchored" data-anchor-id="sec:orga500da1"><span class="header-section-number">3.9.2</span> LFPM Elicitation</h3>
<p>Now, we present the next main result, which is an algorithm to elicit linear-fractional performance metrics. For this task, we will need the following assumption:</p>
<p>Let <span class="math inline">\(\phi \in \varphi_{L F P M}\)</span>. We assume <span class="math inline">\(p_{11}, p_{00} \geq 0, p_{11} \geq q_{11}, p_{00} \geq q_{00},\)</span> <span class="math inline">\(p_{0}=0, q_{0}=\)</span> <span class="math inline">\(\left(p_{11}-q_{11}\right) \zeta+\left(p_{00}-q_{00}\right)(1-\zeta)\)</span>, and <span class="math inline">\(p_{11}+p_{00}=1\)</span>.</p>
<p>These assumptions guarantee that the LFPM <span class="math inline">\(\phi\)</span> which we are trying to elicit is monotonically increasing in <span class="math inline">\(TP\)</span> and <span class="math inline">\(TN\)</span>, just as in the LPM elicitation case.</p>
<p>We first provide motivation and an overview of the approach for LFPM elicitation and then present pseudocode for the algorithm.</p>
<p>The general idea of the algorithm is to use Algorithm 1 to obtain a maximizer and a minimizer for the given dataset; these result in two systems of equations involving the true LFPM <span class="math inline">\(\phi^*\)</span> with 1 degree of freedom. Then, we run a grid search that is independent of oracle queries to find the point where solutions to the systems match pointwise on the resulting confusion matrices; this occurs close to where the true metric lies.</p>
<p>More formally, suppose that the true metric is <span class="math display">\[\phi^{*}(C)=\frac{p_{11}^{*} T P+p_{00}^{*} T N}{q_{11}^{*} T P+q_{00}^{*} T N+q_{0}^{*}}.\]</span> Then, let <span class="math inline">\(\bar{\tau}\)</span> and <span class="math inline">\(\underline{\tau}\)</span> represent the maximizer and minimizer of <span class="math inline">\(\phi\)</span> over <span class="math inline">\(\mathcal{C}\)</span>, respectively. There exists a hyperplane <span class="math display">\[\begin{aligned}
\bar{\ell}_{f}^{*}:=\left(p_{11}^{*}-\bar{\tau}^{*} q_{11}^{*}\right) t p+\left(p_{00}^{*}-\bar{\tau}^{*} q_{00}^{*}\right) t n=\bar{\tau}^{*} q_{0}^{*},
\end{aligned}\]</span> which touches <span class="math inline">\(\mathcal{C}\)</span> at <span class="math inline">\(\left(\overline{T P}^{*}, \overline{T N}^{*}\right)\)</span> on <span class="math inline">\(\partial \mathcal{C}_{+}\)</span>.</p>
<p>Correspondingly, there also exists a hyperplane <span class="math display">\[\begin{aligned}
\underline{\ell}_{f}^{*}:=\left(p_{11}^{*}-\underline{\tau}^{*} q_{11}^{*}\right) t p+\left(p_{00}^{*}-\underline{\tau}^{*} q_{00}^{*}\right) \operatorname{tn}=\underline{\tau}^{*} q_{0}^{*},
\end{aligned}\]</span> which touches <span class="math inline">\(\mathcal{C}\)</span> at <span class="math inline">\(\left(\underline{TP}^{*}, \underline{T N}^{*}\right)\)</span> on <span class="math inline">\(\partial \mathcal{C}_{-}\)</span>. Figure <a href="#minmax" data-reference-type="ref" data-reference="minmax">1.14</a> illustrates this visually on <span class="math inline">\(\mathcal{C}\)</span>.</p>
<div id="minmax" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/Screenshot 2023-11-13 at 6.56.52 PM.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Visual representation of the minimizer and maximizer on <span class="math inline">\(\mathcal{C}\)</span></figcaption>
</figure>
</div>
<p>While we are unable to obtain 1.1 and 1.2 directly, we can use Algorithm 1 to get a hyperplane <span class="math display">\[(]\bar{\ell}:=\bar{m}_{11} t p+\bar{m}_{00} t n= \bar{m}_{11} \overline{T P}^{*}+\bar{m}_{00} \overline{T N}^{*} = \bar{C}_{0},\]</span> which is equivalent to <span class="math inline">\(\bar{\ell}_{f}^{*}\)</span> (1.1) up to a constant multiple. From here, we can obtain the system of equations</p>
<p><span class="math display">\[p_{11}^{*}-\bar{\tau}^{*} q_{11}^{*}=\alpha \bar{m}_{11}, p_{00}^{*}-\bar{\tau}^{*} q_{00}^{*}=\alpha \bar{m}_{00}, \bar{\tau}^{*} q_{0}^{*}=\alpha \bar{C}_{0},\]</span> where <span class="math inline">\(\alpha &gt; 0\)</span> (we know it is <span class="math inline">\(\geq0\)</span> due to our assumptions earlier and because <span class="math inline">\(\bar{m}\)</span> is positive, but if it is equal to <span class="math inline">\(0\)</span> then <span class="math inline">\(\phi^*\)</span> would be constant. So, our resulting system of equations is <span class="math display">\[\begin{aligned}
    p_{11}^{\prime}-\bar{\tau}^{*} q_{11}^{\prime}=\bar{m}_{11}, p_{00}^{\prime}-\bar{\tau}^{*} q_{00}^{\prime}=\bar{m}_{00}, \bar{\tau}^{*} q_{0}^{\prime}=\bar{C}_{0}.
\end{aligned}\]</span></p>
<p>Now, similarly, we can approximate 1.2 using the algorithm we defined for quasiconvex metrics (Algorithm 2), where we altered the search range and comparisons. After finding the minimizer, we obtain the hyperplane <span class="math display">\[\underline{\ell}:=\underline{m}_{11} t p+\underline{m}_{00} t n=\underline{m}_{11} \underline{TP}^{*}+\underline{m}_{00} \underline{TN}^{*} = \underline{C}_{0},\]</span> which is equivalent to <span class="math inline">\(\underline{\ell}_{f}^{*}\)</span> (1.2) up to a constant multiple. So then, our system of equations is <span class="math display">\[p_{11}^{*}-\underline{\tau}^{*} q_{11}^{*}=\gamma \underline{m}_{11}, p_{00}^{*}-\underline{\tau}^{*} q_{00}^{*}=\gamma \underline{m}_{00}, \underline{\tau}^{*} q_{0}^{*}=\gamma \underline{C}_{0},\]</span> where <span class="math inline">\(\gamma &lt;0\)</span> (for a reason analogous to why we have <span class="math inline">\(\alpha &gt;0\)</span>), meaning our resulting system of equations is <span class="math display">\[\begin{aligned}
    p_{11}^{\prime \prime}-\underline{\tau}^{*} q_{11}^{\prime \prime}=\underline{m}_{11}, p_{00}^{\prime \prime}-\underline{\tau}^{*} q_{00}^{\prime \prime}=\underline{m}_{00}, \underline{\tau}^{*} q_{0}^{\prime \prime}=\underline{C}_{0}.
\end{aligned}\]</span></p>
<p>1.3 and 1.4 form the two systems of equations mentioned in our overview of the algorithm. Next, we demonstrate that they have only one degree of freedom. Note that if we know <span class="math inline">\(p_{11}'\)</span>, we could solve both systems of equations as follows: <span class="math display">\[\begin{aligned}
    p_{00}^{\prime}  &amp;=1-p_{11}^{\prime}, q_{0}^{\prime}=\bar{C}_{0} \frac{P^{\prime}}{Q^{\prime}}\\
    q_{11}^{\prime}  &amp;=\left(p_{11}^{\prime}-\bar{m}_{11}\right) \frac{P^{\prime}}{Q^{\prime}} \\
    q_{00}^{\prime}&amp;=\left(p_{00}^{\prime}-\bar{m}_{00}\right) \frac{P^{\prime}}{Q^{\prime}},
\end{aligned}\]</span> where <span class="math inline">\(P^{\prime}=p_{11}^{\prime} \zeta+p_{00}^{\prime}(1-\zeta)\)</span> and <span class="math inline">\(Q^{\prime}=P^{\prime}+\bar{C}_{0}-\)</span> <span class="math inline">\(\bar{m}_{11} \zeta-\bar{m}_{00}(1-\zeta).\)</span></p>
<p>Now, suppose that we were to know <span class="math inline">\(p_{11}'\)</span>. Then, we could use this value to solve both of the systems 1.3 and 1.4, which would give us two metrics <span class="math inline">\(\phi'\)</span> and <span class="math inline">\(\phi''\)</span>, which come from the maximizer and minimizer respectively. Importantly, when <span class="math display">\[p_{11}^{*} / p_{00}^{*}=p_{11}^{\prime} / p_{00}^{\prime}=p_{11}^{\prime \prime} / p_{00}^{\prime \prime},\]</span> then <span class="math inline">\(\phi^{*}(C)=\phi^{\prime}(C) / \alpha=-\phi^{\prime \prime}(C) / \gamma\)</span>. Essentially, when we have found a value of <span class="math inline">\(p_{11}'\)</span> that results in <span class="math inline">\(\phi'\)</span>, <span class="math inline">\(\phi''\)</span> having constant ratios on all points of the boundary of <span class="math inline">\(\mathcal{C}\)</span>, then we can obtain <span class="math inline">\(\phi^*\)</span>, since it is obtainable from <span class="math inline">\(\phi'\)</span> and <span class="math inline">\(\alpha\)</span> (or, alternatively, <span class="math inline">\(\phi''\)</span> and <span class="math inline">\(\gamma\)</span>.)</p>
<p>So, we will grid search for <span class="math inline">\(p_{11}'\)</span> on <span class="math inline">\([0,1]\)</span>. For each point in our search, we will compute <span class="math inline">\(\phi'\)</span> and <span class="math inline">\(\phi''\)</span>. Then, we will generate a number of confusion matrices on the boundaries, and calculate the ratio <span class="math inline">\(\phi'' / \phi'\)</span> on each of them. We will select the value of <span class="math inline">\(p_{11}'\)</span> for which the ratio <span class="math inline">\(\phi'' / \phi'\)</span> is the closest to constant, and use it to compute the elicited metric <span class="math inline">\(\hat{\phi}\)</span>.</p>
<p>The pseudocode for LFPM elicitation is given in Figure <a href="#lfpm" data-reference-type="ref" data-reference="lfpm">1.15</a>.</p>
<figure id="lfpm" class="figure">
<p>
<img src="Figures/Screenshot 2023-11-13 at 6.57.23 PM.png" style="width:70.0%" alt="image" class="figure-img"> <img src="Figures/Screenshot 2023-10-16 at 10.52.44 AM.png" style="width:70.0%" alt="image" class="figure-img">
</p>
<figcaption>
LFPM Elicitation Algorithm
</figcaption>
</figure>
<p>Note that Algorithm 3, the grid search, is independent of oracle queries, rendering it especially ideal for LFPM elicitation since, as a result, our algorithm has the same algorithmic complexity as LPM elicitation.</p>
<p>We provide a Python implementation below.</p>
<div class="python">
<p>def lfpm_elicitation(k, delta): """ Inputs: - k: the number of confusion matrices to evaluate on - delta: the spacing for the grid search Outputs: - p_11’, which will allow us to compute the elicited LFPM """</p>
<p>sigma_opt = np.inf p11_opt = 0 C = compute_confusion_matrices(k) # generates k confusion matrices to evaluate on</p>
<p>for i in range(int(1/delta)): p11 = i * delta phi1 = compute_upper_metric(p11) # solves the first system of equations with p11 phi2 = compute_lower_metric(p11) # solves the second system of equations with p11 utility_1 = [phi1(c) for c in C] #calculate phi for both systems of equations utility_2 = [phi2(c) for c in C]</p>
<p>r = [] for i in range(k): r.append(utility_1[i] / utility_2[i]) sigma = np.std(r)</p>
<p>if(sigma &lt; sigma_opt): sigma_opt = sigma p11_opt = p11 return p11_opt</p>
</div>
<p>In summary, to elicit LFPMs, we utilize a special property of the LPM minimizer and maximizer on <span class="math inline">\(\mathcal{C}\)</span>–namely, that we can use the corresponding supporting hyperplanes to form a system of equations that can be used to approximate <span class="math inline">\(\phi^*\)</span> if one parameter (<span class="math inline">\(p_{11}'\)</span>) is found, and that this parameter can be found using an oracle-independent grid search.</p>
</section>
</section>
<section id="sec:orga500da2" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="sec:orga500da2"><span class="header-section-number">3.10</span> Guarantees</h2>
<p>Importantly, these algorithms can be shown to satisfy some important theoretical guarantees. We provide a formal statement and intuitive interpretation of them here, and their proofs can be found in the appendix of the original paper.</p>
<p>First, we define the oracle noise <span class="math inline">\(\epsilon_{\Omega}\)</span>, which comes from the oracle potentially flipping the comparison output on two confusion matrices that are close enough in utility.</p>
<div class="thm">
<p>Given <span class="math inline">\(\epsilon, \epsilon_{\Omega} \geq 0\)</span> and a metric <span class="math inline">\(\phi\)</span> satisfying our assumptions, Algorithm 1/2 finds an approximate maximizer/minimizer and supporting hyperplane. Also, the value of <span class="math inline">\(\phi\)</span> at that point is within <span class="math inline">\(O\left(\sqrt{\epsilon_{\Omega}}+\epsilon\right)\)</span> of the optimum, and the number of queries is <span class="math inline">\(O\left(\log \frac{1}{\epsilon}\right)\)</span>.</p>
</div>
<div class="thm">
<p>Let <span class="math inline">\(\mathbf{m}^{*}\)</span> be the true performance metric. Given <span class="math inline">\(\epsilon&gt;0, L P M\)</span> elicitation outputs a performance metric <span class="math inline">\(\hat{\mathbf{m}}\)</span>, s.t. <span class="math inline">\(\left\|\mathbf{m}^{*}-\hat{\mathbf{m}}\right\|_{\infty} \leq \sqrt{2} \epsilon+\frac{2}{k_{0}} \sqrt{2 k_{1} \epsilon_{\Omega}}\)</span>.</p>
</div>
<p>These two theorems ensure that Algorithms 1 and 2 find an appropriate maximizer and minimizer in the search space, within a certain range of accuracy that depends on oracle and sample noise, and in a certain number of queries. Both of these statements are guaranteed by the binary search approach.</p>
<div class="thm">
<p>Let <span class="math inline">\(h_{\theta}\)</span> and <span class="math inline">\(\hat{h}_{\theta}\)</span> be two classifiers estimated using <span class="math inline">\(\eta\)</span> and <span class="math inline">\(\hat{\eta}\)</span>, respectively. Further, let <span class="math inline">\(\bar{\theta}\)</span> be such that <span class="math inline">\(h_{\bar{\theta}}=\arg \max _{\theta} \phi\left(h_{\theta}\right)\)</span>. Then <span class="math inline">\(\|C(\hat{h}_{\bar{\theta}})-C\left(h_{\bar{\theta}}\right)\|_{\infty}=O\left(\left\|\hat{\eta}_{n}-\eta\right\|_{\infty}\right)\)</span>.</p>
</div>
<p>This says, importantly, that the drop in elicited metric quality caused by using a dataset of samples rather than population CMs is bounded by the drop in performance of the decision boundary <span class="math inline">\(\eta\)</span>. These three guarantees together ensure that oracle noise and sample noise do not amplify drops in performance when using metric elicitation; rather, these drops in performance are bounded by the drops that would usually occur when using the typical machine learning paradigm of training a decision boundary and using a pre-established metric.</p>
</section>
<section id="summary-and-further-expansions" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="summary-and-further-expansions"><span class="header-section-number">3.11</span> Summary and Further Expansions</h2>
<p>In this chapter, we have introduced the framework of metric elicitation for binary classification. After motivating the problem from a psychological and theoretical perspective, we have introduced the notions of linear performance metrics and linear-fractional performance metrics. Next, we set up the problem of metric elicitation from a population and sample perspective. We then made several key observations about the space of confusion matrices which, along with the notions of the Bayes utility and classifier, allow us to reduce the problem of linear performance metric elicitation to a binary search algorithm over a quasiconvex (or quasiconcave) space.</p>
<p>Next, we presented an algorithm to elicit linear-fractional performance metrics that builds upon the method for eliciting linear performance metrics. We used a key result about the dimensionality of linear-fractional performance metrics to create a simple oracle-independent grid search algorithm, which, in conjunction with the linear performance metric elicitation algorithm, results in an algorithm with the same time complexity that can be used to elicit a much broader range of metrics. We also provided Python implementations for both of these algorithms.</p>
<p>Lastly, we have described three important guarantees about the performance of these algorithms, which makes them suited to real-world, practical settings.</p>
<p>For further interesting exploration of the types of problems that can be solved using the framework of metric elicitation, we refer the reader to <span class="citation" data-cites="nips">(<a href="#ref-nips" role="doc-biblioref">Hiranandani, Narasimhan, and Koyejo 2020</a>)</span>, which performs metric elicitation to determine the oracle’s ideal tradeoff between the classifier’s overall performance and the discrepancy between its performance on certain protected groups.</p>
</section>
<section id="multiclass-performance-metric-elicitation" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="multiclass-performance-metric-elicitation"><span class="header-section-number">3.12</span> Multiclass Performance Metric Elicitation</h2>
<p>Although the previous chapter only described metric elicitation for binary classification problems, the general framework can still be applied to multiclass classification problems, as described in “Multiclass Performance Metric Elicitation” by Hiranandani et al. <span class="citation" data-cites="NEURIPS2019_1fd09c5f">(<a href="#ref-NEURIPS2019_1fd09c5f" role="doc-biblioref">Hiranandani et al. 2019b</a>)</span></p>
<p>Consider the case of classifying subtypes of leukemia <span class="citation" data-cites="YangNaiman+2014+477+496">(<a href="#ref-YangNaiman+2014+477+496" role="doc-biblioref">Yang and Naiman 2014</a>)</span>. We can train a neural network to predict conditional probability of a certain leukemia subtype given certain gene expressions. However, it may not be appropriate to classify the subtype purely based on whichever one has the highest confidence. For instance, a treatment for leukemia subtype C1 may be perfect for cases of C1, but it may be ineffective or harmful for certain other subtypes. Therefore, the final response from the classifier may not be as simple as as choosing the class with the highest conditional probability, just like how the threshold for binary classification may not always be 50%.</p>
<p>With multiclass metric elicitation, we can show confusion matrices to an oracle (like the doctor in the leukemia example) to determine which classifier has the best tradeoffs. In <span class="citation" data-cites="NEURIPS2019_1fd09c5f">(<a href="#ref-NEURIPS2019_1fd09c5f" role="doc-biblioref">Hiranandani et al. 2019b</a>)</span>, the authors focus on eliciting linear performance metrics, which is what we will describe in this chapter.</p>
</section>
<section id="preliminaries" class="level2" data-number="3.13">
<h2 data-number="3.13" class="anchored" data-anchor-id="preliminaries"><span class="header-section-number">3.13</span> Preliminaries</h2>
<p>Most of the notation from Binary Metric Elicitation still persists, just modified to provide categorical responses:</p>
<ul>
<li><p><span class="math inline">\(X \in \mathcal{X}\)</span> is the input random variable.</p></li>
<li><p><span class="math inline">\(Y \in [k]\)</span> is the output random variable, where <span class="math inline">\([k]\)</span> is the index set <span class="math inline">\(\{1, 2, \dots, k\}\)</span>.</p></li>
<li><p>The dataset of size <span class="math inline">\(n\)</span> is denoted by <span class="math inline">\(\{(\vec{x}, y)\}_{i=1}^n\)</span> generated independently and identically from <span class="math inline">\(\mathbb{P}(X, Y)\)</span>.</p></li>
<li><p><span class="math inline">\(\eta_i(\vec{x}) = \mathbb{P}(Y=i | X=\vec{x})\)</span> gives the conditional probability of class <span class="math inline">\(i \in [k]\)</span> given an observation.</p></li>
<li><p><span class="math inline">\(\xi_i = \mathbb{P}(Y=i)\)</span> is the marginal probability of class <span class="math inline">\(i \in [k]\)</span>.</p></li>
<li><p>The set of all classifiers is <span class="math inline">\(\mathcal{H} = \{h : \mathcal{X} \rightarrow \Delta_k\}\)</span>, where <span class="math inline">\(\Delta_k\)</span> is (k-1) dimensional simplex. In this case, the outputs of classifiers are 1-hot vectors of size <span class="math inline">\(k\)</span> where the only index with value 1 is the predicted class and all other positions have a value of 0.</p></li>
<li><p>The confusion matrix for a classifier, <span class="math inline">\(h\)</span>, is <span class="math inline">\(C(h, \mathbb{P}) \in \mathbb{R}^{k \times k}\)</span>, where: $$</p>
<span class="math display">\[\begin{aligned}
        C_{ij}(h, \mathbb{P}) = \mathbb{P}(Y=i, h=j) \text{\qquad for } i, j \in [k]

\end{aligned}\]</span>
<p>$$</p></li>
</ul>
<p>Note that the confusion matrices are <span class="math inline">\(k\times k\)</span> and store the joint probabilities of each type of classification for each possible class. This means that the sum of row <span class="math inline">\(i\)</span> in the confusion matrix equals <span class="math inline">\(\xi_i\)</span>, because this is equivalent to adding over all possible classifications. Since we know the sums of each row, all diagonal elements can be reconstructed from just the off-diagonal elements, so a confusion matrix <span class="math inline">\(C(h, \mathbb{P})\)</span> can be expressed as a vector of off-diagonal elements, <span class="math inline">\(\vec{c}(h, \mathbb{P}) = \textit{off-diag}(C(h, \mathbb{P}))\)</span>, and <span class="math inline">\(\vec{c} \in \mathbb{R}^q\)</span> where <span class="math inline">\(q := k^2 - k\)</span>. The vector <span class="math inline">\(\vec{c}\)</span> is called the vector of <em>‘off-diagonal confusions.’</em> The space of off-diagonal confusions is <span class="math inline">\(\mathcal{C} = \{\vec{c}(h, \mathbb{P}) : h \in \mathcal{H}\}\)</span>.</p>
<p>In cases where the oracle would care about the exact type of misclassification (i.e.&nbsp;misclassifying and object from class 1 as class 2), this off-diagonal confusion matrix is necessary. However, there are many cases where the performance of a classifier is determined by just the probability of correct prediction for each class, which just requires the diagonal elements. In these cases, we can define the vector of <em>‘diagonal confusions’</em> as <span class="math inline">\(\vec{d}(h, \mathbb{P}) = \textit{diag}(C(h, \mathbb{P})) \in \mathbb{R}^k\)</span>. The space of diagonal confusions is <span class="math inline">\(\mathcal{D} = \{\vec{d}(h, \mathbb{P}) : h \in \mathcal{H}\}\)</span>.</p>
<p>Finally, the setup for metric elicitation is identical to the one examined in the previous chapter. We still assume access to an oracle that can choose between two classifiers or confusion matrices, using notation <span class="math inline">\(\Gamma\)</span> for comparing two classifiers and <span class="math inline">\(\Omega\)</span> for comparing confusion matrices, which returns 1 if the first classifier is better and 0 otherwise. We still assume that the oracle behaves according to some unknown performance metric, and we wish to recover this metric up to some small error tolerance (based on a suitable norm).</p>
<p>The two different types of confusion vectors result in different algorithms for metric elicitation, which we will explore in later sections.</p>
</section>
<section id="diagonal-linear-performance-metric-elicitation" class="level2" data-number="3.14">
<h2 data-number="3.14" class="anchored" data-anchor-id="diagonal-linear-performance-metric-elicitation"><span class="header-section-number">3.14</span> Diagonal Linear Performance Metric Elicitation</h2>
<p>In this section, we study metric elicitation in the case where the performance metric is linear and we only care about diagonal confusions.</p>
<section id="dlpm" class="level3" data-number="3.14.1">
<h3 data-number="3.14.1" class="anchored" data-anchor-id="dlpm"><span class="header-section-number">3.14.1</span> DLPM</h3>
<p>A Diagonal Linear Performance Metric (DLPM) is a performance metric that only considers the diagonal elements in the confusion matrix. The metric is defined as <span class="math inline">\(\psi(\vec{d}) = \langle \vec{a}, \vec{d} \rangle\)</span>, where <span class="math inline">\(\vec{a} \in \mathbb{R}^k\)</span> such that <span class="math inline">\(||\vec{a}||_1 = 1\)</span>. It is also called weighted accuracy <span class="citation" data-cites="pmlr-v37-narasimhanb15">(<a href="#ref-pmlr-v37-narasimhanb15" role="doc-biblioref">Narasimhan et al. 2015</a>)</span>.</p>
<p>The family of DLPMs is denoted as <span class="math inline">\(\varphi_{DLPM}\)</span>. Since these only consider the diagonal elements, which we want to maximize, we can focus on only eliciting monotonically increasing DLPMs, meaning that all elements in <span class="math inline">\(\vec{a}\)</span> are non-negative.</p>
</section>
<section id="bayes-optimal-classifiers" class="level3" data-number="3.14.2">
<h3 data-number="3.14.2" class="anchored" data-anchor-id="bayes-optimal-classifiers"><span class="header-section-number">3.14.2</span> Bayes Optimal Classifiers</h3>
<p>The Bayes Optimal diagonal confusion given a metric <span class="math inline">\(\psi\)</span> is <span class="math inline">\(\bar{d} = \arg\max_{\vec{d} \in \mathcal{D}} \psi(\vec{d})\)</span>.</p>
<p>The Restricted Bayes Optimal (RBO) diagonal confusion is the diagonal confusion that maximizes metric <span class="math inline">\(\psi\)</span> given that it is only allowed to predict classes <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>. It is denoted as <span class="math inline">\(\bar{c}_{k_1, k_2} := \arg\max_{\vec{d} \in \mathcal{D}_{k_1, k_2}} \psi(\vec{d})\)</span>.</p>
</section>
<section id="geometry-of-space-of-diagonal-confusions-d" class="level3" data-number="3.14.3">
<h3 data-number="3.14.3" class="anchored" data-anchor-id="geometry-of-space-of-diagonal-confusions-d"><span class="header-section-number">3.14.3</span> Geometry of Space of Diagonal Confusions D</h3>
<p>Consider the trivial classifiers that only predict a single class at all times. The diagonal confusions when only predicting class <span class="math inline">\(i\)</span> are <span class="math inline">\(\vec{v}_i \in \mathbb{R}^k\)</span> with <span class="math inline">\(\xi_i\)</span> at index <span class="math inline">\(i\)</span> and zero elsewhere. Note that this is the maximum possible value in index <span class="math inline">\(i\)</span>, because this represents perfectly classifying all points that have a true class of <span class="math inline">\(i\)</span>.</p>
<p>We can consider the space of diagonal confusions, visualized in Figure <a href="#diag_geom" data-reference-type="ref" data-reference="diag_geom">1.16</a> (taken from <span class="citation" data-cites="NEURIPS2019_1fd09c5f">(<a href="#ref-NEURIPS2019_1fd09c5f" role="doc-biblioref">Hiranandani et al. 2019b</a>)</span>). The space of <span class="math inline">\(\mathcal{D}\)</span> is strictly convex, closed, and contained in the box <span class="math inline">\([0, \xi_1] \times \dots \times [0, \xi_k]\)</span>. We also know that the only vertices are <span class="math inline">\(\vec{v}_i\)</span> for each <span class="math inline">\(i \in [k]^{(k-1)}\)</span>.</p>
<div id="diag_geom" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/diag_geometry.png" class="img-fluid figure-img"></p>
<figcaption>(a) Geometry of space of diagonal confusions for <span class="math inline">\(k=3\)</span>. This is a convex region with three flat areas representing confusions when restricted to only two classes. (b) Geometry of diagonal confusions when restricted to classes <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>. Notice how this is identical to the space of confusion matrices examined in the previous chapter.</figcaption>
</figure>
</div>
<p>We know that this is strictly convex under the assumption that an object from any class can be misclassified as any other class. Mathematically, the assumption is that <span class="math inline">\(g_{ij}(r) = \mathbb{P} \left[\frac{\eta_i(X)}{\eta_j(X)} \geq r \right]\)</span> <span class="math inline">\(\forall i, j \in [k]\)</span> are continuous and strictly decreasing for <span class="math inline">\(r \in [0, \infty)\)</span>.</p>
<p>We can also define the space of binary classification confusion matrices confined to classes <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>, which is the 2-D <span class="math inline">\((k_1, k_2)\)</span> axis-aligned face of <span class="math inline">\(\mathcal{D}\)</span>, denoted as <span class="math inline">\(\mathcal{D}_{k_1, k_2}\)</span>. Note that this is strictly convex, since <span class="math inline">\(\mathcal{D}\)</span> itself is strictly convex, and it has the same geometry as the space of binary confusion matrices examined in the previous chapter. Therefore, we can construct an RBO classifier for <span class="math inline">\(\psi \in \varphi_{DLPM}\)</span>, parameterized by <span class="math inline">\(\vec{a}\)</span>, as follows: <span class="math display">\[\begin{aligned}
\bar{h}_{k_1, k_2}(\vec{x})= \left\{
\begin{array}{ll}
      k_1, \text{ if } a_{k_1} \eta_{k_1}(\vec{x}) \geq a_{k_2} \eta_{k_2}(\vec{x})\\
k_2, \text{ o.w.}
\end{array}
\right\}.
\label{rbo_eq}
\end{aligned}\]</span></p>
<p>We can parameterize the upper boundary of <span class="math inline">\(\mathcal{D}_{k_1, k_2}\)</span>, denoted as <span class="math inline">\(\partial \mathcal{D}^{+}_{k_1, k_2}\)</span>, using a single parameter <span class="math inline">\(m \in [0, 1]\)</span>. Specifically, we can construct a DLPM by setting <span class="math inline">\(a_{k_1} = m\)</span>, <span class="math inline">\(a_{k_2} = 1 - m\)</span>, and all others to 0. Using equation <a href="#rbo_eq" data-reference-type="ref" data-reference="rbo_eq">[rbo_eq]</a>, we can get the diagonal confusions, so varying <span class="math inline">\(m\)</span> parameterizes <span class="math inline">\(\partial \mathcal{D}^{+}_{k_1, k_2}\)</span>. The parameterization is denoted as <span class="math inline">\(\nu(m; k_1, k_2)\)</span>.</p>
</section>
<section id="dlpm-elicitation" class="level3" data-number="3.14.4">
<h3 data-number="3.14.4" class="anchored" data-anchor-id="dlpm-elicitation"><span class="header-section-number">3.14.4</span> DLPM Elicitation</h3>
<p>Suppose the oracle follows a true metric, <span class="math inline">\(\psi\)</span>, that is linear and monotone increasing across all axes. If we consider the composition <span class="math inline">\(\psi \circ \nu(m; k_1, k_2): [0, 1] \rightarrow \mathbb{R}\)</span>, we know it must be concave and unimodal, because <span class="math inline">\(\mathcal{D}_{k_1, k_2}\)</span> is a convex set. Therefore, we can find the value of <span class="math inline">\(m\)</span> that maximizes <span class="math inline">\(\psi \circ \nu(m; k_1, k_2)\)</span> for any given <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> using a binary search procedure.</p>
<p>Since the RBO classifier for classes <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> only rely on the relative weights of the classes in the DLPM (see equation <a href="#rbo_eq" data-reference-type="ref" data-reference="rbo_eq">[rbo_eq]</a>), finding the value of <span class="math inline">\(m\)</span> that maximizes <span class="math inline">\(\psi \circ \nu(m; k_1, k_2)\)</span> gives us the true relative ratio between <span class="math inline">\(a_{k_1}\)</span> and <span class="math inline">\(a_{k_2}\)</span>. Specifically, from the definition of <span class="math inline">\(\nu\)</span>, we know that <span class="math inline">\(\frac{a_{k_2}}{a_{k_1}} = \frac{1-m}{m}\)</span>. We can therefore simply calculate the ratio between <span class="math inline">\(a_1\)</span> and all other weights to reconstruct an estimate for the true metric. A python implementation of this algorithm is provided below.</p>
<div class="python">
<p>import numpy as np</p>
<p>def rbo_dlpm(m, k1, k2, k): """ This constructs DLPM weights for the upper boundary of the restricted diagonal confusions, given a parameter m. This is equivalent to (m; k1, k2)</p>
<p>Inputs: - m: parameter (between 0 and 1) for the upper boundary - k1: first axis for this face - k2: second axis for this face - k: number of classes Outputs: - DLPM weights for this point on the upper boundary """ new_a = np.zeros(k) new_a[k1] = m new_a[k2] = 1 - m return new_a</p>
<p>def dlpm_elicitation(epsilon, oracle, get_d, k): """ Inputs: - epsilon: some epsilon &gt; 0 representing threshold of error - oracle: some function that accepts 2 confusion matrices and returns true if the first is preferred and false otherwise - get_d: some function that accepts dlpm weights and returns diagonal confusions - k: number of classes Outputs: - estimate for true DLPM weights """ a_hat = np.zeros(k) a_hat[0] = 1 for i in range(1, k): # iterate over each axis to find appropriate ratio a = 0 # lower bound of binary search b = 1 # upper bound of binary search</p>
<p>while (b - a &gt; epsilon): c = (3 * a + b) / 4 d = (a + b) / 2 e = (a + 3 * b) / 4</p>
<p># get diagonal confusions for each point d_a, d_c, d_d, d_e, d_b = (get_d(rbo_dlpm(x, 0, i, k)) for x in [a, c, d, e, b])</p>
<p># query oracle for each pair response_ac = oracle(d_a, d_c) response_cd = oracle(d_c, d_d) response_de = oracle(d_d, d_e) response_eb = oracle(d_e, d_b)</p>
<p># update ranges to keep the peak if response_ac: b = d elif response_cd: b = d elif response_de: a = c b = e elif response_eb: a = d else: a = d</p>
<p>midpt = (a + b) / 2 a_hat[i] = (1 - midpt) / midpt return a_hat / np.sum(a_hat)</p>
</div>
<p>To use this algorithm for metric elicitation on a real dataset, we need to supply the “oracle” and “get_d” functions. The oracle function is an interface to an expert who judges which of two confusion matrices is better. The get_d function will need to construct a classifier given the DLPM weights, following the principles of the RBO classifier from equation <a href="#rbo_eq" data-reference-type="ref" data-reference="rbo_eq">[rbo_eq]</a>, and calculate the confusion matrix from a validation set.</p>
</section>
<section id="dlpm-elicitation-guarantees" class="level3" data-number="3.14.5">
<h3 data-number="3.14.5" class="anchored" data-anchor-id="dlpm-elicitation-guarantees"><span class="header-section-number">3.14.5</span> DLPM Elicitation Guarantees</h3>
<p>Using the same oracle feedback noise model from the binary metric elicitation, we can make the following guarantees:</p>
<div class="prop">
<p>Given <span class="math inline">\(\epsilon, \epsilon_\Omega \geq 0\)</span>, and a 1-Lipschitz DLPM <span class="math inline">\(\varphi^*\)</span> parameterized by <span class="math inline">\(\vec{a}^*\)</span>. Then the output <span class="math inline">\(\hat{a}\)</span> of the DLPM elicitation algorithm after <span class="math inline">\(O((k-1)\log\frac{1}{\epsilon})\)</span> queries to the oracle satisfies <span class="math inline">\(||\vec{a}^* - \hat{a}||_\infty \leq O(\epsilon + \sqrt{\epsilon_\Omega})\)</span>, which is equivalent to <span class="math inline">\(||\vec{a}^* - \hat{a}||_2 \leq O(\sqrt{k}(\epsilon + \sqrt{\epsilon_\Omega}))\)</span>.</p>
</div>
<p>In other words, the maximum difference between the estimate and true value along any component (indicated by the L-infinity norm) is linearly bounded by the sum of the epsilon specified by the algorithm and the square root of the oracle’s correctness guarantee (<span class="math inline">\(\epsilon_\Omega\)</span>).</p>
</section>
</section>
<section id="linear-performance-metric-elicitation" class="level2" data-number="3.15">
<h2 data-number="3.15" class="anchored" data-anchor-id="linear-performance-metric-elicitation"><span class="header-section-number">3.15</span> Linear Performance Metric Elicitation</h2>
<p>In this section, we study metric elicitation in the case where performance metric is linear and we care about the entire confusion matrix (i.e.&nbsp;the off-diagonal confusions).</p>
<p>A Linear Performance Metric (LPM) is a performance metric that uses the off-diagonal confusions, which is equivalent to the whole confusion matrix. The metric is defined as <span class="math inline">\(\phi(\vec{c}) = \langle \vec{a}, \vec{c} \rangle\)</span>, where <span class="math inline">\(\vec{a} \in \mathbb{R}^q\)</span> such that <span class="math inline">\(||\vec{a}||_2 = 1\)</span>. As a reminder, <span class="math inline">\(q = k^2 - k\)</span>, which is the number of off-diagonal elements of the confusion matrix. The family of LPMs is denoted as <span class="math inline">\(\varphi_{LPM}\)</span>. Since these only consider the off-diagonal elements, which we want to minimize, we can focus on only eliciting monotonically decreasing LPMs, meaning all elements in <span class="math inline">\(\vec{a}\)</span> are non-positive. The Bayes Optimal confusion given a metric <span class="math inline">\(\phi\)</span> over a subset <span class="math inline">\(\mathcal{S} \subseteq \mathcal{C}\)</span> is <span class="math inline">\(\bar{c} = \arg\max_{\vec{c} \in \mathcal{S}} \phi(\vec{c})\)</span>.</p>
<section id="geometry-of-space-of-off-diagonal-confusions-c" class="level3" data-number="3.15.1">
<h3 data-number="3.15.1" class="anchored" data-anchor-id="geometry-of-space-of-off-diagonal-confusions-c"><span class="header-section-number">3.15.1</span> Geometry of Space of Off-Diagonal Confusions C</h3>
<p>Consider the trivial classifiers that only predict a single class at all times. The off-diagonal confusions when only predicting class <span class="math inline">\(i\)</span> are <span class="math inline">\(\vec{u}_i \in \mathbb{R}^q\)</span>, and all <span class="math inline">\(\xi_j\)</span> (where <span class="math inline">\(j \neq i\)</span>) will appear in this vector, because all classes other than <span class="math inline">\(i\)</span> will be incorrectly predicted.</p>
<p>The space of off-diagonal confusions, <span class="math inline">\(\mathcal{C}\)</span> is convex and contained in the box <span class="math inline">\([0, \xi_1]^{(k-1)} \times \dots \times [0, \xi_k]^{(k-1)}\)</span>. We also know that each <span class="math inline">\(\vec{u}_i\)</span> is a vertex. Furthermore, <span class="math inline">\(\vec{o} = \frac{1}{k} \sum_{i=1}^k \vec{u}_i\)</span> is always contained in <span class="math inline">\(\mathcal{C}\)</span> and represents the confusion matrix for the classifier that chooses a class uniformly at random.</p>
<p>Notice that <span class="math inline">\(\mathcal{C}\)</span> is not <em>strictly</em> convex, unlike <span class="math inline">\(\mathcal{D}\)</span>. However, if we make the assumption that there exists a <span class="math inline">\(q\)</span>-dimensional sphere <span class="math inline">\(\mathcal{S}_\lambda \subset \mathcal{C}\)</span> of radius <span class="math inline">\(\lambda &gt; 0\)</span> centered at <span class="math inline">\(\vec{o}\)</span>, we can now operate on this convex space. Note that this sphere always exists as long as there is some signal for non-trivial classification, so this assumption is safe in practice. Furthermore, since we are eliciting some linear metric with weights <span class="math inline">\(\vec{a}\)</span>, we know the optimal off-diagonal confusion <span class="math inline">\(\bar{c}\)</span> over <span class="math inline">\(\mathcal{S}\)</span> is a point on its boundary <span class="math inline">\(\bar{c} = \lambda \vec{a} + \vec{o}\)</span>.</p>
<p>Since we are only eliciting monotonically decreasing LPMs, we can parameterize the lower boundary of <span class="math inline">\(\mathcal{S}_\lambda\)</span>, denoted as <span class="math inline">\(\partial S^{-}_\lambda\)</span>, using sphere angles. Specifically, let <span class="math inline">\(\vec{\theta}\)</span> be a (q-1) dimensional vector of angles, with all angles in the second quadrant (<span class="math inline">\(\theta_i \in [\pi/2, \pi]\)</span>), except for the primary angle, which is in the third quadrant (<span class="math inline">\(\theta_{(q-1)} \in [\pi, 3\pi/2]\)</span>. To construct the LPM, we can set <span class="math inline">\(a_i = \prod_{j=1}^{i-1} \sin(\theta_j) \cos(\theta_i)\)</span> for <span class="math inline">\(i \in [q-1]\)</span> while <span class="math inline">\(a_q =  \prod_{j=1}^{q-1} \sin(\theta_j)\)</span>. This parameterization ensures that all elements in <span class="math inline">\(\vec{a}\)</span> are non-positive and the 2-norm is 1. This parameterization is denoted as <span class="math inline">\(\mu(\vec{\theta})\)</span>.</p>
</section>
<section id="lpm-elicitation" class="level3" data-number="3.15.2">
<h3 data-number="3.15.2" class="anchored" data-anchor-id="lpm-elicitation"><span class="header-section-number">3.15.2</span> LPM Elicitation</h3>
<p>Unlike the diagonal case, <span class="math inline">\(\mathcal{C}\)</span> is not <em>strictly</em> convex, so the boundary may have flat regions. This means the DLPM elicitation algorithm cannot be used in this case. Instead, we can use the query space of the sphere <span class="math inline">\(\mathcal{S}_\lambda\)</span> to find an optimal point, allowing us to recover the metric.</p>
<p>Drawing from Derivative-Free Optimization (DFO) <span class="citation" data-cites="NIPS2012_e6d8545d">(<a href="#ref-NIPS2012_e6d8545d" role="doc-biblioref">Jamieson, Nowak, and Recht 2012</a>)</span>, we can perform binary search on one coordinate at a time and keep updating cyclically. This algorithm will converge, because the metric is unimodal (over the lower boundary of the sphere) so progress is guaranteed.</p>
<p>A python implementation of this algorithm is provided below.</p>
<div class="python">
<p>import numpy as np</p>
<p>def get_lpm(theta): """ This constructs LPM weights for the lower boundary of the sphere.</p>
<p>Inputs: - theta: parameter for the lower boundary Outputs: - LPM weights """ new_a = np.ones(theta.size + 1) sin_theta = np.sin(theta) new_a[1:] = sin_theta new_a[:-1] *= np.cos(theta) return new_a</p>
<p>def get_mod_lpm(theta, j, new_val): """ This constructs LPM weights where index j of the parameter is set to new_val """ new_theta = np.copy(theta) new_theta[j] = new_val return get_lpm(new_theta)</p>
<p>def lpm_elicitation(epsilon, oracle, get_c, k, T): """ Inputs: - epsilon: some epsilon &gt; 0 representing threshold of error - oracle: some function that accepts 2 confusion matrices and returns true if the first is preferred and false otherwise - get_c: some function that accepts lpm weights and returns off-diagonal confusions - k: number of classes - T: maximum number of iterations Outputs: - estimate for true DLPM weights """ q = k * k - k theta = np.random.rand(q-1) theta[:-1] = (1 + theta[:-1]) * np.pi/2 # initialize params to [pi/2, pi] theta[:-1] = (2 + theta[:-1]) * np.pi/2 # last param in [pi, 3pi/2]</p>
<p>for t in range(T): j = t</p>
<p># initialize range of binary search if j == q-2: a = np.pi b = 3 * np.pi / 2 else: a = np.pi / 2 b = np.pi</p>
<p>while (b - a &gt; epsilon): c = (3 * a + b) / 4 d = (a + b) / 2 e = (a + 3 * b) / 4</p>
<p># get confusions for each point c_a, c_c, c_d, c_e, c_b = (get_c(get_mod_lpm(theta, j, x)) for x in [a, c, d, e, b])</p>
<p># query oracle for each pair response_ac = oracle(c_a, c_c) response_cd = oracle(c_c, c_d) response_de = oracle(c_d, c_e) response_eb = oracle(c_e, c_b)</p>
<p># update ranges to keep the peak if response_ac: b = d elif response_cd: b = d elif response_de: a = c b = e elif response_eb: a = d else: a = d</p>
<p>midpt = (a + b) / 2 theta[j] = midpt return get_lpm(theta)</p>
</div>
<p>Similar to the DLPM case, we need to supply the “oracle” and “get_c” functions. The oracle function is an interface to an expert who judges which of two confusion matrices is better. The get_c function will need to construct a classifier given the LPM weights, following the fact that <span class="math inline">\(\bar{c} = \lambda \vec{a} + \vec{o}\)</span>, and calculate the confusion matrix from a validation set. Using the same oracle feedback noise model from the binary metric elicitation, we can make the following guarantees:</p>
<div class="prop">
<p>Given <span class="math inline">\(\epsilon, \epsilon_\Omega \geq 0\)</span>, and a 1-Lipschitz LPM <span class="math inline">\(\phi^*\)</span> parameterized by <span class="math inline">\(\vec{a}^*\)</span>. Suppose <span class="math inline">\(\lambda \gg \epsilon_\Omega\)</span>, then the output <span class="math inline">\(\hat{a}\)</span> of the LPM elicitation algorithm after <span class="math inline">\(O(z_1 \log(z_2/(q\epsilon^2))(q-1) log(\frac{\pi}{2\epsilon}))\)</span> queries to the oracle satisfies <span class="math inline">\(||a^* - \hat{a}||_2 \leq O(\sqrt{q}(\epsilon + \sqrt{\epsilon_\Omega/\lambda}))\)</span>, where <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> are constants independent of <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(q\)</span>.</p>
</div>
</section>
</section>
<section id="summary" class="level2" data-number="3.16">
<h2 data-number="3.16" class="anchored" data-anchor-id="summary"><span class="header-section-number">3.16</span> Summary</h2>
<p>In this chapter, we explore multiclass performance metric elicitation, extending metric elicitation beyond binary classification. In practice, if weighted accuracy is all that is needed to determine if one classifier is better than another, the DLPM elicitation algorithm is preferred for its lower query complexity. However, if the type of misclassification is important, like in the leukemia example at the start of this chapter, the LPM elicitation algorithm enables more flexibility.</p>
</section>
<section id="linear-reward-estimation" class="level2" data-number="3.17">
<h2 data-number="3.17" class="anchored" data-anchor-id="linear-reward-estimation"><span class="header-section-number">3.17</span> Linear Reward Estimation</h2>
<p>How exactly do robots learn human preferences from just the pairwise comparisons, if they need to learn how to act in the environment itself? The comparisons in turn help robots learn the reward function of the human, which allows them to further take actions in real settings.</p>
<section id="geometry-of-pairwise-comparisons" class="level3" data-number="3.17.1">
<h3 data-number="3.17.1" class="anchored" data-anchor-id="geometry-of-pairwise-comparisons"><span class="header-section-number">3.17.1</span> Geometry of Pairwise Comparisons</h3>
<p>Let’s say there are two trajectories <span class="math inline">\(\xi_A\)</span> and <span class="math inline">\(\xi_B\)</span> that might be taken as the next course of action in any context, like choosing the next turn, or choosing the next chatGPT response. The robot is offering both to a human for comparison. To answer which of them is better, the human would ask themselves if <span class="math inline">\(R(\xi_A)\)</span> or <span class="math inline">\(R(\xi_B)\)</span> is bigger, with <span class="math inline">\(R(\xi) = w * \phi(\xi)\)</span> being the reward function. In this equation <span class="math inline">\(w\)</span> and <span class="math inline">\(\phi(\xi)\)</span> are vectors of weights and features of the trajectory, so alternatively, we can express this as:</p>
<div class="center">
<p><span class="math inline">\(R(\xi) = \begin{bmatrix} w_1 \\ w_2 \\ ... \\ w_N \end{bmatrix} \cdot \begin{bmatrix} \phi_1(\xi) \\ \phi_2(\xi) \\ ... \\ \phi_N(\xi) \end{bmatrix}\)</span></p>
</div>
<p>If one says that they preferred <span class="math inline">\(\xi_2\)</span> less than <span class="math inline">\(\xi_1\)</span> then it means <span class="math inline">\(\xi_2 &lt; \xi_1 \implies R(\xi_2) &lt; R(\xi_1) \implies w * \phi(\xi_2) &lt; w * \phi(\xi_1) \implies 0 &lt; w * (\phi(\xi_1) - \phi(\xi_2)) \implies 0 &lt; w * \Phi\)</span>. Alternatively, if one preferred <span class="math inline">\(\xi_2\)</span> more than <span class="math inline">\(\xi_1\)</span>, the signs would be flipped, resulting in <span class="math inline">\(0 &gt; w * \Phi\)</span>. The two results can be represented in the N-dimensional space, where when it is split by the decision boundary, it creates half-spaces indicating preferences for each of the sides. For example in <a href="#fig:2dcomp" data-reference-type="ref" data-reference="fig:2dcomp">1.17</a> we can see how a query between two objects can split the plain into two halves, indicating preference towards one of the objects. Such an image can be extended into bigger dimensions, where a line would become a separating hyperplane like in <a href="#fig:2dcomp" data-reference-type="ref" data-reference="fig:2dcomp">1.17</a></p>
<figure id="fig:2dcomp" class="figure">
<p>
<img src="Figures/2D-comp.jpg" style="width:40.0%" alt="image" class="figure-img"> <img src="Figures/3D-comp.png" style="width:40.0%" alt="image" class="figure-img">
</p>
<figcaption>
(left) A single query for a comparison between the two objects splits 2D space into two halves, each of which prefers one of the objects based on feature weights <span class="math inline"><em>w</em><sub>1</sub></span> and <span class="math inline"><em>w</em><sub>2</sub></span>. (right) Extension into 3D space
</figcaption>
</figure>
<p>If one is to truly believe the answers of one person, they would remove everything from the other side of the hyperplane that does not agree with the received human preference. But since humans are noisy, that approach is not optimal, thus most applications up-weight the indicated side of the plane to emphasize that points on that side are better, and down-weight the other side as they do not agree with the provided comparison.</p>
<p>How should someone choose which queries to conduct, otherwise, what is the most informative query sequence? After completing one query, the next query should be orthogonal to the previous one so that the potential space consistent with the preferences decreases in half. The intuition behind that is the potential space has all of the reward functions that agree with the provided answers, so to find a specific reward function for a human, decreasing the space narrows down the possible options. For example, orthogonal query to the query in <a href="#fig:2dcomp" data-reference-type="ref" data-reference="fig:2dcomp">1.17</a> is shown in <a href="#fig:2dspace" data-reference-type="ref" data-reference="fig:2dspace">1.18</a>. The original query created the blue space, and a new one created a red space, resulting in a purple intersection of the two which is still consistent with both of the queries’s results. The image shows that the purple portion is exactly half of the blue portion.</p>
<div id="fig:2dspace" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/2D-space.jpg" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>Creating further comparisons limits the space that agrees with answers to all of them. The blue area demonstrates a preference for object 1 over object 2. The red area demonstrates a preference for object 3 over object 4. Combination (purple area) shows the space that is consistent with both of those preferences.</figcaption>
</figure>
</div>
<p>Mathematically, from <span class="citation" data-cites="pmlr-v87-biyik18a">(<a href="#ref-pmlr-v87-biyik18a" role="doc-biblioref">Biyik and Sadigh 2018</a>)</span> this can be expressed as set <span class="math inline">\(F\)</span> of potential queries <span class="math inline">\(\phi\)</span>, where <span class="math inline">\(F = \{\phi: \phi = \Phi(\xi_A) - \Phi(\xi_B), \xi_A, \xi_B \in \Xi\}\)</span> (defining that a query is the difference between the features of two trajectories). Using that, the authors define a human update function <span class="math inline">\(f_{\phi}(w) = \min(1, \exp(I^T\phi))\)</span> that accounts for how much of the space will still be consistent with the preferences. Finally, for a specific query, they define the minimum volume removed as <span class="math inline">\(\min\{\mathbb{E}[1 - f_{\phi}(w)], \mathbb{E}[1 - f_{-\phi}(w)]\}\)</span> (expected size of the two sides of the remaining space after it is split by a query - purple area in <a href="#fig:2dspace" data-reference-type="ref" data-reference="fig:2dspace">1.18</a>), and the final goal is to maximize that amount over all possible queries since it is optimal to get rid of as much space as possible to narrow down the options for the reward function: <span class="math inline">\(\max_{\phi} \min\{ \mathbb{E}[1 - f_{\phi}(w)], \mathbb{E}[1 - f_{-\phi}(w)]\}\)</span>. Effectively this is finding such <span class="math inline">\(\phi\)</span> that maximizes the information one can get by asking the next comparison query. While this approach uses minimum volume removed, there can be other metrics inside the <span class="math inline">\(\max\)</span> function. Some applications like movie recommendations do not require extra constraints, however in robotics one might want to add more constraints that satisfy certain rules, so that the resulting query follows the dynamics of the physical world.</p>
</section>
<section id="driving-simulator-example" class="level3" data-number="3.17.2">
<h3 data-number="3.17.2" class="anchored" data-anchor-id="driving-simulator-example"><span class="header-section-number">3.17.2</span> Driving Simulator Example</h3>
<p>The first real example of learning reward functions from pairwise comparisons is a 2D driving simulator from <span class="citation" data-cites="pmlr-v87-biyik18a">(<a href="#ref-pmlr-v87-biyik18a" role="doc-biblioref">Biyik and Sadigh 2018</a>)</span>. In <a href="#fig:car_direct" data-reference-type="ref" data-reference="fig:car_direct">1.19</a> you can see the setting of a 3-lane road with the orange car being controlled by the computer.</p>
<div id="fig:car_direct" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/car_dir.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>The choices presented to a human for feedback are represented by green and red trajectories. White trajectory demonstrates the lane change of another vehicle in the space. <span class="citation" data-cites="pmlr-v87-biyik18a">(<a href="#ref-pmlr-v87-biyik18a" role="doc-biblioref">Biyik and Sadigh 2018</a>)</span></figcaption>
</figure>
</div>
<p>The queries conducted for this problem are two different trajectories presented to the human, and they are asked to evaluate which one of them is better. For the features that contribute to the reward function, it is important to consider that robots might not find some of the information as informative for the learning process as a human would. For this example, the underlying features included the distance between lane boundaries, distance to other cars, and the heading and speed of the controlled car. The weights toward the last feature were weighted the highest according to the authors, since it takes a lot of effort for the car to change or correct its direction.</p>
<p>At the start of the learning process, the car had no direction learned and was moving all over the road. In the middle of learning after 30 queries, the simulator learned to follow the direction of the road and go straight but still experienced collisions. After 70 queries, the simulator learned to avoid collisions, as well as keep the car within the lane without swerving.</p>
</section>
<section id="active-learning-for-pairwise-comparisons" class="level3" data-number="3.17.3">
<h3 data-number="3.17.3" class="anchored" data-anchor-id="active-learning-for-pairwise-comparisons"><span class="header-section-number">3.17.3</span> Active Learning for Pairwise Comparisons</h3>
<p>We have discussed that pairwise comparisons should be selected to maximize the minimum volume of remaining options removed. The question that can come out of the driving example is does it really matter to follow that goal or does random choice of queries performs as well? It turns out that indeed most active learning algorithms (purposefully selecting queries) over time converge with the performance of the random query selection, so in long term the performance is similar. However, what is different is that active learning achieves better performance earlier, which in time-sensitive tasks can be a critical factor.</p>
<p>One example of such a setting can be exoskeletons for humans as part of the rehabilitation after surgery <span class="citation" data-cites="Li_2021">(<a href="#ref-Li_2021" role="doc-biblioref">Li et al. 2021</a>)</span>. Different people have significantly different walking patterns as well as rehabilitation requirements, so the exoskeleton needs to adapt to the human as soon as possible for a more successful rehabilitation. Figure <a href="#fig:robotics" data-reference-type="ref" data-reference="fig:robotics">1.20</a> demonstrates the difference in the time needed between the two approaches. In general, in robotics, the time differences that might seem small to a human might be detrimental to the final performance.</p>
<div id="fig:robotics" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/robo_graph.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Performance of active learning and random query selection algorithms in the task of exoskeleton learning with human preferences. <span class="citation" data-cites="Li_2021">(<a href="#ref-Li_2021" role="doc-biblioref">Li et al. 2021</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="multi-modal-reward-functions-for-pairwise-comparisons" class="level3" data-number="3.17.4">
<h3 data-number="3.17.4" class="anchored" data-anchor-id="multi-modal-reward-functions-for-pairwise-comparisons"><span class="header-section-number">3.17.4</span> Multi-Modal Reward Functions for Pairwise Comparisons</h3>
<p>What if one is working with multiple people and their responses to the queries for comparisons? It will be impossible to recover the different personalities based on the answers, and it might be necessary to conduct a full ranking before it is clear which responses belonged to which person, but the underlying theory for the number of comparisons is non-trivial. For that, the researchers <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span> have used multi-modal models for reward function learning, which allows to account for different types of valid behaviours and trajectories that can come from different humans.</p>
<div id="fig:negotiation" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/negotiation.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>The negotiation setting with two people and three shared items. Each person has a desired number of items indicated in their utility box. Alice is the controlled agent that has many different response options that are illustrated by the approaches different models might take. <span class="citation" data-cites="kwon2021targeted">(<a href="#ref-kwon2021targeted" role="doc-biblioref">Kwon et al. 2021</a>)</span></figcaption>
</figure>
</div>
<p>An example setting for such type of problem is negotiations <span class="citation" data-cites="kwon2021targeted">(<a href="#ref-kwon2021targeted" role="doc-biblioref">Kwon et al. 2021</a>)</span>. Let’s say there are some shared items and two people with different utilities and desires for items, where each person only knows their utility. In a specific case of <a href="#fig:negotiation" data-reference-type="ref" data-reference="fig:negotiation">1.21</a>, Bob as a proposing agent and Alice as a controlled agent who has many different ways of responding to Bob’s proposals. Different methods can be used to design Alice as an AI agent. The first idea is reinforcement learning, where multiple rounds of negotiations are done, the model simulates game theory and sees how Bob reacts. Authors of this setting <span class="citation" data-cites="kwon2021targeted">(<a href="#ref-kwon2021targeted" role="doc-biblioref">Kwon et al. 2021</a>)</span> show that over time the model learns to ask for the same thing over and over again, as Alice is not trained to be human-like or negotiable, and just tries to maximize Alice’s utility. The second approach is supervised learning, where the model can be trained on some dataset, learning the history of negotiations. This results in Alice being very agreeable, which demonstrates two polar results of the two approaches, and it would be ideal to find a middle ground and combine both of them. The authors proposed the Targeted acquisition approach, which is based on active learning ideas. The model asks diverse questions at different cases and stages of negotiations like humans, determining which questions are more valuable to be asked throughout learning. Such an approach ended up in more fair and optimal results than supervised or reinforcement learning <span class="citation" data-cites="kwon2021targeted">(<a href="#ref-kwon2021targeted" role="doc-biblioref">Kwon et al. 2021</a>)</span>.</p>
<p>In conclusion, pairwise comparisons show to be a great way of learning linear reward functions, but at times present challenges or incapabilities that can be further improved with additional incorporations of approaches like Active Learning. That improves many applications in terms of time spent getting to the result in case of exoskeleton adjustments, as well as getting to a middle ground between polar behaviors in applications like negotiations.</p>
</section>
</section>
<section id="guiding-human-demonstrations-in-robotics" class="level2" data-number="3.18">
<h2 data-number="3.18" class="anchored" data-anchor-id="guiding-human-demonstrations-in-robotics"><span class="header-section-number">3.18</span> Guiding Human Demonstrations in Robotics</h2>
<p>A strong approach to learning policies for robotic manipulation is imitation learning, the technique of learning behaviors from human demonstrations. In particular, interactive imitation learning allows a group of humans to contribute their own demonstrations for a task, allowing for scalable learning. However, not all groups of demonstrators are equally helpful for interactive imitation learning.</p>
<p>The ideal set of demonstrations for imitation learning would follow a single, optimal method for performing the task, which a robot could learn to mimic. Conversely, <em>multimodality</em>, the presence of multiple optimal methods in the demonstration set, is challenging for imitation learning since it has to learn from contradicting information for how to accomplish a task.</p>
<p>A common reason for multimodality is the fact that different people often subconsciously choose different paths for execution, as illustrated in Figure <a href="#fig:multimodalexecution" data-reference-type="ref" data-reference="fig:multimodalexecution">1.22</a>.</p>
<div id="fig:multimodalexecution" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/multimodal_peg.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Examples of two different ways to insert a nut onto a round peg. The orange demonstration picks up the nut from the hole while the blue demonstration picks up the nut from the side <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span></figcaption>
</figure>
</div>
<p>Gandhi et al. <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span> identifies whether demonstrations are compatible with one another and offer an active elicitation interface to guide humans to provide better demonstrations in interactive imitation learning. Their key motivation is to allow multiple users to contribute demonstrations over the course of data collection by guiding users towards compatible demonstrations.</p>
<p>To identify whether a demonstration is “compatible” with a base policy trained with prior demonstrations, the researchers measure the <em>likelihood</em> of demonstrated actions under the base policy, and the <em>novelty</em> of the visited states. Intuitively, low likelihood and low novelty demonstrations should be excluded since they represent conflicting modes of behavior on states that the robot can already handle, and are therefore incompatible. This concept of compatibility is used for filtering a new set of demonstrations and actively eliciting compatible demonstrations.</p>
<p>In the following subsections, we describe the process of estimating compatibility and active elicitation in more detal.</p>
<section id="estimating-compatiblity" class="level3" data-number="3.18.1">
<h3 data-number="3.18.1" class="anchored" data-anchor-id="estimating-compatiblity"><span class="header-section-number">3.18.1</span> Estimating Compatiblity</h3>
<p>We want to define a compatibility measure <span class="math inline">\(\mathcal{M}\)</span>, that estimates the performance of policy <span class="math inline">\(\pi_{base}\)</span> that is retrained on a union of <span class="math inline">\(\mathcal{D}_{base}\)</span>, the known base dataset, and <span class="math inline">\(\mathcal{D}_{new}\)</span>, the newly collected dataset. To define this compatibility measure in a way that is easy to compute, we can use two interpretable metrics: likelihood and novelty.</p>
<p>The likelihood of actions <span class="math inline">\(a_{new}\)</span> in <span class="math inline">\(\mathcal{D}_{new}\)</span> is measured as the negative mean squared error between actions predicted by the base policy and this proposed action:</p>
<p><span class="math display">\[\begin{aligned}
    \textsc{likelihood}(s_{new}, a_{new}) = -\mathbb{E}[|| \pi_{base}(s_{new}) - a_{new} ||^2_2].
\end{aligned}\]</span></p>
<p>The novelty of the state <span class="math inline">\(s_{new}\)</span> in <span class="math inline">\(\mathcal{D}_{new}\)</span> is the standard deviation in the predicted actions under base policy:</p>
<p><span class="math display">\[\begin{aligned}
    \textsc{novelty}(s_{new}) = \mathrm{Var}[\pi_{base}(s_{new})].
\end{aligned}\]</span></p>
<p>We can plot likelihood and novelty on a 2D plane, as shown in Figure <a href="#fig:likelihood_novelty" data-reference-type="ref" data-reference="fig:likelihood_novelty">1.23</a>, and identify thresholds on likelihood and novelty, denoted as <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span> respectively. Intuitively, demonstrations with low likelihood in low novelty states should be excluded, because this indicates that there is a conflict between the base behavior and the new demonstration due to multimodality. Note that in high novelty states, the likelihood should be disregarded because the base policy does not have a concrete idea for how to handle these states anyways so more data is needed.</p>
<div id="fig:likelihood_novelty" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/likelihood_novelty.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Examples of plots of likelihood and novelty for compatible and incompatible operators <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span></figcaption>
</figure>
</div>
<p>The final compatibility metric, parameterized by the likelihood and novelty thresholds <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span>, is <span class="math inline">\(\mathcal{M}(\mathcal{D}_{base}, (s_{new}, a_{new})) \in [0, 1]\)</span>, defined as:</p>
<p><span class="math display">\[\begin{aligned}
    \mathcal{M} = \begin{cases}
        1 - \min(\frac{\mathbb{E}[|| \pi_{base}(s_{new}) - a_{new} ||^2_2]}{\lambda}, 1) &amp; \text{ if } \textsc{novelty}(s_{new}) &lt; \eta \\
        1 &amp; \text{ otherwise }
       \end{cases}.
\end{aligned}\]</span></p>
<p>Note that <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span> need to be specified by hand. This is accomplished by assuming the ability to collect <em>a priori incompatible</em> demonstrations to identify reasonable thresholds that remove the most datapoints in the incompatible demonstrations while keeping the most datapoints in the compatible demonstrations.</p>
</section>
<section id="case-studies-with-fixed-sets" class="level3" data-number="3.18.2">
<h3 data-number="3.18.2" class="anchored" data-anchor-id="case-studies-with-fixed-sets"><span class="header-section-number">3.18.2</span> Case Studies with Fixed Sets</h3>
<p>The researchers evaluate the utility of the compatibility metric on three tasks: placing a square nut on a square peg, placing a round nut on a round peg, and opening a drawer and placing a hammer inside. For each task, they train a base policy using a “proficient” operator’s demonstration while sampling trajectories from other operators for the new set.</p>
<p>The naive baseline is to use all datapoints while the <span class="math inline">\(\mathcal{M}\)</span>-Filtered demonstrations use the compatibility metric to filter out incompatible demonstrations. The results are presented in Figure <a href="#fig:m_filter_table" data-reference-type="ref" data-reference="fig:m_filter_table">1.3</a>. As you can see, M-filtering results in equal or greater performance despite using less data than the naive baseline, demonstrating the effectiveness of compatibility-based filtering.</p>
<div id="fig:m_filter_table">
<table class="caption-top table">
<caption>Success rates (mean/std across 3 training runs) for policies trained on <span class="math inline">\(\mathcal{D}_{new}\)</span> by using all the data (Naive) or filtering by compatibility (<span class="math inline">\(\mathcal{M}\)</span>-Filtered) <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span></caption>
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><strong>Square Nut</strong></td>
<td></td>
<td style="text-align: center;"><strong>Round Nut</strong></td>
<td></td>
<td style="text-align: center;"><strong>Hammer Placement</strong></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Operator</strong></td>
<td style="text-align: center;">Naive</td>
<td><span class="math inline">\(\mathcal{M}\)</span>-Filtered</td>
<td style="text-align: center;">Naive</td>
<td><span class="math inline">\(\mathcal{M}\)</span>-Filtered</td>
<td style="text-align: center;">Naive</td>
<td><span class="math inline">\(\mathcal{M}\)</span>-Filtered</td>
</tr>
<tr class="odd">
<td>Base Operator</td>
<td style="text-align: center;">38.7 (2.1)</td>
<td>-</td>
<td style="text-align: center;">13.3 (2.3)</td>
<td>-</td>
<td style="text-align: center;">24.7 (6.1)</td>
<td>-</td>
</tr>
<tr class="even">
<td>Operator 1</td>
<td style="text-align: center;">54.3 (1.5)</td>
<td>61.0 (4.4)</td>
<td style="text-align: center;">26.7 (11.7)</td>
<td>32.0 (12.2)</td>
<td style="text-align: center;">38.0 (2.0)</td>
<td>39.7 (4.6)</td>
</tr>
<tr class="odd">
<td>Operator 2</td>
<td style="text-align: center;">40.3 (5.1)</td>
<td>42.0 (2.0)</td>
<td style="text-align: center;">22.0 (7.2)</td>
<td>26.7 (5.0)</td>
<td style="text-align: center;">33.3 (3.1)</td>
<td>32.7 (6.4)</td>
</tr>
<tr class="even">
<td>Operator 3</td>
<td style="text-align: center;">37.3 (2.1)</td>
<td>42.7 (0.6)</td>
<td style="text-align: center;">17.3 (4.6)</td>
<td>18.0 (13.9)</td>
<td style="text-align: center;">8.0 (0.0)</td>
<td>12.0 (0.0)</td>
</tr>
<tr class="odd">
<td>Operator 4</td>
<td style="text-align: center;">27.3 (3.5)</td>
<td>37.3 (2.1)</td>
<td style="text-align: center;">7.3 (4.6)</td>
<td>13.3 (1.2)</td>
<td style="text-align: center;">4.0 (0.0)</td>
<td>4.0 (0.0)</td>
</tr>
</tbody>
</table>
</div>
<div id="fig:active_elicitation" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Figures/active_elicitation.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>The phases of the active elicitation interface: (a) initial prompting, (b) demonstrations with live feedback, and (c) corrective feedback <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="actively-eliciting-compatible-demonstrations" class="level3" data-number="3.18.3">
<h3 data-number="3.18.3" class="anchored" data-anchor-id="actively-eliciting-compatible-demonstrations"><span class="header-section-number">3.18.3</span> Actively Eliciting Compatible Demonstrations</h3>
<p>In the previous section, we assume access to a dataset that has already been collected, and we see how filtering out incompatible demonstrations helps improve performance. However, when collecting a new dataset, it would be better to ensure that operators collect compatible demonstrations from the start, allowing us to retain as much data as possible for training.</p>
<p>To actively elicit compatible demonstrations, the researchers set up a pipeline for live feedback and examples. At the start, operators are given a task specification and some episodes to practice using the robot. Then, the active elicitation process begins, as shown in Figure <a href="#fig:active_elicitation" data-reference-type="ref" data-reference="fig:active_elicitation">1.24</a>. Each operator is shown some rollouts of the base policy to understand the style of the base operator. Next, the operator provides a demonstration similar to the ones they were shown. As they record their demonstrations, the interface provides online feedback, with green indicating compatible actions and red indicating incompatible actions. If the number of incompatible state-action pairs (ones where <span class="math inline">\(\mathcal{M}\)</span> is zero) exceeds 5% of the demonstration length, the demonstration is rejected. However, to provide corrective feedback, the interface shows the areas of the demonstration with the highest average incompatibility and also provides an expert demo that shows what should actually be done. Demonstrators can use this feedback to provide more compatible demonstrations moving forward.</p>
<p>This process helps improve the demonstration quality in both simulation and real experiments, as show in Figure <a href="#fig:active_elicitation_results" data-reference-type="ref" data-reference="fig:active_elicitation_results">1.4</a>. Specifically, on the real results, active elicitation outperformed the base policy by 25% and naive data collection by 55%. Overall, active elicitation is a powerful tool to ensure that data collected for imitation learning improves the quality of the learned policy.</p>
<div id="fig:active_elicitation_results">
<table class="caption-top table">
<caption>Success rates (mean/std across users) for policies trained on <span class="math inline">\(\mathcal{D}_{new}\)</span> by using all the data (Naive), filtering by compatibility (<span class="math inline">\(\mathcal{M}\)</span>-Filtered), or using informed demonstration collection <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span></caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Task</strong></th>
<th style="text-align: center;"><strong>Base</strong></th>
<th style="text-align: center;"><strong>Naive</strong></th>
<th style="text-align: center;"><strong>Naive + Filtered</strong></th>
<th style="text-align: center;"><strong>Informed</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Round Nut</strong></td>
<td style="text-align: center;">13.3 (2.3)</td>
<td style="text-align: center;">9.6 (4.6)</td>
<td style="text-align: center;">9.7 (4.2)</td>
<td style="text-align: center;">15.7 (6.0)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Hammer Placement</strong></td>
<td style="text-align: center;">24.7 (6.1)</td>
<td style="text-align: center;">20.8 (15.7)</td>
<td style="text-align: center;">22.0 (15.5)</td>
<td style="text-align: center;">31.8 (16.3)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><span class="math inline">\(\left[ \textup{Real} \right]\)</span> Food Plating</strong></td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">30.0 (17.3)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.0 (9.6)</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="limitations-and-future-work-for-active-elicitation" class="level3" data-number="3.18.4">
<h3 data-number="3.18.4" class="anchored" data-anchor-id="limitations-and-future-work-for-active-elicitation"><span class="header-section-number">3.18.4</span> Limitations and Future Work for Active Elicitation</h3>
<p>A fundamental limitation of eliciting compatible demonstrations is the fact that the “base” demonstrator is considered the ground truth. When the base demonstrator specifies a preference, all other demonstrators must abide by it, even if they have strong preferences against it. For instance, when pouring milk and cereal into a bowl, different people have different preferences for what is the correct order, but active elicitation forces all demonstrators to follow the initial preference of the base operator. The researchers hope that future work can enable users to override the default demonstration set and follow a base behavior that better aligns with their preferences. This could enable multiple modes of behavior to be collected in data while only following a user’s specified preference instead of attempting to collapse all modes into a single policy.</p>
<p>Looking forward, active elicitation provides a foundation for allowing robots to query humans about the type of data needed, enabling more efficient data collection through transparency.</p>
</section>
</section>
<section id="conclusion-1" class="level2" data-number="3.19">
<h2 data-number="3.19" class="anchored" data-anchor-id="conclusion-1"><span class="header-section-number">3.19</span> Conclusion</h2>
<p>In summary, this chapter has explored the complexities and innovations in interactive learning as applied to large models within robotics. It begins by investigating pairwise comparisons and their role in efficiently learning linear reward functions from large datasets, overcoming limitations in supervised learning. When combined with active learning techniques, these comparisons supply timely, targeted, and context-appropriate feedback, enhancing performance in time-critical applications like exoskeleton adjustments during rehabilitation.</p>
<p>We then shift to imitation learning or inverse reward learning from demonstrations, emphasizing the difficulties introduced by multimodal demonstration sets. active elicitation approaches to compile compatible demonstrations, streamlining the learning process by guiding users to provide more valuable, steady examples are incredibly promising, however, to tackling this issue. This method shows promise in refining the interactive imitation learning data collection pipeline, enabling more capable and effective robotic training.</p>
<p>Additionally, the chapter examines the integration of foundation models into robotics, highlighting the transformative innovations of R3M and Voltron. R3M’s pre-training on diverse human activities dramatically improves robotic manipulation with minimal supervision. Meanwhile, Voltron builds on these capabilities by incorporating language-driven representation learning for remarkably adaptable and nuanced robotic task performance. These models represent significant leaps in robotics while opening new frontiers for future research and applications.</p>
</section>
<section id="truthful-preference-elicitation-with-adversary" class="level2" data-number="3.20">
<h2 data-number="3.20" class="anchored" data-anchor-id="truthful-preference-elicitation-with-adversary"><span class="header-section-number">3.20</span> Truthful Preference Elicitation with Adversary</h2>
<p>In our study of social choice models in Chapter <a href="002-reward_model.html" data-reference-type="ref" data-reference="2model">[2model]</a>, we study how axiomatic properties are implemented to prevent strategic manipulation of a population. This brings us onto the field of <strong>mechanism design</strong>. At its core, mechanism design is the science of making rules. The intent in this field is to design systems so that the strategic behaviour of individuals leads to desirable outcomes. Just thinking about services on the Internet – file sharing, reputation systems, web search, web advertising, email, Internet auctions, congestion control – all have to be set up so that an individual’s selfish behavior leads to better outcomes for the entire community. A more specific example of this is the phenomenon of “bid-sniping” that was present on eBay in the early 2000s. When people could bid on E-bay, the rule was that the highest bidder by the end of some specified time period would get the item. As a result, people would just wait until the very last minute to bid in order to not raise the price of the item too early. On the other hand, when Amazon still allowed bidding, they had a rule that any time a bid was placed it would extend the time of the bid by ten minutes. This simple difference had drastic effects on bidding prices over time. Mechanism design develops the theoretical framework for learning social choices and eliciting truthful preference.</p>
<p>We will cover frameworks that model several scenarios that mechanism design is usefully applied to: recommendation systems (where users will selfishly try to stick to their preferences while a planner encourages exploration); auctions (where bidders will try to maximise their reward compared to others); and peer grading (where truthful reporting is not necessarily an incentive for students).</p>
<section id="auction-theory" class="level3" data-number="3.20.1">
<h3 data-number="3.20.1" class="anchored" data-anchor-id="auction-theory"><span class="header-section-number">3.20.1</span> Auction Theory</h3>
<section id="single-item-auctions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="single-item-auctions">Single-Item Auctions</h4>
<p>The first problem within auction theory we will consider is the <em>single-item auction</em>. The premise of this problem is that there is a single item to sell, <span class="math inline">\(n\)</span> bidders (with unknown private valuations of the item <span class="math inline">\(v_1\)</span>, ..., <span class="math inline">\(v_n\)</span>). The bidder’s individual objective is to maximize utility: the value <span class="math inline">\(v_i\)</span> of the item subtracted by the price paid for the item. The auction procedure is standard in the sense that bids are solicited and the highest bid will win the auction. While the objective of the individual bidder is clear, there could be a plethora of different objectives for the auction as a whole. One option could be to maximize social surplus, meaning the goal is to maximize the value of the winner. Another objective could be to maximize seller profit which is the payment of the winner. For simplicity, we can focus on the first objective where the goal is to maximize social surplus. If we want to maximize social surplus it turns out that a great way to do this is the “second-price auction”.</p>
<section id="maximizing-social-surplus" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="maximizing-social-surplus">Maximizing Social Surplus</h5>
<p>In the second-price auction, we will operate under slightly different conditions. In the second-price auction we 1) solicit sealed bids, 2) have the winner be the highest bidder, and 3) charger winner the second-highest bid price. As an example, if the solicited bids are <span class="math inline">\(b = (2, 6, 4, 1)\)</span> the winner will be that who bid <span class="math inline">\(6\)</span>, but will pay a price of <span class="math inline">\(4\)</span>. From here, we can do some equilibrium analysis to try and learn what the optimal bidding strategy is for each bidder. Let the amount bidder <span class="math inline">\(i\)</span> bids to be <span class="math inline">\(b_i\)</span>, so we have bids <span class="math inline">\(b_1, b_2, ..., b_n\)</span>. How much should bidder <span class="math inline">\(i\)</span> bid? To analyze this, let us define <span class="math inline">\(t_i = max_{j \neq i} b_j\)</span> which represents the max of the bids that is not from bidder <span class="math inline">\(i\)</span>. There are now two cases to consider: if <span class="math inline">\(b_i\)</span> &gt; <span class="math inline">\(t_i\)</span> and if <span class="math inline">\(b_i\)</span> &lt; <span class="math inline">\(t_i\)</span>. In the first case the bidder <span class="math inline">\(i\)</span> wins, and if the bidder bid <span class="math inline">\(b_i = v_i\)</span>, they are guaranteed to have a positive return on bid. In the other case, they lose the bid and the net loss is 0 because they don’t have to pay. From this we can conclude that bidder <span class="math inline">\(i\)</span>’s dominant strategy is to just bid <span class="math inline">\(b_i = v_i\)</span>. Rigorously proving this is a little bit trickier, but it was shown from Vickrey in 1961 [cite] that truthful bidding is the dominant strategy in second-price auctions. A corollary of this is that we are maximizing social surplus since bids are values and the winner is the bidder with highest valuation.</p>
</section>
<section id="maximize-seller-profit" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="maximize-seller-profit">Maximize Seller Profit</h5>
<p>If we want to look at things from the perspective of a seller trying to maximize their profit we need to treat the bidder’s bids as uniform random variables. Consider the example scenario where we have two bidders each bidding uniformly between 0 and 1. What is the seller’s expected profit? (in this case profit and revenue for the seller are the same because we assume the seller throws away the item if it doesn’t sell/has no valuation for it).</p>
<p>From there the question now becomes, can we get more expected profit from the seller’s perspective? It turns out there is a design where we can add a reserve price of <span class="math inline">\(r\)</span> to the second-price auction. The way this works is we can 1) Insert seller-bid at <span class="math inline">\(r\)</span>, 2) solicit bids, 3) pick the highest bidder, and 3) charge the 2nd-highest bid. In effect, this is just the second-price auction but with a bid from the seller as well, at a price of <span class="math inline">\(r\)</span>. A lemma, that we won’t prove here, is that the second-price auction with reserve price <span class="math inline">\(r\)</span> still has a dominant strategy of just being truthful.</p>
<p>Let’s now consider what the profit of a second-price auction would be with two bidders that uniformly bid between 0 and 1 – but this time we have a reserve price of <span class="math inline">\(1/2\)</span>. To calculate the expected profit we break down the situation into 3 cases:</p>
<ul>
<li><p>Case 1: <span class="math inline">\(1/2 &gt; v_1 &gt; v_2 \rightarrow 1/4 \text{ probability} \rightarrow  E[\text{profit}] = 0\)</span></p></li>
<li><p>Case 2: <span class="math inline">\(v_1 &gt; v_2 &gt; 1/2 \rightarrow 1/4 \text{probability} \rightarrow E[v2 | case 2] = 2/3\)</span></p></li>
<li><p>Case 3: <span class="math inline">\(v_1 &gt; 1/2 &gt; v_2 \rightarrow 1/2 \text{ probability} \rightarrow 1/2\)</span></p></li>
</ul>
<p>Why is <span class="math inline">\(E[v2 | case 2] = 2/3\)</span>? If <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> are greater than <span class="math inline">\(1/2\)</span>, they are evenly spread across the interval, meaning the expectation will be 1/2 + 1/6 = 2/3. Adding up all these cases we get <span class="math inline">\(E[profit] = 5/12\)</span>. It turns out that second-price auctions with reserve actually maximize profit in general (for symmetric bidders)!</p>
<p>In the previous section we conclude that second-price auctions with reserve maximize profit for the seller. In order to prove this, we now move to the more general topic of asking how should a monopolist divide good across separate markets. We can make the assumption that the demand model is a concave revenue <span class="math inline">\(R(q)\)</span> in quantity <span class="math inline">\(q\)</span>. Under this assumption, we can just divide supply into <span class="math inline">\(q = q_a + q_b\)</span> such that <span class="math inline">\(R'_a(q_a) = R'_b(q_b)\)</span>. The idea from here is a theorem from Myerson in 1981 that states an optimal action maximizes "marginal revenue". Consider an example where we have two bidders bidding a uniform value between 0 and 1. Our revenue curve can now be derived from the offering price <span class="math inline">\(V(q) = 1 - q\)</span> like so: <span class="math inline">\(R(q) = qV(q) = q - q^2\)</span>. Taking the derivative gives us the marginal revenue <span class="math inline">\(R'(q) = 1-2q\)</span>. This means two things: 1) we want to sell to bidder <span class="math inline">\(i\)</span> with the highest <span class="math inline">\(R'(q_i)\)</span> and 2) we want to sell to bidder <span class="math inline">\(i\)</span> with value at least <span class="math inline">\(1/2\)</span> (if we want a positive <span class="math inline">\(R'(q_i)\)</span>. But this is just a second-price auction with reserve <span class="math inline">\(1/2\)</span>! This means that for symmetric bidders, a second price with reserve is the optimal auction.</p>
</section>
<section id="what-good-are-auctions" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="what-good-are-auctions">What good are auctions?</h5>
<p>An interesting topic to discuss is what benefits auctions bring to the table as opposed to just standard pricing. Online auctions used to be a lot more popular in the early 2000s and have been completely replaced by standard online pricing, even on sites like e-bay. While auctions are slower and have added inherent complexities, they are actually optimal on paper. Standard pricing on the other is non-optimal; although it is fast and simpler for buyers. There is actually a way to quantify this: for pricing <span class="math inline">\(k\)</span> units, the loss is at most <span class="math inline">\(1 / \sqrt{2\pi k}\)</span> of optimal profit.</p>
<p>Let’s consider applications in duopoly platform design. We know that the optimal auction is second-price with reserve, but what happens when we introduce competition between two auction platforms? Some important details related to the revenue of a second-price auction is that a second-price auction with no reserve and n bidders leads to larger revenue having an optimal reserve and n - 1 bidders <span class="citation" data-cites="bulow-klemperer1996">(<a href="#ref-bulow-klemperer1996" role="doc-biblioref">Bulow and Klemperer 1996</a>)</span>. Additionally, with an entry cost, no reserve is the optimal strategy for maximizing revenue <span class="citation" data-cites="mcafee-87">(<a href="#ref-mcafee-87" role="doc-biblioref">McAfee and McMillan 1987</a>)</span>. Let’s consider an example of a competing auction system which is Google ads vs Bing ads. How should an advertiser divide the budget between Google and Bing? They should give the same budget to both companies. What happens if Bing raises their prices? Then, the advertising company moves more of its budget to Google from Bing.</p>
</section>
</section>
<section id="prior-independent-auctions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="prior-independent-auctions">Prior-Independent Auctions</h4>
<p>The Bulow-Klemperer theorem demonstrates that increased competition can be more valuable than perfect knowledge of bidders’ valuation distributions. This result provides insight into the potential of simple, prior-independent auctions to approach the performance of optimal auctions. The theorem states that for a single-item auction with bidders’ valuations drawn independently from a regular distribution <span class="math inline">\(F\)</span>:</p>
<div class="theorem">
<p>Let <span class="math inline">\(F\)</span> be a regular distribution and <span class="math inline">\(n\)</span> a positive integer. Then: <span class="math display">\[E_{v_1,\ldots,v_{n+1} \sim F}[\text{Rev(VA)}(n+1 \text{ bidders})] \geq E_{v_1,\ldots,v_n \sim F}[\text{Rev(OPT}_F)(n \text{ bidders})]\]</span> where VA denotes the Vickrey auction and <span class="math inline">\(\text{OPT}_F\)</span> denotes the optimal auction for <span class="math inline">\(F\)</span>.</p>
</div>
<p>This shows that running a simple Vickrey auction with one extra bidder outperforms the revenue-optimal auction that requires precise knowledge of the distribution. It suggests that in practice, effort spent on recruiting additional bidders may be more fruitful than fine-tuning auction parameters.</p>
</section>
<section id="the-vcg-mechanism" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-vcg-mechanism">The VCG Mechanism</h4>
<p>The VCG mechanism is a cornerstone of mechanism design theory, providing a general solution for welfare maximization in multi-parameter environments. The key result is:</p>
<div class="theorem">
<p><span id="thm:VCG" label="thm:VCG"></span> In every general mechanism design environment, there is a dominant-strategy incentive-compatible (DSIC) welfare-maximizing mechanism.</p>
</div>
<p>The VCG mechanism operates as follows:</p>
<ol type="1">
<li><p>Given bids <span class="math inline">\(b_1, \ldots, b_n\)</span>, where each <span class="math inline">\(b_i\)</span> is indexed by the outcome set <span class="math inline">\(\Omega\)</span>, the allocation rule is:</p>
<p><span class="math display">\[x(b) = \argmax_{\omega \in \Omega} \sum_{i=1}^n b_i(\omega)\]</span></p></li>
<li><p>The payment rule for each agent <span class="math inline">\(i\)</span> is:</p>
<p><span class="math display">\[p_i(b) = \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) - \sum_{j \neq i} b_j(\omega^*)\]</span></p>
<p>where <span class="math inline">\(\omega^* = x(b)\)</span> is the chosen outcome.</p></li>
</ol>
<p>The key insight is to charge each agent its “externality” - the welfare loss inflicted on other agents by its presence. This payment rule, coupled with the welfare-maximizing allocation rule, yields a DSIC mechanism.</p>
<p>The VCG mechanism can be interpreted as having each agent pay its bid minus a "rebate" equal to the increase in welfare attributable to its presence:</p>
<p><span class="math display">\[p_i(b) = b_i(\omega^*) - \left[ \sum_{j=1}^n b_j(\omega^*) - \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) \right]\]</span></p>
<p>While the VCG mechanism provides a theoretical solution for DSIC welfare-maximization in general environments, it can be challenging to implement in practice due to computational and communication complexities.</p>
</section>
<section id="combinatorial-auctions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="combinatorial-auctions">Combinatorial Auctions</h4>
<p>Combinatorial auctions are an important class of multi-parameter mechanism design problems, with applications ranging from spectrum auctions to airport slot allocation. In a combinatorial auction:</p>
<ul>
<li><p>There are <span class="math inline">\(n\)</span> bidders and a set <span class="math inline">\(M\)</span> of <span class="math inline">\(m\)</span> items.</p></li>
<li><p>The outcome set <span class="math inline">\(\Omega\)</span> consists of allocations <span class="math inline">\((S_1, \ldots, S_n)\)</span>, where <span class="math inline">\(S_i\)</span> is the bundle allocated to bidder <span class="math inline">\(i\)</span>.</p></li>
<li><p>Each bidder <span class="math inline">\(i\)</span> has a private valuation <span class="math inline">\(v_i(S)\)</span> for each bundle <span class="math inline">\(S \subseteq M\)</span>.</p></li>
</ul>
<p>While the VCG mechanism theoretically solves the welfare-maximization problem, combinatorial auctions face several major challenges in practice:</p>
<ol type="1">
<li><p>Preference Elicitation: Each bidder has <span class="math inline">\(2^m - 1\)</span> private parameters, making direct revelation infeasible for even moderate numbers of items. This necessitates the use of indirect mechanisms that elicit information on a "need-to-know" basis.</p></li>
<li><p>Computational Complexity: Even when preference elicitation is not an issue, welfare-maximization can be an intractable problem. In practice, approximations are often used, hoping to achieve reasonably good welfare.</p></li>
<li><p>VCG Limitations: The VCG mechanism can exhibit bad revenue and incentive properties in combinatorial settings. For example, adding bidders can sometimes decrease revenue to zero, and the mechanism can be vulnerable to collusion and false-name bids.</p></li>
<li><p>Strategic Behavior in Iterative Auctions: Most practical combinatorial auctions are iterative, comprising multiple rounds. This introduces new opportunities for strategic behavior, such as using bids to signal intentions to other bidders.</p></li>
</ol>
<p>These challenges make combinatorial auctions a rich and complex area of study, requiring careful design to balance theoretical guarantees with practical considerations.</p>
</section>
<section id="spectrum-auctions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="spectrum-auctions">Spectrum Auctions</h4>
<p>Spectrum auctions represent a complex application of combinatorial auction theory. With n bidders and m non-identical items, each bidder has a private valuation for every possible bundle of items, making it impractical to directly elicit all preferences. This necessitates the use of indirect, iterative mechanisms that query bidders for valuation information on a “need-to-know” basis, sacrificing some of the desirable properties of direct mechanisms like dominant strategy incentive compatibility (DSIC) and full welfare maximization.</p>
<p>The fundamental challenge in spectrum auctions lies in the nature of the items being sold. There is a dichotomy between items that are substitutes (where <span class="math inline">\(v(AB) \leq v(A) + v(B))\)</span> and those that are complements (where <span class="math inline">\(v(AB) &gt; v(A) + v(B))\)</span>. Substitute items, such as licenses for the same area with equal-sized frequency ranges, are generally easier to handle. When items are substitutes, welfare maximization is computationally tractable, and the VCG mechanism avoids many undesirable properties. However, complementary items, which arise naturally in spectrum auctions when bidders want adjacent licenses, present significant challenges.</p>
<p>Early attempts at spectrum auctions revealed the pitfalls of naive approaches. Sequential auctions, where items are sold one after another, proved problematic as demonstrated by a Swiss auction in 2000. Bidders struggled to bid intelligently without knowing future prices, leading to unpredictable outcomes and potential revenue loss. Similarly, simultaneous sealed-bid auctions, as used in New Zealand in 1990, created difficulties for bidders in coordinating their bids across multiple items, resulting in severely suboptimal outcomes.</p>
<p>The Simultaneous Ascending Auction (SAA) emerged as a solution to these issues and has formed the basis of most spectrum auctions over the past two decades. In an SAA, multiple items are auctioned simultaneously in rounds, with bidders placing bids on any subset of items subject to an activity rule. This format facilitates price discovery, allowing bidders to adjust their strategies as they learn about others’ valuations. It also allows bidders to determine valuations on a need-to-know basis, reducing the cognitive burden compared to direct-revelation auctions.</p>
<p>Despite its advantages, the SAA is not without vulnerabilities. Demand reduction, where bidders strategically reduce their demand to lower prices, can lead to inefficient outcomes even when items are substitutes. The exposure problem arises with complementary items, where bidders risk winning only a subset of desired items at unfavorable prices. These issues highlight the ongoing challenges in designing effective spectrum auctions, balancing theoretical guarantees with practical considerations.</p>
</section>
<section id="case-study-classroom-peer-grading" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="case-study-classroom-peer-grading">Case study: Classroom Peer Grading</h4>
<p>This chapter discusses work by Jason Hartline, Yingkai Li, Liren Shan, and Yifan Wu at Northwestern University, where researchers examined mechanism design for the classroom, specifically in terms of the optimization of scoring rules. They explored peer grading in the classroom and how to construct a peer grading system that optimizes the objectives for each stakeholder in the system, including those being graded, the peer graders, the TAs of the class, and the professor.</p>
<p>Firstly, let’s think of the classroom like a computer. We can think of students as local optimizers; their incentive is to minimize the amount of work they need to do and maximize the grades that they receive. The graders are imprecise operators, which means that there is some uncertainty in their ability to grade the work completed by the students. The syllabus can be thought of as the rules that map the actions of the students to the grade they end up receiving in the class. Our overall goals for this classroom based on these definitions is to minimize work, maximize learning, and fairly assess the students for the work that they do <span class="citation" data-cites="jasonH2020">(<a href="#ref-jasonH2020" role="doc-biblioref">Hartline et al. 2020</a>)</span>.</p>
<p>One basic question that we can examine, is what is the best syllabus that maximizes our objectives for our classroom design. Some components of this could include grading randomized exams, grading with partial credit, group projects, and finally, peer grading, which is the component that we will be taking a deeper dive into.</p>
<p>The general situation of the peer grading problem is that proper scoring rules make peer grades horrible <span class="citation" data-cites="jasonH2020">(<a href="#ref-jasonH2020" role="doc-biblioref">Hartline et al. 2020</a>)</span>. So we want to be able to optimize scoring rules and make sure that we are optimizing each component of the peer grading pipeline.</p>
<p>The main algorithms focused on in this peer grading design paper were matching peers and TAs to submissions and the grading of those submissions from the TAs and the peer reviews <span class="citation" data-cites="jasonH2020">(<a href="#ref-jasonH2020" role="doc-biblioref">Hartline et al. 2020</a>)</span>. There are quite a number of advantages to peer grading including that peers are able to learn from reviewing other people’s work, it reduces the work for the teacher, and improves the turnaround time for assignment feedback (which are all part of our overarching goals for our mechanism design for the classroom). But, it is also important to acknowledge the potential disadvantages of the peer grading system: it is possible that the peer graders present inaccurate grades and there is student unrest. This presents us with a challenge: being able to incentivize accurate peer reviews.</p>
<p>One problem that we run into, when we use the proper scoring rule to score peer reviews, if the peer graders use the lazy peer strategy, which means that they always report 80<span class="math inline">\(\%\)</span> for their peer reviews, they get graded very well using the proper scoring rule algorithm. In fact, the proper scoring rule says that their peer review is 96<span class="math inline">\(\%\)</span> accurate <span class="citation" data-cites="jasonH2023">(<a href="#ref-jasonH2023" role="doc-biblioref">Hartline et al. 2023</a>)</span>. So how do we incentivize effort in reviews from peer graders? We use a scoring rule that maximizes the difference in score between effort or no effort reviews as indicated by the peer reviewers <span class="citation" data-cites="jasonH2023">(<a href="#ref-jasonH2023" role="doc-biblioref">Hartline et al. 2023</a>)</span>. So overall, the analysis of datasets leads to decision optimizations and, eventually, payoff from those decisions.</p>
<p>To conclude our mechanism design in the classroom discussion, we have two key takeaways: scoring rules are essential in being able to understand and analyze data thoroughly, and optimal scoring rules for binary effort allow us to understand the setting independent of the dataset <span class="citation" data-cites="jasonH2023">(<a href="#ref-jasonH2023" role="doc-biblioref">Hartline et al. 2023</a>)</span>.</p>
</section>
</section>
<section id="mutual-information-paradigm" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="mutual-information-paradigm">Mutual Information Paradigm</h3>
<p>In this section we discuss an influential new framework for designing peer prediction mechanisms, the Mutual Information Paradigm (MIP) introduced by Kong and Schoenebeck <span class="citation" data-cites="kongschoenebeck2019">(<a href="#ref-kongschoenebeck2019" role="doc-biblioref">Kong and Schoenebeck 2019</a>)</span>. Traditional peer prediction approaches typically rely on scoring rules and correlation between agents’ signals. However, these methods often struggle with issues like uninformed equilibria, where agents can coordinate on uninformative strategies that yield higher payoffs than truth-telling. The core idea is to reward agents based on the mutual information between their report and the reports of other agents.</p>
<p>We consider a setting with <span class="math inline">\(n\)</span> agents, each possessing a private signal <span class="math inline">\(\Psi_i\)</span> drawn from some set <span class="math inline">\(\Sigma\)</span>. The mechanism asks each agent to report their signal, which we denote as <span class="math inline">\(\hat{\Psi}_i\)</span>. For each agent <span class="math inline">\(i\)</span>, the mechanism randomly selects a reference agent <span class="math inline">\(j \neq i\)</span>. Agent <span class="math inline">\(i\)</span>’s payment is then calculated as: <span class="math display">\[MI(\hat{\Psi}_i; \hat{\Psi}_j)\]</span> where <span class="math inline">\(MI\)</span> is an information-monotone mutual information measure. An information-monotone <span class="math inline">\(MI\)</span> measure must satisfy the following properties:</p>
<ul>
<li><p><strong>Symmetry</strong>: <span class="math inline">\(MI(X; Y) = MI(Y; X)\)</span>.</p></li>
<li><p><strong>Non-negativity</strong>: <span class="math inline">\(MI(X; Y) \geq 0\)</span>, with equality if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p></li>
<li><p><strong>Data processing inequality</strong>: For any transition probability <span class="math inline">\(M\)</span>, if <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(M(X)\)</span> conditioned on <span class="math inline">\(X\)</span>, then <span class="math inline">\(MI(M(X); Y) \leq MI(X; Y)\)</span>.</p></li>
</ul>
<p>Two important families of mutual information measures that satisfy these properties are <span class="math inline">\(f\)</span>-mutual information and Bregman mutual information. The <span class="math inline">\(f\)</span>-mutual information is defined as: <span class="math display">\[MI_f(X; Y) = D_f(U_{X,Y}, V_{X,Y})\]</span> where <span class="math inline">\(D_f\)</span> is an <span class="math inline">\(f\)</span>-divergence, <span class="math inline">\(U_{X,Y}\)</span> is the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(V_{X,Y}\)</span> is the product of their marginal distributions. The Bregman mutual information is defined as: <span class="math display">\[BMI_{PS}(X; Y) = \mathbb{E}X [D{PS}(U_{Y|X}, U_Y)]\]</span> where <span class="math inline">\(D_{PS}\)</span> is a Bregman divergence based on a proper scoring rule <span class="math inline">\(PS\)</span>, <span class="math inline">\(U_{Y|X}\)</span> is the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and <span class="math inline">\(U_Y\)</span> is the marginal distribution of <span class="math inline">\(Y\)</span>.</p>
<p>The MIP framework can be applied in both single-question and multi-question settings. In the multi-question setting, the mechanism can estimate the mutual information empirically from multiple questions. In the single-question setting, additional techniques like asking for predictions about other agents’ reports are used to estimate the mutual information.</p>
<p>A key theoretical result of the MIP framework is that when the chosen mutual information measure is strictly information-monotone with respect to agents’ priors, the resulting mechanism is both dominantly truthful and strongly truthful. This means that truth-telling is a dominant strategy for each agent and that the truth-telling equilibrium yields strictly higher payoffs than any other non-permutation strategy profile.</p>
<p>As research continues to address practical implementation challenges of designing truthful mechanisms, MIP-based approaches have significant potential to improve preference elicitation and aggregation in real-world applications lacking verifiable ground truth.</p>
</section>
<section id="auction-theory-2" class="level3" data-number="3.20.2">
<h3 data-number="3.20.2" class="anchored" data-anchor-id="auction-theory-2"><span class="header-section-number">3.20.2</span> Auction Theory 2</h3>
<section id="single-item-auctions-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="single-item-auctions-1">Single-Item Auctions</h4>
<p>The first problem within auction we will consider is the <em>single-item auction</em>. In this problem setup, there is a single item to sell and <span class="math inline">\(n\)</span> bidders each with unknown private valuations of the item <span class="math inline">\(v_1, \ldots, v_n\)</span>,</p>


</section>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-unnoisy_humans" class="csl-entry" role="listitem">
Amershi, Saleema, Maya Cakmak, W. Bradley Knox, and Todd Kulesza. 2014. <span>“Power to the People: The Role of Humans in Interactive Machine Learning.”</span> <em>AI Magazine</em>.
</div>
<div id="ref-AL_committee" class="csl-entry" role="listitem">
Beluch, William H., Tim Genewein, A. Nürnberger, and Jan M. Köhler. 2018. <span>“The Power of Ensembles for Active Learning in Image Classification.”</span> <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9368–77. <a href="https://api.semanticscholar.org/CorpusID:52838058">https://api.semanticscholar.org/CorpusID:52838058</a>.
</div>
<div id="ref-AL_usercentered" class="csl-entry" role="listitem">
Bernard, J., Matthias Zeppelzauer, Markus Lehmann, Martin Müller, and Michael Sedlmair. 2018. <span>“Towards User‐centered Active Learning Algorithms.”</span> <em>Computer Graphics Forum</em> 37. <a href="https://api.semanticscholar.org/CorpusID:51875861">https://api.semanticscholar.org/CorpusID:51875861</a>.
</div>
<div id="ref-pmlr-v87-biyik18a" class="csl-entry" role="listitem">
Biyik, Erdem, and Dorsa Sadigh. 2018. <span>“Batch Active Preference-Based Learning of Reward Functions.”</span> In <em>Proceedings of the 2nd Conference on Robot Learning</em>, edited by Aude Billard, Anca Dragan, Jan Peters, and Jun Morimoto, 87:519–28. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v87/biyik18a.html">https://proceedings.mlr.press/v87/biyik18a.html</a>.
</div>
<div id="ref-bommasani2022opportunities" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, et al. 2022. <span>“On the Opportunities and Risks of Foundation Models.”</span> <a href="https://arxiv.org/abs/2108.07258">https://arxiv.org/abs/2108.07258</a>.
</div>
<div id="ref-AL_exploreexploit" class="csl-entry" role="listitem">
Bouneffouf, Djallel, Romain Laroche, Tanguy Urvoy, Raphaël Féraud, and Robin Allesiardo. 2014. <span>“Contextual Bandit for Active Learning: Active Thompson Sampling.”</span> In <em>International Conference on Neural Information Processing</em>. <a href="https://api.semanticscholar.org/CorpusID:1701357">https://api.semanticscholar.org/CorpusID:1701357</a>.
</div>
<div id="ref-pref4" class="csl-entry" role="listitem">
Braziunas, Darius, and Craig Boutilier. 2012. <span>“Minimax Regret Based Elicitation of Generalized Additive Utilities.”</span> <a href="https://arxiv.org/abs/1206.5255">https://arxiv.org/abs/1206.5255</a>.
</div>
<div id="ref-brohan2023rt2" class="csl-entry" role="listitem">
Brohan, Anthony, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, et al. 2023. <span>“RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.”</span> <a href="https://arxiv.org/abs/2307.15818">https://arxiv.org/abs/2307.15818</a>.
</div>
<div id="ref-bulow-klemperer1996" class="csl-entry" role="listitem">
Bulow, Jeremy, and Paul Klemperer. 1996. <span>“Auctions Versus Negotiations.”</span> <em>The American Economic Review</em> 86 (1): 180–94. <a href="http://www.jstor.org/stable/2118262">http://www.jstor.org/stable/2118262</a>.
</div>
<div id="ref-AL_expmodelchange" class="csl-entry" role="listitem">
Cai, Wenbin, Ya Zhang, and Jun Zhou. 2013. <span>“Maximizing Expected Model Change for Active Learning in Regression.”</span> In <em>2013 IEEE 13th International Conference on Data Mining</em>, 51–60. <a href="https://doi.org/10.1109/ICDM.2013.104">https://doi.org/10.1109/ICDM.2013.104</a>.
</div>
<div id="ref-AL_variance" class="csl-entry" role="listitem">
Cohn, David A., Zoubin Ghahramani, and Michael I. Jordan. 1996. <span>“Active Learning with Statistical Models.”</span> <em>CoRR</em> cs.AI/9603104. <a href="https://arxiv.org/abs/cs/9603104">https://arxiv.org/abs/cs/9603104</a>.
</div>
<div id="ref-deng2009imagenet" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. <span>“ImageNet: A Large-Scale Hierarchical Image Database.”</span> In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–55. IEEE.
</div>
<div id="ref-geo_paper" class="csl-entry" role="listitem">
G., Jamieson Kevin, and Robert Nowak. 2011. <span>“Active Ranking Using Pairwise Comparisons.”</span> <em>Advances in Neural Information Processing Systems</em> 24.
</div>
<div id="ref-gandhi2022eliciting" class="csl-entry" role="listitem">
Gandhi, Kanishk, Siddharth Karamcheti, Madeline Liao, and Dorsa Sadigh. 2022. <span>“Eliciting Compatible Demonstrations for Multi-Human Imitation Learning.”</span> In <em>Proceedings of the 6th Conference on Robot Learning (CoRL)</em>.
</div>
<div id="ref-bias_variance_orig_paper" class="csl-entry" role="listitem">
Geman, Stuart, Elie Bienenstock, and René Doursat. 1992. <span>“Neural Networks and the Bias/Variance Dilemma.”</span> <em>Neural Computation</em> 4: 1–58. <a href="https://api.semanticscholar.org/CorpusID:14215320">https://api.semanticscholar.org/CorpusID:14215320</a>.
</div>
<div id="ref-monte-carlo" class="csl-entry" role="listitem">
Ghojogh, Benyamin, Hadi Nekoei, Aydin Ghojogh, Fakhri Karray, and Mark Crowley. 2020. <span>“Sampling Algorithms, from Survey Sampling to Monte Carlo Methods: Tutorial and Literature Review.”</span> <a href="https://arxiv.org/abs/2011.00901">https://arxiv.org/abs/2011.00901</a>.
</div>
<div id="ref-grauman2022ego4d" class="csl-entry" role="listitem">
Grauman, Kristen, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, et al. 2022. <span>“Ego4D: Around the World in 3,000 Hours of Egocentric Video.”</span> <a href="https://arxiv.org/abs/2110.07058">https://arxiv.org/abs/2110.07058</a>.
</div>
<div id="ref-noisy_humans" class="csl-entry" role="listitem">
Guillory, Andrew, and Jeff Bilmes. 2011. <span>“Simultaneous Learning and Covering with Adversarial Noise.”</span> <em>ICML</em>.
</div>
<div id="ref-max_halford" class="csl-entry" role="listitem">
Halford, Max. 2023. <span>“Online Active Learning in 80 Lines of Python.”</span>
</div>
<div id="ref-jasonH2020" class="csl-entry" role="listitem">
Hartline, Jason D., Yingkai Li, Liren Shan, and Yifan Wu. 2020. <span>“Optimization of Scoring Rules.”</span> <em>CoRR</em> abs/2007.02905. <a href="https://arxiv.org/abs/2007.02905">https://arxiv.org/abs/2007.02905</a>.
</div>
<div id="ref-jasonH2023" class="csl-entry" role="listitem">
Hartline, Jason D., Liren Shan, Yingkai Li, and Yifan Wu. 2023. <span>“Optimal Scoring Rules for Multi-Dimensional Effort.”</span> In <em>Proceedings of Thirty Sixth Conference on Learning Theory</em>, edited by Gergely Neu and Lorenzo Rosasco, 195:2624–50. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v195/hartline23a.html">https://proceedings.mlr.press/v195/hartline23a.html</a>.
</div>
<div id="ref-he2020momentum" class="csl-entry" role="listitem">
He, Kaiming, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. <span>“Momentum Contrast for Unsupervised Visual Representation Learning.”</span> In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9729–38. IEEE.
</div>
<div id="ref-pmlr-v89-hiranandani19a" class="csl-entry" role="listitem">
Hiranandani, Gaurush, Shant Boodaghians, Ruta Mehta, and Oluwasanmi Koyejo. 2019a. <span>“Performance Metric Elicitation from Pairwise Classifier Comparisons.”</span> In <em>Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics</em>, edited by Kamalika Chaudhuri and Masashi Sugiyama, 89:371–79. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v89/hiranandani19a.html">https://proceedings.mlr.press/v89/hiranandani19a.html</a>.
</div>
<div id="ref-NEURIPS2019_1fd09c5f" class="csl-entry" role="listitem">
Hiranandani, Gaurush, Shant Boodaghians, Ruta Mehta, and Oluwasanmi O Koyejo. 2019b. <span>“Multiclass Performance Metric Elicitation.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett. Vol. 32. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2019/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf</a>.
</div>
<div id="ref-nips" class="csl-entry" role="listitem">
Hiranandani, Gaurush, Harikrishna Narasimhan, and Sanmi Koyejo. 2020. <span>“Fair Performance Metric Elicitation.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:11083–95. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/7ec2442aa04c157590b2fa1a7d093a33-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2020/file/7ec2442aa04c157590b2fa1a7d093a33-Paper.pdf</a>.
</div>
<div id="ref-claus" class="csl-entry" role="listitem">
Holladay, Rachel, Shervin Javdani, Anca Dragan, and Siddhartha Srinivasa. 2016. <span>“Active Comparison Based Learning Incorporating User Uncertainty and Noise.”</span> <em>Proceedings of RSS ’16 Workshop on Model Learning for Human-Robot Communication</em>.
</div>
<div id="ref-NIPS2012_e6d8545d" class="csl-entry" role="listitem">
Jamieson, Kevin G, Robert Nowak, and Ben Recht. 2012. <span>“Query Complexity of Derivative-Free Optimization.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2012/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf</a>.
</div>
<div id="ref-AL_app_autonomous" class="csl-entry" role="listitem">
Jarl, Sanna, Linus Aronsson, Sadegh Rahrovani, and Morteza Haghir Chehreghani. 2021. <span>“Active Learning of Driving Scenario Trajectories.”</span> <em>Eng. Appl. Artif. Intell.</em> 113: 104972. <a href="https://api.semanticscholar.org/CorpusID:249113683">https://api.semanticscholar.org/CorpusID:249113683</a>.
</div>
<div id="ref-karamcheti2023languagedriven" class="csl-entry" role="listitem">
Karamcheti, Siddharth, Suraj Nair, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy Liang. 2023. <span>“Language-Driven Representation Learning for Robotics.”</span> <a href="https://arxiv.org/abs/2302.12766">https://arxiv.org/abs/2302.12766</a>.
</div>
<div id="ref-kongschoenebeck2019" class="csl-entry" role="listitem">
Kong, Yuqing, and Grant Schoenebeck. 2019. <span>“An Information Theoretic Framework for Designing Information Elicitation Mechanisms That Reward Truth-Telling.”</span> <em>ACM Trans. Econ. Comput.</em> 7 (1). <a href="https://doi.org/10.1145/3296670">https://doi.org/10.1145/3296670</a>.
</div>
<div id="ref-kwon2021targeted" class="csl-entry" role="listitem">
Kwon, Minae, Siddharth Karamcheti, Mariano-Florentino Cuellar, and Dorsa Sadigh. 2021. <span>“Targeted Data Acquisition for Evolving Negotiation Agents.”</span> <a href="https://arxiv.org/abs/2106.07728">https://arxiv.org/abs/2106.07728</a>.
</div>
<div id="ref-Li_2021" class="csl-entry" role="listitem">
Li, Kejun, Maegan Tucker, Erdem Biyik, Ellen Novoseller, Joel W. Burdick, Yanan Sui, Dorsa Sadigh, Yisong Yue, and Aaron D. Ames. 2021. <span>“ROIAL: Region of Interest Active Learning for Characterizing Exoskeleton Gait Preference Landscapes.”</span> In <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>. IEEE. <a href="https://doi.org/10.1109/icra48506.2021.9560840">https://doi.org/10.1109/icra48506.2021.9560840</a>.
</div>
<div id="ref-AL_partition" class="csl-entry" role="listitem">
Ma, Jiaqi, Ziqiao Ma, Joyce Chai, and Qiaozhu Mei. 2022. <span>“Partition-Based Active Learning for Graph Neural Networks.”</span> <em>ArXiv</em> abs/2201.09391. <a href="https://api.semanticscholar.org/CorpusID:246240846">https://api.semanticscholar.org/CorpusID:246240846</a>.
</div>
<div id="ref-AL_conformal" class="csl-entry" role="listitem">
Makili, Lázaro Emílio, Jesús A. Vega Sánchez, and Sebastián Dormido-Canto. 2012. <span>“Active Learning Using Conformal Predictors: Application to Image Classification.”</span> <em>Fusion Science and Technology</em> 62: 347–55. <a href="https://api.semanticscholar.org/CorpusID:115384000">https://api.semanticscholar.org/CorpusID:115384000</a>.
</div>
<div id="ref-AL_app_LLMs" class="csl-entry" role="listitem">
Margatina, Katerina, Timo Schick, Nikolaos Aletras, and Jane Dwivedi-Yu. 2023. <span>“Active Learning Principles for in-Context Learning with Large Language Models.”</span> <em>ArXiv</em> abs/2305.14264. <a href="https://api.semanticscholar.org/CorpusID:258841313">https://api.semanticscholar.org/CorpusID:258841313</a>.
</div>
<div id="ref-pref2" class="csl-entry" role="listitem">
Mas-Colell, Andreu. 1977. <span>“The Recoverability of Consumers’ Preferences from Market Demand Behavior.”</span> <em>Econometrica</em> 45 (6): 1409–30. <a href="http://www.jstor.org/stable/1912308">http://www.jstor.org/stable/1912308</a>.
</div>
<div id="ref-mcafee-87" class="csl-entry" role="listitem">
McAfee, R. Preston, and John McMillan. 1987. <span>“Auctions and Bidding.”</span> <em>Journal of Economic Literature</em> 25 (2): 699–738. <a href="http://www.jstor.org/stable/2726107">http://www.jstor.org/stable/2726107</a>.
</div>
<div id="ref-AL_experrorredn" class="csl-entry" role="listitem">
Mussmann, Stephen, Julia Reisler, Daniel Tsai, Ehsan Mousavi, Shayne O’Brien, and Moises Goldszmidt. 2022. <span>“Active Learning with Expected Error Reduction.”</span> <a href="https://arxiv.org/abs/2211.09283">https://arxiv.org/abs/2211.09283</a>.
</div>
<div id="ref-myers2021learning" class="csl-entry" role="listitem">
Myers, Vivek, Erdem Bıyık, Nima Anari, and Dorsa Sadigh. 2021. <span>“Learning Multimodal Rewards from Rankings.”</span> <a href="https://arxiv.org/abs/2109.12750">https://arxiv.org/abs/2109.12750</a>.
</div>
<div id="ref-nair2022r3m" class="csl-entry" role="listitem">
Nair, Suraj, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. 2022. <span>“R3M: A Universal Visual Representation for Robot Manipulation.”</span> <a href="https://arxiv.org/abs/2203.12601">https://arxiv.org/abs/2203.12601</a>.
</div>
<div id="ref-pmlr-v37-narasimhanb15" class="csl-entry" role="listitem">
Narasimhan, Harikrishna, Harish Ramaswamy, Aadirupa Saha, and Shivani Agarwal. 2015. <span>“Consistent Multiclass Algorithms for Complex Performance Measures.”</span> In <em>Proceedings of the 32nd International Conference on Machine Learning</em>, edited by Francis Bach and David Blei, 37:2398–2407. Proceedings of Machine Learning Research. Lille, France: PMLR. <a href="https://proceedings.mlr.press/v37/narasimhanb15.html">https://proceedings.mlr.press/v37/narasimhanb15.html</a>.
</div>
<div id="ref-radford2021learning" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>“Learning Transferable Visual Models from Natural Language Supervision.”</span> <em>arXiv Preprint arXiv:2103.00020</em>.
</div>
<div id="ref-pref1" class="csl-entry" role="listitem">
Samuelson, P. A. 1938. <span>“A Note on the Pure Theory of Consumer’s Behaviour.”</span> <em>Economica</em> 5 (17): 61–71. <a href="http://www.jstor.org/stable/2548836">http://www.jstor.org/stable/2548836</a>.
</div>
<div id="ref-lus-shep" class="csl-entry" role="listitem">
Shepard, Roger N. 1957. <span>“Stimulus and Response Generalization: A Stochastic Model Relating Generalization to Distance in Psychological Space.”</span> <em>Psychometrika</em> 22(4):325–345.
</div>
<div id="ref-AL_app_sensors" class="csl-entry" role="listitem">
Singh, Aarti, Robert D. Nowak, and Parameswaran Ramanathan. 2006. <span>“Active Learning for Adaptive Mobile Sensing Networks.”</span> <em>2006 5th International Conference on Information Processing in Sensor Networks</em>, 60–68. <a href="https://api.semanticscholar.org/CorpusID:17590956">https://api.semanticscholar.org/CorpusID:17590956</a>.
</div>
<div id="ref-ab" class="csl-entry" role="listitem">
Tamburrelli, Giordano, and Alessandro Margara. 2014. <span>“Towards Automated a/b Testing.”</span> In <em>Search-Based Software Engineering</em>. <a href="https://doi.org/10.1007/978-3-319-09940-8_13">https://doi.org/10.1007/978-3-319-09940-8_13</a>.
</div>
<div id="ref-AL_app_robotics" class="csl-entry" role="listitem">
Taylor, Annalisa T., Thomas A. Berrueta, and Todd D. Murphey. 2021. <span>“Active Learning in Robotics: A Review of Control Principles.”</span> <em>ArXiv</em> abs/2106.13697. <a href="https://api.semanticscholar.org/CorpusID:235652039">https://api.semanticscholar.org/CorpusID:235652039</a>.
</div>
<div id="ref-pref3" class="csl-entry" role="listitem">
Varian, Hal R. 2006. <span>“Revealed Preference.”</span> In <em>The SAGE Encyclopedia of Business Ethics and Society</em>. <a href="https://api.semanticscholar.org/CorpusID:1632873">https://api.semanticscholar.org/CorpusID:1632873</a>.
</div>
<div id="ref-lus-log" class="csl-entry" role="listitem">
Viappiani, Paolo, and Craig Boutilier. 2010. <span>“Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets.”</span> <em>NIPS</em>, 2352–60.
</div>
<div id="ref-walke2023bridgedata" class="csl-entry" role="listitem">
Walke, Homer, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, et al. 2023. <span>“BridgeData V2: A Dataset for Robot Learning at Scale.”</span> <a href="https://arxiv.org/abs/2308.12952">https://arxiv.org/abs/2308.12952</a>.
</div>
<div id="ref-xiao2022masked" class="csl-entry" role="listitem">
Xiao, Tete, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. 2022. <span>“Masked Visual Pre-Training for Motor Control.”</span> <a href="https://arxiv.org/abs/2203.06173">https://arxiv.org/abs/2203.06173</a>.
</div>
<div id="ref-ask_help" class="csl-entry" role="listitem">
Xie, Annie, Fahim Tajwar, Archit Sharma, and Chelsea Finn. 2022. <span>“When to Ask for Help: Proactive Interventions in Autonomous Reinforcement Learning.”</span> <a href="https://arxiv.org/abs/2210.10765">https://arxiv.org/abs/2210.10765</a>.
</div>
<div id="ref-YangNaiman+2014+477+496" class="csl-entry" role="listitem">
Yang, Sitan, and Daniel Q. Naiman. 2014. <span>“Multiclass Cancer Classification Based on Gene Expression Comparison.”</span> <em>Statistical Applications in Genetics and Molecular Biology</em> 13 (4): 477–96. <a href="https://doi.org/doi:10.1515/sagmb-2013-0053">https://doi.org/doi:10.1515/sagmb-2013-0053</a>.
</div>
<div id="ref-AL_mismatch" class="csl-entry" role="listitem">
Zhao, Shuyang, Toni Heittola, and Tuomas Virtanen. 2020. <span>“Active Learning for Sound Event Detection.”</span> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 28: 2895–905. <a href="https://api.semanticscholar.org/CorpusID:211082815">https://api.semanticscholar.org/CorpusID:211082815</a>.
</div>
<div id="ref-AL_uncertainty" class="csl-entry" role="listitem">
Zhu, Jingbo, Huizhen Wang, Benjamin Ka-Yin T’sou, and Matthew Y. Ma. 2010. <span>“Active Learning with Sampling by Uncertainty and Density for Data Annotations.”</span> <em>IEEE Transactions on Audio, Speech, and Language Processing</em> 18: 1323–31. <a href="https://api.semanticscholar.org/CorpusID:5777911">https://api.semanticscholar.org/CorpusID:5777911</a>.
</div>
</div>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../src/002-reward_model.html" class="pagination-link" aria-label="Human Decision Making and Choice Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/004-optim.html" class="pagination-link" aria-label="Model-Free Preference Optimization">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/003-measure.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>